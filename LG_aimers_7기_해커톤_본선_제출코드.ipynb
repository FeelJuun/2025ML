{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**LG Aimers 7기 본선 제출용 코드(해커톤 본선은 LG인화원에서 하였으며 제공된 서버로 진행하여 코드가 남아있지않지만 마지막으로 코랩에 저장된 코드)**\n",
        "\n",
        "LightGBM + XGBoost + Meta 데이터(예선에 사용했던 LSTM 제거)\n",
        "\n",
        "public 0.53972점으로 17등이었지만 일반화 성능에 집중하여 ->\n",
        "\n",
        "private 점수 0.54399점으로 9등 달성\n",
        "\n",
        "https://dacon.io/competitions/official/236594/leaderboard\n"
      ],
      "metadata": {
        "id": "4tnaJFMOhhl8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMjDXmRSHL2X"
      },
      "outputs": [],
      "source": [
        "## ============================================================\n",
        "# Resort F&B 7-day Forecast — LGB + XGB (+ meta 28-day rolls)\n",
        "# - 시간 퍼지 GroupKFold OOF → α(h,DOW[,season]) 튜닝 → γ/β & 캘맵 보정\n",
        "# - FULL 재학습 멀티시드 백깅 → TEST별 독립 추론\n",
        "# - meta: group/hwadam/room/ski/weather 의 28일 롤링 통합 (누출 방지)\n",
        "# - 강화: price 피처, 동요일 반복 신호, 동시매장 모멘텀, 시즌별 α·q\n",
        "# ============================================================\n",
        "import os, glob, re, math, gc, warnings, random, unicodedata\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# ---------------- Repro & blend knobs ----------------\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED)\n",
        "\n",
        "OOB_BLEND = 0.40    # OOF 기준 XGB 비중 초기값\n",
        "ALPHA_XGB = 0.40    # 추론 fallback\n",
        "MAX_AX_SUM = 0.70   # 트리 비중 상한\n",
        "\n",
        "# ---------------- path ----------------\n",
        "def _pick(*candidates):\n",
        "    for p in candidates:\n",
        "        if isinstance(p, str) and os.path.exists(p):\n",
        "            return p\n",
        "    return candidates[-1]\n",
        "\n",
        "if os.path.exists('/mnt/data'):\n",
        "    BASE_DIR = '/mnt/data'\n",
        "elif os.path.exists('data'):\n",
        "    BASE_DIR = 'data'\n",
        "else:\n",
        "    BASE_DIR = './'\n",
        "\n",
        "TRAIN_DIR       = _pick(os.path.join(BASE_DIR, 'train'), BASE_DIR)\n",
        "TEST_DIR        = _pick(os.path.join(BASE_DIR, 'test'), BASE_DIR)\n",
        "TRAIN_META_DIR  = _pick(os.path.join(BASE_DIR, 'meta'),\n",
        "                        os.path.join(TRAIN_DIR, 'meta'),\n",
        "                        BASE_DIR)\n",
        "TEST_META_DIR   = _pick(os.path.join(TEST_DIR, 'meta'),\n",
        "                        os.path.join(BASE_DIR, 'test', 'meta'),\n",
        "                        os.path.join(BASE_DIR, 'meta'),\n",
        "                        BASE_DIR)\n",
        "\n",
        "TRAIN_PATH        = _pick(os.path.join(TRAIN_DIR, 'train.csv'),\n",
        "                          os.path.join(BASE_DIR, 'train.csv'))\n",
        "SAMPLE_SUB_PATH   = _pick(os.path.join(BASE_DIR, 'sample_submission.csv'),\n",
        "                          os.path.join(BASE_DIR, 'data', 'sample_submission.csv'))\n",
        "PRICE_PATH        = _pick(os.path.join(BASE_DIR, 'price.csv'),\n",
        "                          os.path.join(TRAIN_DIR, 'price.csv'))\n",
        "\n",
        "# meta paths (train)\n",
        "TRAIN_GROUP_PATH   = _pick(os.path.join(TRAIN_META_DIR, 'TRAIN_group.csv'),\n",
        "                           os.path.join(BASE_DIR, 'TRAIN_group.csv'))\n",
        "TRAIN_HWADAM_PATH  = _pick(os.path.join(TRAIN_META_DIR, 'TRAIN_hwadam.csv'),\n",
        "                           os.path.join(BASE_DIR, 'TRAIN_hwadam.csv'))\n",
        "TRAIN_ROOM_PATH    = _pick(os.path.join(TRAIN_META_DIR, 'TRAIN_room.csv'),\n",
        "                           os.path.join(BASE_DIR, 'TRAIN_room.csv'))\n",
        "TRAIN_SKI_PATH     = _pick(os.path.join(TRAIN_META_DIR, 'TRAIN_ski.csv'),\n",
        "                           os.path.join(BASE_DIR, 'TRAIN_ski.csv'))\n",
        "TRAIN_WEATHER_PATH = _pick(os.path.join(TRAIN_META_DIR, 'TRAIN_weather.csv'),\n",
        "                           os.path.join(BASE_DIR, 'TRAIN_weather.csv'))\n",
        "\n",
        "def _pick_test(fname):  # TEST_xx meta 파일 탐색\n",
        "    return _pick(os.path.join(TEST_META_DIR, fname),\n",
        "                 os.path.join(BASE_DIR, fname))\n",
        "\n",
        "TEST_FILES = sorted(\n",
        "    glob.glob(os.path.join(TEST_DIR, 'TEST_*.csv'))\n",
        "    + glob.glob(os.path.join(BASE_DIR, 'TEST_*.csv'))\n",
        ")\n",
        "OUT_PATH = os.path.join(BASE_DIR, 'submission.csv')\n",
        "print(f\"[Path] BASE_DIR={BASE_DIR}\")\n",
        "print(f\"        TRAIN_DIR={TRAIN_DIR}\")\n",
        "print(f\"        TEST_DIR={TEST_DIR}\")\n",
        "\n",
        "# ---------------- Utils ----------------\n",
        "def _canon_text(s: str) -> str:\n",
        "    if s is None: return ''\n",
        "    s = unicodedata.normalize('NFKC', str(s))\n",
        "    return s.replace('\\ufeff','').replace('\\u200b','').replace('\\xa0','').strip()\n",
        "\n",
        "def _clean_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    return df.rename(columns={c:_canon_text(c) for c in df.columns})\n",
        "\n",
        "def _force_kor_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    m = { 'date':'영업일자', 'key':'영업장명_메뉴명', 'y':'매출수량' }\n",
        "    return df.rename(columns={k:v for k,v in m.items() if k in df.columns})\n",
        "\n",
        "def _normalize_key_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    if '영업장명_메뉴명' in df.columns:\n",
        "        df['영업장명_메뉴명'] = df['영업장명_메뉴명'].map(_canon_text)\n",
        "    return df\n",
        "\n",
        "def _safe_read_csv(path):\n",
        "    try:\n",
        "        if path and os.path.exists(path):\n",
        "            df = pd.read_csv(path)\n",
        "            return _clean_columns(df)\n",
        "    except Exception:\n",
        "        return None\n",
        "    return None\n",
        "\n",
        "# --- Calibration shrink helper ---\n",
        "def _shrink(x, tau=0.5):\n",
        "    return 1.0 + float(tau) * (float(x) - 1.0)\n",
        "\n",
        "TAU = dict(\n",
        "    gamma=1.05,  # γ 수축\n",
        "    store=0.40,\n",
        "    sdow=0.40,\n",
        "    sh=0.40,\n",
        "    winter=0.50,\n",
        "    wk=0.25,\n",
        "    season=0.30,\n",
        "    aff=0.20\n",
        ")\n",
        "\n",
        "# ---------------- Domain: stores/holidays ----------------\n",
        "HEAVY_STORES = {'담하','미라시아'}\n",
        "HEAVY_W = 1.35\n",
        "def store_weight(name: str) -> float:\n",
        "    for s in HEAVY_STORES:\n",
        "        if str(name).startswith(s):\n",
        "            return HEAVY_W\n",
        "    return 1.0\n",
        "\n",
        "def is_hwadam_store(s: str) -> bool:\n",
        "    s = str(s)\n",
        "    return s.startswith('화담숲주막') or s.startswith('화담숲카페')\n",
        "\n",
        "def _season_id_from_dt(dt: pd.Timestamp) -> int:\n",
        "    m = int(dt.month)\n",
        "    return 1 if 6<=m<=8 else (2 if 9<=m<=11 else (0 if 3<=m<=5 else 3))\n",
        "\n",
        "def _spring_autumn_active(dt: pd.Timestamp) -> int:\n",
        "    m = int(dt.month)\n",
        "    return int(m in [4,5,6,9,10,11])\n",
        "\n",
        "def _summer_family(dt: pd.Timestamp) -> int:\n",
        "    return int(int(dt.month) in [7,8])\n",
        "\n",
        "STORE_SEASON_AFFINITY = {\n",
        "    ('화담숲주막','spring'): 1.15, ('화담숲주막','autumn'): 1.20,\n",
        "    ('화담숲카페','spring'): 1.15, ('화담숲카페','autumn'): 1.20,\n",
        "    ('미라시아','summer'): 1.08, ('느티나무 셀프BBQ','summer'): 1.10,\n",
        "}\n",
        "\n",
        "K_HOLS = set(pd.to_datetime([\n",
        "    # 2023\n",
        "    \"2023-01-01\",\"2023-01-21\",\"2023-01-22\",\"2023-01-23\",\"2023-01-24\",\n",
        "    \"2023-03-01\",\"2023-05-05\",\"2023-05-27\",\"2023-06-06\",\"2023-08-15\",\n",
        "    \"2023-09-28\",\"2023-09-29\",\"2023-09-30\",\"2023-10-03\",\"2023-10-09\",\n",
        "    \"2023-12-25\",\n",
        "    # 2024\n",
        "    \"2024-01-01\",\"2024-02-09\",\"2024-02-10\",\"2024-02-11\",\"2024-02-12\",\n",
        "    \"2024-03-01\",\"2024-05-05\",\"2024-05-06\",\"2024-05-15\",\"2024-06-06\",\n",
        "    \"2024-08-15\",\"2024-09-16\",\"2024-09-17\",\"2024-09-18\",\"2024-10-03\",\n",
        "    \"2024-10-09\",\"2024-12-25\",\n",
        "    # 2025\n",
        "    \"2025-01-01\",\"2025-01-27\",\"2025-01-28\",\"2025-01-29\",\n",
        "    \"2025-03-01\",\"2025-05-05\",\"2025-05-06\",\"2025-06-06\",\"2025-08-15\",\n",
        "    \"2025-10-03\",\"2025-10-06\",\"2025-10-07\",\"2025-10-08\",\"2025-10-09\",\n",
        "    \"2025-12-25\",\n",
        "]).date)\n",
        "\n",
        "def is_holiday(ts) -> pd.Series:\n",
        "    td = pd.to_datetime(ts)\n",
        "    if not isinstance(td, pd.Series):\n",
        "        td = pd.Series(td)\n",
        "    return td.dt.date.map(lambda d: int(d in K_HOLS)).astype(int)\n",
        "\n",
        "def season_code(m: int) -> int:\n",
        "    if m in [12,1,2]: return 0\n",
        "    if m in [3,4,5]:  return 1\n",
        "    if m in [6,7,8]:  return 2\n",
        "    return 3\n",
        "\n",
        "def _is_winter_date(dt: pd.Timestamp) -> int:\n",
        "    m = dt.month; d = dt.day\n",
        "    return int((m==12 and d>=10) or (m in [1,2]) or (m==3 and d<=10))\n",
        "\n",
        "# ---------------- Store/menu ecosystem priors ----------------\n",
        "_STORE_PRIOR_RAW = {\n",
        "    '포레스트릿': 47.84,\n",
        "    '화담숲주막': 34.38,\n",
        "    '카페테리아': 18.86,\n",
        "    '담하': 5.50,\n",
        "    '미라시아': 5.50,\n",
        "    '라그로타': 1.31\n",
        "}\n",
        "_sp_mean = float(np.mean(list(_STORE_PRIOR_RAW.values()))) if _STORE_PRIOR_RAW else 1.0\n",
        "STORE_PRIOR = {k: (v/_sp_mean if _sp_mean>0 else 1.0) for k,v in _STORE_PRIOR_RAW.items()}\n",
        "def store_prior_index(store: str) -> float:\n",
        "    return float(STORE_PRIOR.get(str(store), 1.0))\n",
        "\n",
        "# ---------------- Time-pattern priors (월/요일) ----------------\n",
        "_MONTH_PRIOR_RAW = {1:21.64, 2:17.37, 3:2.64, 10:15.47, 12:13.71}\n",
        "_m_mean = float(np.mean(list(_MONTH_PRIOR_RAW.values()))) if _MONTH_PRIOR_RAW else 1.0\n",
        "MONTH_PRIOR = {m: (v/_m_mean if _m_mean>0 else 1.0) for m,v in _MONTH_PRIOR_RAW.items()}\n",
        "\n",
        "_DOW_PRIOR_RAW = {0:7.76, 4:12.26, 5:15.28, 6:12.72}\n",
        "_d_mean = float(np.mean(list(_DOW_PRIOR_RAW.values()))) if _DOW_PRIOR_RAW else 1.0\n",
        "DOW_PRIOR = {d: (v/_d_mean if _d_mean>0 else 1.0) for d,v in _DOW_PRIOR_RAW.items()}\n",
        "\n",
        "def month_prior(m: int) -> float:\n",
        "    return float(MONTH_PRIOR.get(int(m), 1.0))\n",
        "\n",
        "def dow_prior(d: int) -> float:\n",
        "    return float(DOW_PRIOR.get(int(d), 1.0))\n",
        "\n",
        "# ---------------- General helpers ----------------\n",
        "def split_store_menu(x: str):\n",
        "    x = str(x)\n",
        "    if \"_\" in x:\n",
        "        p = x.find(\"_\"); return x[:p], x[p+1:]\n",
        "    return \"UNKNOWN\", x\n",
        "\n",
        "def ensure_full_daily_index_multi(df, value_cols):\n",
        "    df = df.copy()\n",
        "    df['영업일자'] = pd.to_datetime(df['영업일자'])\n",
        "    out=[]\n",
        "    for name,g in df.groupby('영업장명_메뉴명'):\n",
        "        g = g.sort_values('영업일자')\n",
        "        idx = pd.date_range(g['영업일자'].min(), g['영업일자'].max(), freq=\"D\")\n",
        "        gg = g.set_index('영업일자').reindex(idx).rename_axis('영업일자').reset_index()\n",
        "        gg['영업장명_메뉴명'] = name\n",
        "        for c in value_cols:\n",
        "            gg[c] = gg[c].fillna(0.0)\n",
        "        out.append(gg)\n",
        "    return pd.concat(out, ignore_index=True)\n",
        "\n",
        "def build_feature_from_window(y28: np.ndarray, y28_raw: np.ndarray):\n",
        "    f={}\n",
        "    y = np.clip(y28.astype(float), 0.0, None)\n",
        "    yr = y28_raw.astype(float)\n",
        "\n",
        "    for i in range(28):\n",
        "        f[f'lag_{i+1}'] = float(y[-(i+1)])\n",
        "\n",
        "    def stats(prefix, arr):\n",
        "        f[f'{prefix}_mean'] = float(arr.mean())\n",
        "        f[f'{prefix}_std']  = float(arr.std(ddof=0))\n",
        "        f[f'{prefix}_max']  = float(arr.max())\n",
        "        f[f'{prefix}_sum']  = float(arr.sum())\n",
        "        f[f'{prefix}_zero_share'] = float((arr==0).mean())\n",
        "        f[f'{prefix}_nz_share']   = float((arr>0).mean())\n",
        "    stats('r7',  y[-7:])\n",
        "    stats('r14', y[-14:])\n",
        "    stats('r28', y[-28:])\n",
        "\n",
        "    a7 = y[-7:].mean(); a14 = y[-14:-7].mean() if y[-14:-7].size>0 else 0.0\n",
        "    f['mom_7_7']  = float(a7/(a14+1e-6))\n",
        "    f['mom_7_28'] = float(a7/(y.mean()+1e-6))\n",
        "    x = np.arange(1,29); ylog = np.log1p(y)\n",
        "    denom = np.sum((x-x.mean())**2); slope = 0.0 if denom==0 else np.sum((x-x.mean())*(ylog-ylog.mean()))/denom\n",
        "    f['trend_slope'] = float(slope)\n",
        "\n",
        "    nz_idx = np.where(y>0)[0]\n",
        "    f['days_since_last_nz'] = float(28 - (nz_idx[-1]+1) if len(nz_idx)>0 else 28)\n",
        "    f['last_nz_val'] = float(y[nz_idx[-1]] if len(nz_idx)>0 else 0.0)\n",
        "\n",
        "    neg_mask = (yr < 0)\n",
        "    f['neg_cnt_7']  = float(neg_mask[-7:].sum())\n",
        "    f['neg_cnt_14'] = float(neg_mask[-14:].sum())\n",
        "    f['neg_cnt_28'] = float(neg_mask[-28:].sum())\n",
        "    f['neg_sum_abs_28'] = float(np.abs(yr[neg_mask][-28:]).sum()) if neg_mask.any() else 0.0\n",
        "    f['neg_min_28'] = float(yr[-28:].min())\n",
        "    f['neg_share_28'] = float((yr[-28:]<0).mean())\n",
        "    neg_idx = np.where(yr<0)[0]\n",
        "    f['days_since_last_neg'] = float(28 - (neg_idx[-1]+1) if len(neg_idx)>0 else 28)\n",
        "\n",
        "    # --- 동일 요일 반복 신호(4주) ---\n",
        "    dows = np.array([y[-7], y[-14], y[-21], y[-28]])\n",
        "    f['dow_repeat_mean4'] = float(dows.mean())\n",
        "    f['dow_repeat_std4']  = float(dows.std(ddof=0))\n",
        "    f['dow_repeat_max4']  = float(dows.max())\n",
        "    f['dow_repeat_min4']  = float(dows.min())\n",
        "    return f\n",
        "\n",
        "# ============================================================\n",
        "# Meta loaders (weather fix included)\n",
        "# ============================================================\n",
        "def _extract_weather_series(wdf):\n",
        "    \"\"\"일단위 날씨 시계열 3종 추출: temp_mean, temp_range, rain_sum\"\"\"\n",
        "    if wdf is None or wdf.empty:\n",
        "        return None, None, None\n",
        "    w = wdf.copy()\n",
        "    if '일시' in w.columns:\n",
        "        w['date'] = pd.to_datetime(w['일시']).dt.date\n",
        "    elif '영업일자' in w.columns:\n",
        "        w['date'] = pd.to_datetime(w['영업일자']).dt.date\n",
        "    elif 'date' not in w.columns:\n",
        "        return None, None, None\n",
        "    w['date'] = pd.to_datetime(w['date'])\n",
        "\n",
        "    def _pick(col_sub):\n",
        "        cand = [c for c in w.columns if col_sub in str(c)]\n",
        "        return cand[0] if cand else None\n",
        "\n",
        "    c_tm  = _pick('평균기온')\n",
        "    c_rng = _pick('일교차')\n",
        "    c_rn  = _pick('강수량')\n",
        "\n",
        "    g = w.groupby('date')\n",
        "    temp_mean = g[c_tm].mean().astype(float) if c_tm else None\n",
        "    temp_rng  = g[c_rng].mean().astype(float) if c_rng else None\n",
        "    rain      = g[c_rn].sum().astype(float)  if c_rn else None\n",
        "    return temp_mean, temp_rng, rain\n",
        "\n",
        "def _load_train_meta_bundle(train_dates_index: pd.DatetimeIndex, store_names: np.ndarray):\n",
        "    # group\n",
        "    gdf = _safe_read_csv(TRAIN_GROUP_PATH)\n",
        "    group_df = None; site_group = None\n",
        "    if gdf is not None and not gdf.empty:\n",
        "        gdf['date'] = pd.to_datetime(gdf.iloc[:,0])\n",
        "        group_cols = [c for c in gdf.columns if c not in ['date']]\n",
        "        group_df = gdf.set_index('date')[group_cols].apply(pd.to_numeric, errors='coerce').fillna(0.0).sort_index()\n",
        "        site_group = group_df.sum(axis=1)\n",
        "\n",
        "    # hwadam\n",
        "    hdf = _safe_read_csv(TRAIN_HWADAM_PATH)\n",
        "    site_hwadam = None\n",
        "    if hdf is not None and not hdf.empty:\n",
        "        hdf['date'] = pd.to_datetime(hdf.iloc[:,0])\n",
        "        cols = [c for c in hdf.columns if c not in ['date']]\n",
        "        site_hwadam = hdf.set_index('date')[cols].apply(pd.to_numeric, errors='coerce').sum(axis=1).sort_index()\n",
        "\n",
        "    # room\n",
        "    rdf = _safe_read_csv(TRAIN_ROOM_PATH)\n",
        "    site_room = None\n",
        "    if rdf is not None and not rdf.empty:\n",
        "        rdf['date'] = pd.to_datetime(rdf.iloc[:,0])\n",
        "        cols = [c for c in rdf.columns if c not in ['date']]\n",
        "        site_room = rdf.set_index('date')[cols].apply(pd.to_numeric, errors='coerce').sum(axis=1).sort_index()\n",
        "\n",
        "    # ski\n",
        "    sdf = _safe_read_csv(TRAIN_SKI_PATH)\n",
        "    site_ski = None\n",
        "    if sdf is not None and not sdf.empty:\n",
        "        sdf['date'] = pd.to_datetime(sdf.iloc[:,0])\n",
        "        cols = [c for c in sdf.columns if ('1일' in c) or ('내장객' in c)]\n",
        "        if not cols:\n",
        "            # 시간대 태깅 합으로 대체\n",
        "            cols = [c for c in sdf.columns if c not in ['date']]\n",
        "            site_ski = sdf.set_index('date')[cols].apply(pd.to_numeric, errors='coerce').sum(axis=1).sort_index()\n",
        "        else:\n",
        "            site_ski = sdf.set_index('date')[cols].apply(pd.to_numeric, errors='coerce').sum(axis=1).sort_index()\n",
        "\n",
        "    # weather\n",
        "    wdf = _safe_read_csv(TRAIN_WEATHER_PATH)\n",
        "    temp_mean = temp_rng = rain = None\n",
        "    if wdf is not None:\n",
        "        temp_mean, temp_rng, rain = _extract_weather_series(wdf)\n",
        "\n",
        "    site_df = pd.DataFrame(index=pd.DatetimeIndex(sorted(pd.unique(train_dates_index))))\n",
        "    for ser, name in [\n",
        "        (site_group, 'site_group'),\n",
        "        (site_hwadam,'site_hwadam'),\n",
        "        (site_room,  'site_room'),\n",
        "        (site_ski,   'site_ski'),\n",
        "        (temp_mean,  'temp_mean'),\n",
        "        (temp_rng,   'temp_range'),\n",
        "        (rain,       'rain_sum'),\n",
        "    ]:\n",
        "        if ser is not None:\n",
        "            site_df[name] = ser\n",
        "    site_df = site_df.sort_index().fillna(0.0)\n",
        "\n",
        "    # group_store_map\n",
        "    group_store_map = {}\n",
        "    if group_df is not None:\n",
        "        gcols = list(group_df.columns)\n",
        "        for s in np.unique(store_names.astype(str)):\n",
        "            hit = None\n",
        "            for c in gcols:\n",
        "                if s.startswith(str(c)):\n",
        "                    hit = c; break\n",
        "            group_store_map[s] = hit  # 없으면 None\n",
        "    return dict(site_df=site_df, group_df=group_df, group_store_map=group_store_map)\n",
        "\n",
        "def _load_test_meta_bundle(tid: int):\n",
        "    # group\n",
        "    gpath = _pick_test(f'TEST_group_{tid:02d}.csv'); group_df=None; site_group=None\n",
        "    gdf = _safe_read_csv(gpath)\n",
        "    if gdf is not None and not gdf.empty:\n",
        "        gdf['date'] = pd.to_datetime(gdf.iloc[:,0])\n",
        "        group_cols = [c for c in gdf.columns if c not in ['date']]\n",
        "        group_df = gdf.set_index('date')[group_cols].apply(pd.to_numeric, errors='coerce').fillna(0.0).sort_index()\n",
        "        site_group = group_df.sum(axis=1)\n",
        "\n",
        "    # hwadam\n",
        "    hpath = _pick_test(f'TEST_hwadam_{tid:02d}.csv'); site_hwadam=None\n",
        "    hdf = _safe_read_csv(hpath)\n",
        "    if hdf is not None and not hdf.empty:\n",
        "        hdf['date'] = pd.to_datetime(hdf.iloc[:,0])\n",
        "        cols = [c for c in hdf.columns if c not in ['date']]\n",
        "        site_hwadam = hdf.set_index('date')[cols].apply(pd.to_numeric, errors='coerce').sum(axis=1).sort_index()\n",
        "\n",
        "    # room\n",
        "    rpath = _pick_test(f'TEST_room_{tid:02d}.csv'); site_room=None\n",
        "    rdf = _safe_read_csv(rpath)\n",
        "    if rdf is not None and not rdf.empty:\n",
        "        rdf['date'] = pd.to_datetime(rdf.iloc[:,0])\n",
        "        cols = [c for c in rdf.columns if c not in ['date']]\n",
        "        site_room = rdf.set_index('date')[cols].apply(pd.to_numeric, errors='coerce').sum(axis=1).sort_index()\n",
        "\n",
        "    # ski\n",
        "    spath = _pick_test(f'TEST_ski_{tid:02d}.csv'); site_ski=None\n",
        "    sdf = _safe_read_csv(spath)\n",
        "    if sdf is not None and not sdf.empty:\n",
        "        sdf['date'] = pd.to_datetime(sdf.iloc[:,0])\n",
        "        cols = [c for c in sdf.columns if ('1일' in c) or ('내장객' in c)]\n",
        "        if not cols:\n",
        "            cols = [c for c in sdf.columns if c not in ['date']]\n",
        "            site_ski = sdf.set_index('date')[cols].apply(pd.to_numeric, errors='coerce').sum(axis=1).sort_index()\n",
        "        else:\n",
        "            site_ski = sdf.set_index('date')[cols].apply(pd.to_numeric, errors='coerce').sum(axis=1).sort_index()\n",
        "\n",
        "    # weather\n",
        "    wpath = _pick_test(f'TEST_weather_{tid:02d}.csv')\n",
        "    wdf = _safe_read_csv(wpath)\n",
        "    temp_mean = temp_rng = rain = None\n",
        "    if wdf is not None:\n",
        "        temp_mean, temp_rng, rain = _extract_weather_series(wdf)\n",
        "\n",
        "    site_df = pd.DataFrame()\n",
        "    for ser, name in [\n",
        "        (site_group, 'site_group'),\n",
        "        (site_hwadam,'site_hwadam'),\n",
        "        (site_room,  'site_room'),\n",
        "        (site_ski,   'site_ski'),\n",
        "        (temp_mean,  'temp_mean'),\n",
        "        (temp_rng,   'temp_range'),\n",
        "        (rain,       'rain_sum'),\n",
        "    ]:\n",
        "        if ser is not None:\n",
        "            site_df[name] = ser\n",
        "    if not site_df.empty:\n",
        "        site_df = site_df.sort_index().fillna(0.0)\n",
        "    return dict(site_df=site_df, group_df=group_df)\n",
        "\n",
        "# meta → 28일 윈도우에서 요약 피처 생성\n",
        "def _meta_feats_from_dates(dates_28: np.ndarray,\n",
        "                           store_name: str,\n",
        "                           site_df: pd.DataFrame,\n",
        "                           group_df: pd.DataFrame = None,\n",
        "                           group_store_map: dict = None,\n",
        "                           prefix: str = 'meta'):\n",
        "    out={}\n",
        "    if site_df is None or site_df.empty:\n",
        "        cols = ['site_group','site_hwadam','site_room','site_ski','temp_mean','temp_range','rain_sum']\n",
        "        for c in cols:\n",
        "            out[f'{prefix}_{c}_r7_mean']  = 0.0\n",
        "            out[f'{prefix}_{c}_r28_mean'] = 0.0\n",
        "            out[f'{prefix}_{c}_last']     = 0.0\n",
        "    else:\n",
        "        idx = pd.to_datetime(dates_28)\n",
        "        sub = site_df.reindex(idx).fillna(0.0)\n",
        "        for c in sub.columns:\n",
        "            arr = sub[c].values.astype(float)\n",
        "            out[f'{prefix}_{c}_r7_mean']  = float(arr[-7:].mean())\n",
        "            out[f'{prefix}_{c}_r28_mean'] = float(arr[-28:].mean())\n",
        "            out[f'{prefix}_{c}_last']     = float(arr[-1])\n",
        "\n",
        "    # 매장별 단체건수(있으면)\n",
        "    v7=v28=vlast=0.0\n",
        "    if (group_df is not None) and (group_store_map is not None):\n",
        "        gcol = group_store_map.get(str(store_name))\n",
        "        if gcol and (gcol in group_df.columns):\n",
        "            subg = group_df.reindex(pd.to_datetime(dates_28))[gcol].fillna(0.0).values.astype(float)\n",
        "            v7, v28, vlast = float(subg[-7:].mean()), float(subg[-28:].mean()), float(subg[-1])\n",
        "    out[f'{prefix}_group_store_r7_mean']  = v7\n",
        "    out[f'{prefix}_group_store_r28_mean'] = v28\n",
        "    out[f'{prefix}_group_store_last']     = vlast\n",
        "    return out\n",
        "\n",
        "# ---------------- Load ----------------\n",
        "print(f\"[Load] {TRAIN_PATH}\")\n",
        "train_raw = pd.read_csv(TRAIN_PATH)\n",
        "train_raw = _clean_columns(train_raw); train_raw = _force_kor_cols(train_raw)\n",
        "train_raw = train_raw.rename(columns={\"영업일자\":\"date\",\"영업장명_메뉴명\":\"key\",\"매출수량\":\"y_raw\"})\n",
        "train_raw[\"date\"] = pd.to_datetime(train_raw[\"date\"])\n",
        "train_raw[\"y_raw\"] = train_raw[\"y_raw\"].astype(float)\n",
        "\n",
        "# --- price map ---\n",
        "PRICE_MAP = {}; STORE_PRICE_MEAN = {}; GLOBAL_PRICE_MEAN = 0.0\n",
        "pdf = _safe_read_csv(PRICE_PATH)\n",
        "if pdf is not None and not pdf.empty:\n",
        "    pdf = pdf.rename(columns={'영업장명_메뉴명':'key','평균판매금액':'avg_price'})\n",
        "    pdf['key'] = pdf['key'].map(_canon_text)\n",
        "    pdf['avg_price'] = pd.to_numeric(pdf['avg_price'], errors='coerce').fillna(0.0)\n",
        "    pdf['store'], pdf['menu'] = zip(*pdf['key'].map(split_store_menu))\n",
        "    PRICE_MAP = dict(zip(pdf['key'], pdf['avg_price'].astype(float)))\n",
        "    STORE_PRICE_MEAN = pdf.groupby('store')['avg_price'].mean().to_dict()\n",
        "    GLOBAL_PRICE_MEAN = float(pdf['avg_price'].mean())\n",
        "\n",
        "train = train_raw.copy()\n",
        "train[\"y\"] = train[\"y_raw\"].clip(lower=0.0)\n",
        "train[\"store\"], train[\"menu\"] = zip(*train[\"key\"].map(split_store_menu))\n",
        "\n",
        "print(\"Train:\", train.shape, \"| Period:\", train[\"date\"].min().date(), \"~\", train[\"date\"].max().date())\n",
        "print(\"Unique series:\", train[\"key\"].nunique(), \"| Stores:\", train[\"store\"].nunique())\n",
        "print(f\"Zero share: {(train['y']==0).mean():.3f}\")\n",
        "\n",
        "train_full = ensure_full_daily_index_multi(\n",
        "    train[[\"date\",\"key\",\"y\",\"y_raw\"]].rename(columns={\"date\":\"영업일자\",\"key\":\"영업장명_메뉴명\"}),\n",
        "    value_cols=[\"y\",\"y_raw\"]\n",
        ")\n",
        "train_full = train_full.rename(columns={\"영업일자\":\"date\",\"영업장명_메뉴명\":\"key\"})\n",
        "train_full[\"store\"], train_full[\"menu\"] = zip(*train_full[\"key\"].map(split_store_menu))\n",
        "\n",
        "# ---------------- Store totals (for peer/ecosystem) ----------------\n",
        "_store_tot_ser = train_full.groupby(['date','store'])['y'].sum()\n",
        "STORE_TOTAL_SER = _store_tot_ser.rename_axis(['date','store']).swaplevel(0,1).sort_index()\n",
        "\n",
        "def _lookup_store_total_series(store_name, dates, ser):\n",
        "    idx = pd.MultiIndex.from_arrays(\n",
        "        [np.array([store_name]*len(dates), dtype=object), pd.to_datetime(dates)],\n",
        "        names=['store','date']\n",
        "    )\n",
        "    return ser.reindex(idx, fill_value=0.0).values\n",
        "\n",
        "# ---------------- Train meta bundle ----------------\n",
        "TRAIN_META = _load_train_meta_bundle(train_full['date'], train_full['store'].values)\n",
        "\n",
        "# ---------------- Build windows (Tabular only) ----------------\n",
        "print(\"[Windows] 28->7 supervised windows ...\")\n",
        "\n",
        "def make_supervised_windows(df_item, store_name, menu_name):\n",
        "    vals     = df_item['y'].values.astype(float)\n",
        "    vals_raw = df_item['y_raw'].values.astype(float)\n",
        "    dates    = pd.to_datetime(df_item['영업일자']).values\n",
        "    st28_all = _lookup_store_total_series(store_name, dates, STORE_TOTAL_SER)\n",
        "\n",
        "    X_rows=[]; Y_rows=[]; meta=[]\n",
        "    if len(vals)<35: return X_rows, Y_rows, meta\n",
        "    for end in range(27, len(vals)-7):\n",
        "        y28  = vals[end-27:end+1]\n",
        "        y28r = vals_raw[end-27:end+1]\n",
        "        st28 = st28_all[end-27:end+1]\n",
        "        peer28 = np.clip(st28 - y28, 0.0, None)\n",
        "        dts = pd.to_datetime(dates[end-27:end+1])\n",
        "\n",
        "        feats = build_feature_from_window(y28, y28r)\n",
        "        feats['store_prior']    = store_prior_index(store_name)\n",
        "        feats['store_r7_mean']  = float(st28[-7:].mean())\n",
        "        feats['store_r28_mean'] = float(st28[-28:].mean())\n",
        "        feats['peer_r7_mean']   = float(peer28[-7:].mean())\n",
        "        feats['peer_r28_mean']  = float(peer28[-28:].mean())\n",
        "        feats['share_in_store_r7']  = float(y28[-7:].sum()/(st28[-7:].sum()+1e-6))\n",
        "        feats['share_in_store_r28'] = float(y28[-28:].sum()/(st28[-28:].sum()+1e-6))\n",
        "\n",
        "        # 동시 매장 총수요/피어 모멘텀\n",
        "        def _mom(a,b): return float(a/(b+1e-6))\n",
        "        feats['store_mom_7_7'] = _mom(st28[-7:].mean(),  st28[-14:-7].mean())\n",
        "        feats['peer_mom_7_7']  = _mom(peer28[-7:].mean(), peer28[-14:-7].mean())\n",
        "\n",
        "        # ---- meta 28일 롤링 피처 (누출 금지: 오직 과거 28일)\n",
        "        mf = _meta_feats_from_dates(\n",
        "            dates_28=dts,\n",
        "            store_name=store_name,\n",
        "            site_df=TRAIN_META['site_df'],\n",
        "            group_df=TRAIN_META['group_df'],\n",
        "            group_store_map=TRAIN_META['group_store_map'],\n",
        "            prefix='meta'\n",
        "        )\n",
        "        feats.update(mf)\n",
        "\n",
        "        # 가격 피처\n",
        "        key_name = f\"{store_name}_{menu_name}\"\n",
        "        p = PRICE_MAP.get(key_name, STORE_PRICE_MEAN.get(store_name, GLOBAL_PRICE_MEAN))\n",
        "        mu_s = STORE_PRICE_MEAN.get(store_name, GLOBAL_PRICE_MEAN if GLOBAL_PRICE_MEAN>0 else 1.0)\n",
        "        feats['avg_price']   = float(p if p is not None else 0.0)\n",
        "        feats['price_index'] = float((p/(mu_s+1e-6)) if p and mu_s else 1.0)\n",
        "\n",
        "        feats['store_name'] = store_name\n",
        "        feats['menu_name']  = menu_name\n",
        "        yf  = vals[end+1:end+8]\n",
        "        X_rows.append(feats); Y_rows.append(yf)\n",
        "        meta.append({'anchor_date': pd.to_datetime(dates[end]),\n",
        "                     'store_name': store_name,\n",
        "                     'menu_name' : menu_name})\n",
        "    return X_rows, Y_rows, meta\n",
        "\n",
        "def df_to_windows(df_full):\n",
        "    X_all=[]; Y_all=[]; M_all=[]\n",
        "    for name,g in df_full.groupby('영업장명_메뉴명'):\n",
        "        store, menu = split_store_menu(name)\n",
        "        g = g.sort_values('영업일자')\n",
        "        X,Y,M = make_supervised_windows(g, store, menu)\n",
        "        if X:\n",
        "            X_all.extend(X); Y_all.extend(Y); M_all.extend(M)\n",
        "    return pd.DataFrame(X_all), np.array(Y_all), pd.DataFrame(M_all)\n",
        "\n",
        "X_base, Y, META = df_to_windows(\n",
        "    train_full[[\"date\",\"key\",\"y\",\"y_raw\"]].rename(columns={\"date\":\"영업일자\",\"key\":\"영업장명_메뉴명\"})\n",
        ")\n",
        "META = META.reset_index(drop=True); X_base = X_base.reset_index(drop=True)\n",
        "\n",
        "# Encoders\n",
        "def fit_le_with_unk(values):\n",
        "    le = LabelEncoder()\n",
        "    uniq = pd.Series(values).astype(str).unique().tolist()\n",
        "    uniq = sorted(list(set(uniq + [\"<UNK>\"])))\n",
        "    le.fit(uniq)\n",
        "    return le\n",
        "\n",
        "le_store = fit_le_with_unk(META['store_name'])\n",
        "le_menu  = fit_le_with_unk(META['menu_name'])\n",
        "\n",
        "X_base[\"store_le\"] = le_store.transform(X_base[\"store_name\"].astype(str).fillna(\"<UNK>\"))\n",
        "X_base[\"menu_le\"]  = le_menu.transform(X_base[\"menu_name\"].astype(str).fillna(\"<UNK>\"))\n",
        "\n",
        "def add_kw(df):\n",
        "    MENU_KW = ['세트','라떼','커피','아메리카노','맥주','소주','와인','막걸리','사케','피자','파스타','국수','라면','우동',\n",
        "               '볶음','탕','스테이크','버거','샐러드','밥','비빔','디저트','케이크','아이스','주스','티','차','빵',\n",
        "               '샌드위치','BBQ','꼬치','튀김','만두','키즈']\n",
        "    STORE_KW = ['카페','주막','BBQ','라그로타','담하','미라시아','연회장','포레스트릿','화담','카페테리아']\n",
        "    df = df.copy()\n",
        "    df['store_name'] = df['store_name'].astype(str).fillna('')\n",
        "    df['menu_name']  = df['menu_name'].astype(str).fillna('')\n",
        "    for w in MENU_KW:\n",
        "        df[f'kw_m_{w}'] = df['menu_name'].str.contains(w).astype(int)\n",
        "    for w in STORE_KW:\n",
        "        df[f'kw_s_{w}'] = df['store_name'].str.contains(w).astype(int)\n",
        "    df['menu_len'] = df['menu_name'].str.len().astype(int)\n",
        "    return df\n",
        "\n",
        "X_base = add_kw(X_base)\n",
        "X_feat = X_base.drop(columns=['store_name','menu_name']).copy()\n",
        "anchor = META['anchor_date']\n",
        "stores_for_rows = META['store_name'].values\n",
        "\n",
        "print(\"Supervised X:\", X_feat.shape, \"Y:\", Y.shape)\n",
        "\n",
        "# ---------------- Site/Store calendar profiles ----------------\n",
        "print(\"[Profiles] build Site/Store calendar indices (DOW & WOY) ...\")\n",
        "tf_ = train_full[['date','store','y']].copy()\n",
        "tf_['date'] = pd.to_datetime(tf_['date'])\n",
        "\n",
        "site_daily = tf_.groupby('date')['y'].sum().sort_index()\n",
        "site_mu = site_daily.mean()\n",
        "SITE_DOW_IDX = {d: (site_daily[site_daily.index.dayofweek==d].mean()/site_mu if site_mu>0 else 1.0) for d in range(7)}\n",
        "SITE_WOY_IDX = {}\n",
        "tmp = site_daily.to_frame('y')\n",
        "tmp['woy'] = tmp.index.isocalendar().week.astype(int)\n",
        "for w, g in tmp.groupby('woy'):\n",
        "    SITE_WOY_IDX[int(w)] = (g['y'].mean()/site_mu if site_mu>0 else 1.0)\n",
        "\n",
        "store_daily = tf_.groupby(['date','store'])['y'].sum().rename('y').reset_index()\n",
        "STORE_DOW_IDX = {}\n",
        "STORE_WOY_IDX = {}\n",
        "for s, g in store_daily.groupby('store'):\n",
        "    g = g.sort_values('date').copy()\n",
        "    mu = g['y'].mean()\n",
        "    if mu<=0:\n",
        "        for d in range(7): STORE_DOW_IDX[(s,d)] = 1.0\n",
        "        continue\n",
        "    g['dow'] = g['date'].dt.dayofweek\n",
        "    for d, gg in g.groupby('dow'):\n",
        "        STORE_DOW_IDX[(s,int(d))] = gg['y'].mean()/mu\n",
        "    g['woy'] = g['date'].dt.isocalendar().week.astype(int)\n",
        "    for w, gg in g.groupby('woy'):\n",
        "        STORE_WOY_IDX[(s,int(w))] = gg['y'].mean()/mu\n",
        "\n",
        "def horizon_calendar(anchor_dates: pd.Series, h: int) -> pd.DataFrame:\n",
        "    td = pd.to_datetime(anchor_dates) + pd.to_timedelta(h, unit='D') + pd.to_timedelta(1, unit='D')\n",
        "    dow = td.dt.dayofweek\n",
        "    mon = td.dt.month\n",
        "    woy = td.dt.isocalendar().week.astype(int)\n",
        "    hol = is_holiday(td)\n",
        "    hol_adj = ((td - pd.Timedelta(days=1)).dt.date.map(lambda d: int(d in K_HOLS)) |\n",
        "               (td + pd.Timedelta(days=1)).dt.date.map(lambda d: int(d in K_HOLS))).astype(int)\n",
        "    is_winter = td.map(_is_winter_date).astype(int)\n",
        "    season_id = td.map(_season_id_from_dt).astype(int)      # 0봄 1여름 2가을 3겨울\n",
        "    spring_autumn_act = td.map(_spring_autumn_active).astype(int)\n",
        "    summer_fam = td.map(_summer_family).astype(int)\n",
        "\n",
        "    dow_prior_vals = dow.map(dow_prior).astype(float)\n",
        "    mon_prior_vals = mon.map(month_prior).astype(float)\n",
        "    return pd.DataFrame({\n",
        "        f'dow_h{h}': dow.astype(int),\n",
        "        f'month_h{h}': mon.astype(int),\n",
        "        f'woy_h{h}': woy.astype(int),\n",
        "        f'is_weekend_h{h}': (dow>=5).astype(int),\n",
        "        f'is_holiday_h{h}': hol,\n",
        "        f'is_hol_adj_h{h}': hol_adj,\n",
        "        f'season_h{h}': mon.map(season_code).astype(int),\n",
        "        f'season_id_h{h}': season_id,\n",
        "        f'is_winter_h{h}': is_winter,\n",
        "        f'spring_autumn_active_h{h}': spring_autumn_act,\n",
        "        f'summer_family_h{h}': summer_fam,\n",
        "        f'is_winter_weekend_h{h}': (is_winter & (dow>=5)).astype(int),\n",
        "        f'dow_prior_h{h}': dow_prior_vals.values,\n",
        "        f'month_prior_h{h}': mon_prior_vals.values,\n",
        "    })\n",
        "\n",
        "def _profile_block(anchor_series: pd.Series, h: int, store_series: pd.Series) -> pd.DataFrame:\n",
        "    cal = horizon_calendar(anchor_series, h+1)\n",
        "    dow = cal[f'dow_h{h+1}'].values\n",
        "    woy = cal[f'woy_h{h+1}'].values\n",
        "    stores = store_series.values\n",
        "    site_dow = np.array([SITE_DOW_IDX.get(int(d),1.0) for d in dow], dtype=float)\n",
        "    site_woy = np.array([SITE_WOY_IDX.get(int(w),1.0) for w in woy], dtype=float)\n",
        "    store_dow= np.array([STORE_DOW_IDX.get((str(s), int(d)),1.0) for s,d in zip(stores,dow)], dtype=float)\n",
        "    store_woy= np.array([STORE_WOY_IDX.get((str(s), int(w)),1.0) for s,w in zip(stores,woy)], dtype=float)\n",
        "    return pd.DataFrame({\n",
        "        f'site_dow_idx_h{h+1}': site_dow,\n",
        "        f'site_woy_idx_h{h+1}': site_woy,\n",
        "        f'store_dow_idx_h{h+1}': store_dow,\n",
        "        f'store_woy_idx_h{h+1}': store_woy,\n",
        "    })\n",
        "\n",
        "def add_h_feats(Xb: pd.DataFrame, anchor_series: pd.Series, h: int, store_series: pd.Series):\n",
        "    cal = horizon_calendar(anchor_series, h+1)  # +1..+7\n",
        "    prof = _profile_block(anchor_series, h, store_series)\n",
        "    return pd.concat([Xb.reset_index(drop=True), cal.reset_index(drop=True), prof.reset_index(drop=True)], axis=1)\n",
        "\n",
        "# weighted SMAPE\n",
        "def smape_store_weighted(y_true, y_pred, stores_arr):\n",
        "    idx = y_true>0\n",
        "    if not np.any(idx): return 0.0\n",
        "    A = y_true[idx]; P = y_pred[idx]; S = stores_arr[idx]\n",
        "    sm = 2.0*np.abs(A-P)/(np.abs(A)+np.abs(P)+1e-8)\n",
        "    score=0.0; tot=0.0\n",
        "    for s in np.unique(S):\n",
        "        m = (S==s); w = store_weight(s)\n",
        "        score += w * sm[m].mean(); tot += w\n",
        "    return float(score/tot)\n",
        "\n",
        "def lgb_wsmape_feval(preds, dset):\n",
        "    y = dset.get_label()\n",
        "    w = dset.get_weight()\n",
        "    if w is None: w = np.ones_like(y)\n",
        "    mask = y>0\n",
        "    if not np.any(mask): return ('wSMAPE', 0.0, False)\n",
        "    s = 2.0*np.abs(y - preds) / (np.abs(y) + np.abs(preds) + 1e-8)\n",
        "    return ('wSMAPE', float(np.average(s[mask], weights=w[mask])), False)\n",
        "\n",
        "def xgb_wsmape_feval(preds, dtrain):\n",
        "    y = dtrain.get_label()\n",
        "    w = dtrain.get_weight()\n",
        "    if w is None or len(w)==0: w = np.ones_like(y)\n",
        "    mask = y>0\n",
        "    if not np.any(mask): return 'wSMAPE', 0.0\n",
        "    s = 2.0*np.abs(y - preds) / (np.abs(y) + np.abs(preds) + 1e-8)\n",
        "    return 'wSMAPE', float(np.average(s[mask], weights=w[mask]))\n",
        "\n",
        "# ---- XGBoost compatibility helpers ----\n",
        "def xgb_train_compat(params, dtrain, num_boost_round, evals, early_stopping_rounds=200, verbose_eval=200):\n",
        "    try:\n",
        "        return xgb.train(params, dtrain, num_boost_round=num_boost_round, evals=evals,\n",
        "                         feval=xgb_wsmape_feval, early_stopping_rounds=early_stopping_rounds,\n",
        "                         verbose_eval=verbose_eval)\n",
        "    except TypeError:\n",
        "        params2 = dict(params); params2.setdefault('eval_metric', 'rmse')\n",
        "        return xgb.train(params2, dtrain, num_boost_round=num_boost_round, evals=evals,\n",
        "                         early_stopping_rounds=early_stopping_rounds, verbose_eval=verbose_eval)\n",
        "\n",
        "def xgb_best_iteration(model):\n",
        "    bi = getattr(model, 'best_iteration', None)\n",
        "    if bi is not None: return int(bi)\n",
        "    bn = getattr(model, 'best_ntree_limit', None)\n",
        "    if bn is not None and bn > 0: return int(bn)\n",
        "    return None\n",
        "\n",
        "def xgb_predict_compat(model, dmatrix):\n",
        "    bi = xgb_best_iteration(model)\n",
        "    try:\n",
        "        if bi is not None: return model.predict(dmatrix, iteration_range=(0, bi))\n",
        "        else: return model.predict(dmatrix)\n",
        "    except TypeError:\n",
        "        bn = getattr(model, 'best_ntree_limit', None)\n",
        "        if bn is not None and bn > 0: return model.predict(dmatrix, ntree_limit=bn)\n",
        "        return model.predict(dmatrix)\n",
        "\n",
        "# ---------------- Model params (tuned) ----------------\n",
        "lgb_params = dict(\n",
        "    objective=\"poisson\",\n",
        "    learning_rate=0.045,\n",
        "    num_leaves=48,\n",
        "    feature_fraction=0.85,\n",
        "    bagging_fraction=0.75,\n",
        "    bagging_freq=1,\n",
        "    min_data_in_leaf=60,\n",
        "    max_depth=-1,\n",
        "    lambda_l1=1e-3,\n",
        "    lambda_l2=4e-2,\n",
        "    metric=\"None\",\n",
        "    n_estimators=3000,\n",
        "    verbosity=-1,\n",
        "    seed=SEED,\n",
        "    feature_fraction_seed=SEED,\n",
        "    bagging_seed=SEED,\n",
        "    deterministic=True,\n",
        "    force_row_wise=True\n",
        ")\n",
        "\n",
        "XGB_TREE_METHOD = 'hist'\n",
        "xgb_params_base = dict(\n",
        "    objective='reg:tweedie',\n",
        "    tweedie_variance_power=1.2,\n",
        "    eta=0.045,\n",
        "    max_depth=8,\n",
        "    min_child_weight=6,\n",
        "    subsample=0.85,\n",
        "    colsample_bytree=0.85,\n",
        "    reg_lambda=1.0,\n",
        "    reg_alpha=0.10,\n",
        "    tree_method=XGB_TREE_METHOD,\n",
        "    disable_default_eval_metric=1,\n",
        "    seed=SEED,\n",
        "    sampling_method='uniform',\n",
        "    max_bin=384,\n",
        "    monotone_constraints='()'\n",
        ")\n",
        "\n",
        "def get_cat_cols(cols):\n",
        "    cats = []\n",
        "    base_cats = ['store_le','menu_le']\n",
        "    hol_cats  = [c for c in cols if c.startswith((\n",
        "        'dow_h','month_h','woy_h','is_weekend_h','is_holiday_h','is_hol_adj_h',\n",
        "        'season_h','season_id_h','is_winter_h','is_winter_weekend_h',\n",
        "        'spring_autumn_active_h','summer_family_h'\n",
        "    ))]\n",
        "    kw_cats   = [c for c in cols if c.startswith(('kw_m_','kw_s_'))]\n",
        "    cats.extend(base_cats + hol_cats + kw_cats)\n",
        "    return sorted(list(set([c for c in cats if c in cols])))\n",
        "\n",
        "# ---------------- Train 7 heads with GroupKFold (LGB + XGB OOF) ----------------\n",
        "print(\"[Train] LGB + XGB heads with GROUP time-purged CV ...\")\n",
        "oof_lgb = np.zeros_like(Y, dtype=float)\n",
        "oof_xgb = np.zeros_like(Y, dtype=float)\n",
        "models_cv_lgb = []\n",
        "models_cv_xgb = []\n",
        "avg_iters_lgb=[]; avg_iters_xgb=[]\n",
        "\n",
        "META_store_series = META['store_name']\n",
        "\n",
        "# 그룹: 앵커 주차 × 매장\n",
        "groups = pd.to_datetime(META['anchor_date']).dt.to_period('W').astype(str) + \"_\" + META['store_name'].astype(str)\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "split_iter = list(gkf.split(X_feat, groups=groups))\n",
        "\n",
        "for h in range(7):\n",
        "    print(f\"  - Horizon +{h+1}d\")\n",
        "    yh = Y[:,h].astype(float).clip(min=0.0)\n",
        "    Xh = add_h_feats(X_feat, anchor, h, META_store_series)\n",
        "\n",
        "    cal_h = horizon_calendar(anchor, h+1)\n",
        "    is_weekend = cal_h[f'is_weekend_h{h+1}'].values\n",
        "    mon = cal_h[f'month_h{h+1}'].values\n",
        "    is_winter = cal_h[f'is_winter_h{h+1}'].values\n",
        "\n",
        "    # recency & weights\n",
        "    max_anchor = pd.to_datetime(anchor).max()\n",
        "    days_from_edge = (pd.to_datetime(anchor) - (max_anchor - pd.Timedelta(days=240))).dt.days.clip(lower=0, upper=240).values\n",
        "    rec = 0.5 + 0.8*(days_from_edge/240.0)**2\n",
        "\n",
        "    pos = yh[yh>0]\n",
        "    thr_low = np.quantile(pos, 0.25) if pos.size>50 else (pos.mean() if pos.size>0 else 0.0)\n",
        "    lowpos_bonus = np.where((yh>0) & (yh<=thr_low), 1.10, 1.0)\n",
        "\n",
        "    sw = np.array([store_weight(s) for s in stores_for_rows]) * rec * lowpos_bonus\n",
        "    sw = sw * (1.17**(is_weekend)) * (1.10**(pd.Series(mon).isin([12,1,2]).astype(int).values))\n",
        "    sw = sw * (1.12**(is_winter)) * (1.06**(is_winter & is_weekend))\n",
        "    sw = sw * (yh>0)\n",
        "\n",
        "    # 화담 성수기/여름 가중\n",
        "    spring_autumn_act = cal_h[f'spring_autumn_active_h{h+1}'].values.astype(int)\n",
        "    summer_fam = cal_h[f'summer_family_h{h+1}'].values.astype(int)\n",
        "    is_hwadam_arr = np.array([is_hwadam_store(s) for s in stores_for_rows]).astype(int)\n",
        "    summer_aff = np.array([STORE_SEASON_AFFINITY.get((s,'summer'),1.0) for s in stores_for_rows])\n",
        "    sw = sw * (1.05 ** (is_hwadam_arr & spring_autumn_act))\n",
        "    sw = sw * np.where((summer_fam==1) & (summer_aff>1.0), 1.03, 1.0)\n",
        "\n",
        "    cat_cols = get_cat_cols(Xh.columns.tolist())\n",
        "    fold_models_lgb=[]; fold_models_xgb=[];\n",
        "    oof_col_lgb=np.zeros_like(yh); oof_col_xgb=np.zeros_like(yh)\n",
        "\n",
        "    for fold,(tr,va) in enumerate(split_iter):\n",
        "        # LGB\n",
        "        dtr = lgb.Dataset(Xh.iloc[tr], label=yh[tr], weight=sw[tr], categorical_feature=cat_cols, free_raw_data=False)\n",
        "        dva = lgb.Dataset(Xh.iloc[va], label=yh[va], weight=sw[va], categorical_feature=cat_cols, free_raw_data=False)\n",
        "        callbacks=[lgb.early_stopping(200), lgb.log_evaluation(200)]\n",
        "        model_lgb = lgb.train(lgb_params, dtr, valid_sets=[dtr,dva], valid_names=['train','valid'],\n",
        "                              feval=lgb_wsmape_feval, num_boost_round=5000, callbacks=callbacks)\n",
        "        best_it_lgb = model_lgb.best_iteration or model_lgb.current_iteration()\n",
        "        avg_iters_lgb.append(best_it_lgb)\n",
        "        oof_col_lgb[va] = model_lgb.predict(Xh.iloc[va], num_iteration=best_it_lgb)\n",
        "        fold_models_lgb.append(model_lgb)\n",
        "\n",
        "        # XGB\n",
        "        xtr = xgb.DMatrix(Xh.iloc[tr], label=yh[tr], weight=sw[tr], feature_names=Xh.columns.tolist())\n",
        "        xva = xgb.DMatrix(Xh.iloc[va], label=yh[va], weight=sw[va], feature_names=Xh.columns.tolist())\n",
        "        params = dict(xgb_params_base); params['seed'] = SEED + fold + 100*h\n",
        "        evals = [(xtr,'train'), (xva,'valid')]\n",
        "        model_xgb = xgb_train_compat(params, xtr, num_boost_round=5000, evals=evals,\n",
        "                                     early_stopping_rounds=200, verbose_eval=200)\n",
        "        best_it_xgb = xgb_best_iteration(model_xgb)\n",
        "        avg_iters_xgb.append(best_it_xgb if best_it_xgb is not None else model_xgb.num_boosted_rounds())\n",
        "        oof_col_xgb[va] = xgb_predict_compat(model_xgb, xva)\n",
        "        fold_models_xgb.append(model_xgb)\n",
        "\n",
        "    oof_lgb[:,h] = oof_col_lgb\n",
        "    oof_xgb[:,h] = oof_col_xgb\n",
        "    models_cv_lgb.append((fold_models_lgb, Xh.columns.tolist(), cat_cols))\n",
        "    models_cv_xgb.append((fold_models_xgb, Xh.columns.tolist()))\n",
        "    gc.collect()\n",
        "\n",
        "# ---------------- OOF dynamic blend α(h,DOW) + q(h,DOW) ----------------\n",
        "stores_rep = np.repeat(stores_for_rows, 7)\n",
        "\n",
        "alpha_hd = {}  # (h,d) -> alpha for XGB\n",
        "for h in range(7):\n",
        "    y   = Y[:, h].astype(float)\n",
        "    pL  = oof_lgb[:, h].astype(float)\n",
        "    pX  = oof_xgb[:, h].astype(float)\n",
        "    dws = horizon_calendar(anchor, h+1)[f'dow_h{h+1}'].values\n",
        "    for d in range(7):\n",
        "        m = (y>0) & (dws==d)\n",
        "        if m.sum() < 300:\n",
        "            continue\n",
        "        best_s, best_a = 1e9, OOB_BLEND\n",
        "        a_grid = [0.30,0.35,0.40,0.45,0.50,0.55,0.60,0.65]\n",
        "        for a in a_grid:\n",
        "            if a > MAX_AX_SUM: continue\n",
        "            pr = pL[m]*(1-a) + pX[m]*a\n",
        "            s  = smape_store_weighted(y[m], pr, stores_for_rows[m])\n",
        "            if s < best_s:\n",
        "                best_s, best_a = s, a\n",
        "        alpha_hd[(h,d)] = 0.5*OOB_BLEND + 0.5*best_a  # 스무딩\n",
        "\n",
        "oof_blend = np.zeros_like(oof_lgb)\n",
        "for h in range(7):\n",
        "    pL  = oof_lgb[:, h].astype(float)\n",
        "    pX  = oof_xgb[:, h].astype(float)\n",
        "    dws = horizon_calendar(anchor, h+1)[f'dow_h{h+1}'].values\n",
        "    a_arr = np.array([alpha_hd.get((h, int(dd)), OOB_BLEND) for dd in dws], dtype=float)\n",
        "    a_arr = np.clip(a_arr, 0.0, MAX_AX_SUM)\n",
        "    oof_blend[:, h] = pL*(1.0 - a_arr) + pX*a_arr\n",
        "\n",
        "Q_HD = {}\n",
        "for h in range(7):\n",
        "    y  = Y[:, h].astype(float)\n",
        "    p  = oof_blend[:, h].astype(float)\n",
        "    dws= horizon_calendar(anchor, h+1)[f'dow_h{h+1}'].values\n",
        "    for d in range(7):\n",
        "        m = (y>0) & (dws==d)\n",
        "        if m.sum() < 300:\n",
        "            continue\n",
        "        r = np.median(y[m] / (p[m] + 1e-6))\n",
        "        Q_HD[(h,d)] = 0.60 if r > 1.03 else 0.50\n",
        "\n",
        "val_score = smape_store_weighted(Y.flatten(), oof_blend.flatten(), stores_rep)\n",
        "print(f\"[OOF] weighted SMAPE (LGB+XGB dyn α, OOB_BLEND={OOB_BLEND}) ≈ {val_score:.4f}\")\n",
        "print(f\"       tuned α buckets: {len(alpha_hd)} | q buckets: {len(Q_HD)}\")\n",
        "\n",
        "# ---------------- α(h,d,season) + q(h,d,season) 튜닝 (추가) ----------------\n",
        "alpha_hds = {}   # (h,d,season) -> alpha\n",
        "Q_HDS = {}       # (h,d,season) -> quantile\n",
        "for h in range(7):\n",
        "    y   = Y[:, h].astype(float); pL = oof_lgb[:, h]; pX = oof_xgb[:, h]\n",
        "    cal = horizon_calendar(anchor, h+1)\n",
        "    dws = cal[f'dow_h{h+1}'].values\n",
        "    sez = cal[f'season_id_h{h+1}'].values  # 0봄 1여름 2가을 3겨울\n",
        "    for d in range(7):\n",
        "        for g in range(4):\n",
        "            m = (y>0) & (dws==d) & (sez==g)\n",
        "            if m.sum() < 220:\n",
        "                continue\n",
        "            best_s, best_a = 1e9, OOB_BLEND\n",
        "            for a in [0.30,0.35,0.40,0.45,0.50,0.55,0.60,0.65]:\n",
        "                if a > MAX_AX_SUM: continue\n",
        "                pr = pL[m]*(1-a) + pX[m]*a\n",
        "                s  = smape_store_weighted(y[m], pr, stores_for_rows[m])\n",
        "                if s < best_s:\n",
        "                    best_s, best_a = s, a\n",
        "            alpha_hds[(h,d,g)] = 0.5*OOB_BLEND + 0.5*best_a\n",
        "            blend_m = pL[m]*(1-best_a) + pX[m]*best_a\n",
        "            r = np.median(y[m]/(blend_m + 1e-6))\n",
        "            Q_HDS[(h,d,g)] = 0.60 if r > 1.03 else 0.50\n",
        "print(f\"[Tune] α(h,d,season): {len(alpha_hds)}  q(h,d,season): {len(Q_HDS)}\")\n",
        "\n",
        "# ---------------- Horizon gamma/beta from blended OOF ----------------\n",
        "print(\"[Calibrate] gamma(h,dow) & beta(h,dow) from blended OOF ...\")\n",
        "eps=1e-6\n",
        "h_gamma = np.ones(7, dtype=float)\n",
        "h_beta  = np.zeros(7, dtype=float)\n",
        "gamma_hd = {}\n",
        "beta_hd  = {}\n",
        "\n",
        "for h in range(7):\n",
        "    y  = Y[:,h].astype(float)\n",
        "    p  = oof_blend[:,h].astype(float)\n",
        "    nz = y>0\n",
        "    if np.any(nz):\n",
        "        ratio = np.median(y[nz]/(p[nz]+eps))\n",
        "        h_gamma[h] = float(np.clip(ratio, 0.90, 1.08))\n",
        "    naive = X_feat[f'lag_{7-h}'].values.astype(float)\n",
        "    best_s, best_b = 1e9, 0.0\n",
        "    for b in [0.00,0.02,0.04,0.06,0.08,0.10,0.12,0.15,0.18,0.20]:\n",
        "        pr = p*h_gamma[h]*(1-b) + naive*b\n",
        "        s = smape_store_weighted(y, pr, stores_for_rows)\n",
        "        if s < best_s:\n",
        "            best_s, best_b = s, b\n",
        "    h_beta[h] = best_b\n",
        "\n",
        "    dows = horizon_calendar(anchor, h+1)[f'dow_h{h+1}'].values\n",
        "    for d in range(7):\n",
        "        m = (y>0) & (dows==d)\n",
        "        if m.sum() >= 200:\n",
        "            r = np.median(y[m]/(p[m]+eps))\n",
        "            gamma_hd[(h,d)] = float(np.clip(r, 0.88, 1.10))\n",
        "            n = naive[m]\n",
        "            best_s, best_b = 1e9, h_beta[h]\n",
        "            for b in [0.00,0.02,0.04,0.06,0.08,0.10,0.12,0.15,0.18]:\n",
        "                pr = p[m]*gamma_hd[(h,d)]*(1-b) + n*b\n",
        "                s = smape_store_weighted(y[m], pr, stores_for_rows[m])\n",
        "                if s < best_s:\n",
        "                    best_s, best_b = s, b\n",
        "            beta_hd[(h,d)] = best_b\n",
        "\n",
        "print(\"  gamma_h (fallback):\", np.round(h_gamma,3))\n",
        "print(\"  beta_h  (fallback):\", np.round(h_beta,3))\n",
        "print(f\"  gamma_hd keys: {len(gamma_hd)}  beta_hd keys: {len(beta_hd)}\")\n",
        "\n",
        "# ---------------- OOF-based calibration maps (store/*) ----------------\n",
        "print(\"[Calibrate] store / (store,dow) / (store,h) ratios from blended OOF ...\")\n",
        "all_dows=[]; h_idx=[]\n",
        "for h in range(7):\n",
        "    d = horizon_calendar(anchor, h+1)[f'dow_h{h+1}'].values\n",
        "    all_dows.append(d)\n",
        "    h_idx.append(np.full(Y.shape[0], h, dtype=int))\n",
        "all_dows = np.concatenate(all_dows)\n",
        "h_idx    = np.concatenate(h_idx)\n",
        "\n",
        "y_flat = Y.flatten(); p_flat = oof_blend.flatten()\n",
        "mask = y_flat>0\n",
        "y_pos, p_pos = y_flat[mask], p_flat[mask]\n",
        "s_pos = stores_rep[mask]\n",
        "dow_pos = all_dows[mask]\n",
        "h_pos = h_idx[mask]\n",
        "\n",
        "store_corr = {}; store_dow_corr = {}; store_h_corr = {}\n",
        "for s in np.unique(s_pos):\n",
        "    m = (s_pos==s)\n",
        "    if m.sum()>=50:\n",
        "        r = np.median(y_pos[m]/(p_pos[m]+eps))\n",
        "        store_corr[s] = float(np.clip(r, 0.75, 1.35))\n",
        "    else:\n",
        "        store_corr[s] = 1.0\n",
        "\n",
        "for s in np.unique(s_pos):\n",
        "    for d in range(7):\n",
        "        m = (s_pos==s) & (dow_pos==d)\n",
        "        if m.sum()>=30:\n",
        "            r = np.median(y_pos[m]/(p_pos[m]+eps))\n",
        "            store_dow_corr[(s,d)] = float(np.clip(r, 0.80, 1.30))\n",
        "\n",
        "for s in np.unique(s_pos):\n",
        "    for hh in range(7):\n",
        "        m = (s_pos==s) & (h_pos==hh)\n",
        "        if m.sum()>=30:\n",
        "            r = np.median(y_pos[m]/(p_pos[m]+eps))\n",
        "            store_h_corr[(s,hh)] = float(np.clip(r, 0.80, 1.25))\n",
        "\n",
        "print(\"[Calibrate] (store, winter_flag) & (store, weekend_or_holiday) ratios ...\")\n",
        "winter_flags=[]; wk_or_h_flags=[]\n",
        "for h in range(7):\n",
        "    cal = horizon_calendar(anchor, h+1)\n",
        "    winter_flags.append(cal[f'is_winter_h{h+1}'].values)\n",
        "    wk_or_h_flags.append((cal[f'is_weekend_h{h+1}'].values | cal[f'is_holiday_h{h+1}'].values).astype(int))\n",
        "winter_flags = np.concatenate(winter_flags)\n",
        "wk_or_h_flags = np.concatenate(wk_or_h_flags)\n",
        "\n",
        "winter_pos = winter_flags[mask]\n",
        "wk_pos  = wk_or_h_flags[mask]\n",
        "\n",
        "store_winter_corr = {}\n",
        "for s in np.unique(s_pos):\n",
        "    for flag in [0,1]:\n",
        "        m = (s_pos==s) & (winter_pos==flag)\n",
        "        if m.sum()>=30:\n",
        "            r = np.median(y_pos[m]/(p_pos[m]+eps))\n",
        "            store_winter_corr[(s,int(flag))] = float(np.clip(r, 0.80, 1.25))\n",
        "\n",
        "store_weekend_corr = {}\n",
        "for s in np.unique(s_pos):\n",
        "    for flag in [0,1]:\n",
        "        m = (s_pos==s) & (wk_pos==flag)\n",
        "        if m.sum()>=30:\n",
        "            r = np.median(y_pos[m]/(p_pos[m]+eps))\n",
        "            store_weekend_corr[(s,int(flag))] = float(np.clip(r, 0.80, 1.25))\n",
        "\n",
        "# ---------------- (store×season) OOF 보정 ----------------\n",
        "print(\"[Calibrate] (store, season_id) ratios from blended OOF ...\")\n",
        "all_season_ids=[]\n",
        "for h in range(7):\n",
        "    all_season_ids.append(horizon_calendar(anchor, h+1)[f'season_id_h{h+1}'].values)\n",
        "season_ids = np.concatenate(all_season_ids)\n",
        "g_pos = season_ids[mask]\n",
        "\n",
        "store_season_corr = {}\n",
        "for s in np.unique(s_pos):\n",
        "    for g in [0,1,2,3]:  # 0spring 1summer 2autumn 3winter\n",
        "        m = (s_pos==s) & (g_pos==g)\n",
        "        if m.sum()>=30:\n",
        "            r = np.median(y_pos[m]/(p_pos[m]+eps))\n",
        "            store_season_corr[(s,int(g))] = float(np.clip(r, 0.85, 1.20))\n",
        "print(\"  calibration maps ready.\")\n",
        "\n",
        "# ---------------- Winsorized caps & positive floors ----------------\n",
        "print(\"[Calibrate] winsor caps & positive floors ...\")\n",
        "caps = {}\n",
        "floor_sm = {}\n",
        "floor_s  = {}\n",
        "GLOBAL_POS_Q05 = 0.0\n",
        "\n",
        "train_full2 = train_full.copy()\n",
        "train_full2['dow'] = pd.to_datetime(train_full2['date']).dt.dayofweek\n",
        "train_full2['is_weekend'] = (train_full2['dow']>=5).astype(int)\n",
        "train_full2['is_hol'] = is_holiday(train_full2['date'])\n",
        "\n",
        "pos_all = train_full2.loc[train_full2['y']>0, 'y'].values\n",
        "if pos_all.size>0:\n",
        "    GLOBAL_POS_Q05 = float(np.quantile(pos_all, 0.05))\n",
        "\n",
        "for (s,m), g in train_full2.groupby(['store','menu']):\n",
        "    pos = g.loc[g['y']>0, 'y'].values\n",
        "    if pos.size>=20:\n",
        "        floor_sm[(s,m)] = float(np.quantile(pos, 0.05))\n",
        "\n",
        "for s, g in train_full2.groupby('store'):\n",
        "    pos = g.loc[g['y']>0, 'y'].values\n",
        "    if pos.size>=20:\n",
        "        floor_s[s] = float(np.quantile(pos, 0.05))\n",
        "\n",
        "for s, g1 in train_full2.groupby('store'):\n",
        "    for d, g2 in g1.groupby('dow'):\n",
        "        for flag, g3 in g2.groupby((g2['is_weekend'] | g2['is_hol']).astype(int)):\n",
        "            arr = g3['y'].values.astype(float)\n",
        "            if len(arr)>=50:\n",
        "                q95 = float(np.quantile(arr, 0.95))\n",
        "                q99 = float(np.quantile(arr, 0.99))\n",
        "                caps[(s,d,int(flag))] = (q95, q99)\n",
        "\n",
        "def get_floor(store, menu):\n",
        "    if (store, menu) in floor_sm: return floor_sm[(store, menu)]\n",
        "    if store in floor_s: return floor_s[store]\n",
        "    return GLOBAL_POS_Q05\n",
        "\n",
        "def apply_cap(store, dow, is_weekend, is_holiday, yhat):\n",
        "    flag = int((is_weekend==1) or (is_holiday==1))\n",
        "    q = caps.get((store,dow,flag))\n",
        "    if q is None: return yhat\n",
        "    q95,q99 = q\n",
        "    upper = (1.05 if flag else 1.00) * q99\n",
        "    if yhat > upper:\n",
        "        return float(0.7*upper + 0.3*yhat)\n",
        "    return yhat\n",
        "\n",
        "# ---------------- Retrain on FULL with multi-seed bagging (LGB+XGB) ----------------\n",
        "print(\"[Retrain] full data with multi-seed bagging (LGB+XGB) ...\")\n",
        "num_rounds_lgb = int(1.05*(np.mean(avg_iters_lgb) if len(avg_iters_lgb)>0 else 1500))\n",
        "num_rounds_xgb = int(1.05*(np.mean(avg_iters_xgb) if len(avg_iters_xgb)>0 else 1500))\n",
        "\n",
        "SEEDS = [42, 202, 404]\n",
        "\n",
        "final_models_lgb = []\n",
        "final_models_xgb = []\n",
        "\n",
        "for h in range(7):\n",
        "    yh = Y[:,h].astype(float).clip(min=0.0)\n",
        "    Xh = add_h_feats(X_feat, anchor, h, META_store_series)\n",
        "    cal_h = horizon_calendar(anchor, h+1)\n",
        "    is_weekend = cal_h[f'is_weekend_h{h+1}'].values\n",
        "    mon = cal_h[f'month_h{h+1}'].values\n",
        "    is_winter = cal_h[f'is_winter_h{h+1}'].values\n",
        "\n",
        "    max_anchor = pd.to_datetime(anchor).max()\n",
        "    days_from_edge = (pd.to_datetime(anchor) - (max_anchor - pd.Timedelta(days=240))).dt.days.clip(lower=0, upper=240).values\n",
        "    rec = 0.5 + 0.8*(days_from_edge/240.0)**2\n",
        "\n",
        "    pos = yh[yh>0]\n",
        "    thr_low = np.quantile(pos, 0.25) if pos.size>50 else (pos.mean() if pos.size>0 else 0.0)\n",
        "    lowpos_bonus = np.where((yh>0) & (yh<=thr_low), 1.10, 1.0)\n",
        "\n",
        "    sw = np.array([store_weight(s) for s in stores_for_rows]) * rec * lowpos_bonus\n",
        "    sw = sw * (1.17**(is_weekend)) * (1.10**(pd.Series(mon).isin([12,1,2]).astype(int).values))\n",
        "    sw = sw * (1.12**(is_winter)) * (1.06**(is_winter & is_weekend))\n",
        "    sw = sw * (yh>0)\n",
        "\n",
        "    spring_autumn_act = cal_h[f'spring_autumn_active_h{h+1}'].values.astype(int)\n",
        "    summer_fam = cal_h[f'summer_family_h{h+1}'].values.astype(int)\n",
        "    is_hwadam_arr = np.array([is_hwadam_store(s) for s in stores_for_rows]).astype(int)\n",
        "    summer_aff = np.array([STORE_SEASON_AFFINITY.get((s,'summer'),1.0) for s in stores_for_rows])\n",
        "    sw = sw * (1.05 ** (is_hwadam_arr & spring_autumn_act))\n",
        "    sw = sw * np.where((summer_fam==1) & (summer_aff>1.0), 1.03, 1.0)\n",
        "\n",
        "    cat_cols = get_cat_cols(Xh.columns.tolist())\n",
        "\n",
        "    # LGB multi-seed\n",
        "    models_h_lgb=[]\n",
        "    for sd in SEEDS:\n",
        "        params = dict(lgb_params); params['random_state']=sd\n",
        "        dtr = lgb.Dataset(Xh, label=yh, weight=sw, categorical_feature=cat_cols)\n",
        "        model = lgb.train(params, dtr, num_boost_round=num_rounds_lgb, feval=lgb_wsmape_feval,\n",
        "                          callbacks=[lgb.log_evaluation(250)])\n",
        "        models_h_lgb.append((model, Xh.columns.tolist(), cat_cols))\n",
        "    final_models_lgb.append(models_h_lgb)\n",
        "\n",
        "    # XGB multi-seed\n",
        "    models_h_xgb=[]\n",
        "    for sd in SEEDS:\n",
        "        params = dict(xgb_params_base); params['seed']=sd\n",
        "        dtr = xgb.DMatrix(Xh, label=yh, weight=sw, feature_names=Xh.columns.tolist())\n",
        "        model = xgb.train(params, dtr, num_boost_round=num_rounds_xgb, verbose_eval=250)\n",
        "        models_h_xgb.append((model, Xh.columns.tolist()))\n",
        "    final_models_xgb.append(models_h_xgb)\n",
        "    gc.collect()\n",
        "\n",
        "# ---------------- Inference helpers ----------------\n",
        "def weekly_naive_from_history(history28, h):\n",
        "    return float(history28[-(7 - h)])\n",
        "\n",
        "def infer_single_item(history28_raw, store_name, menu_name, last_date,\n",
        "                      history_dates=None, store_total_lookup=None,\n",
        "                      site_df_28=None, group_df_28=None, group_store_map=None):\n",
        "    history28 = np.clip(np.asarray(history28_raw, dtype=float), 0.0, None)\n",
        "    feats = build_feature_from_window(history28, np.asarray(history28_raw, dtype=float))\n",
        "\n",
        "    feats['store_prior'] = store_prior_index(store_name)\n",
        "    if (history_dates is not None) and (store_total_lookup is not None):\n",
        "        st28 = _lookup_store_total_series(store_name, pd.to_datetime(history_dates), store_total_lookup)\n",
        "        peer28 = np.clip(st28 - history28, 0.0, None)\n",
        "        feats['store_r7_mean']  = float(np.mean(st28[-7:]))\n",
        "        feats['store_r28_mean'] = float(np.mean(st28[-28:]))\n",
        "        feats['peer_r7_mean']   = float(np.mean(peer28[-7:]))\n",
        "        feats['peer_r28_mean']  = float(np.mean(peer28[-28:]))\n",
        "        feats['share_in_store_r7']  = float(history28[-7:].sum()/(st28[-7:].sum()+1e-6))\n",
        "        feats['share_in_store_r28'] = float(history28[-28:].sum()/(st28[-28:].sum()+1e-6))\n",
        "        # 동시 매장 모멘텀\n",
        "        def _mom(a,b): return float(a/(b+1e-6))\n",
        "        feats['store_mom_7_7'] = _mom(st28[-7:].mean(),  st28[-14:-7].mean())\n",
        "        feats['peer_mom_7_7']  = _mom(peer28[-7:].mean(), peer28[-14:-7].mean())\n",
        "    else:\n",
        "        feats['store_r7_mean']=feats['store_r28_mean']=0.0\n",
        "        feats['peer_r7_mean']=feats['peer_r28_mean']=0.0\n",
        "        feats['share_in_store_r7']=feats['share_in_store_r28']=0.0\n",
        "        feats['store_mom_7_7']=feats['peer_mom_7_7']=1.0\n",
        "\n",
        "    # meta 28일 롤링 (TEST는 파일별 28일만 사용)\n",
        "    if history_dates is not None:\n",
        "        mf = _meta_feats_from_dates(\n",
        "            dates_28=pd.to_datetime(history_dates),\n",
        "            store_name=store_name,\n",
        "            site_df=site_df_28 if site_df_28 is not None else pd.DataFrame(),\n",
        "            group_df=group_df_28,\n",
        "            group_store_map=group_store_map,\n",
        "            prefix='meta'\n",
        "        )\n",
        "        feats.update(mf)\n",
        "    else:\n",
        "        for c in ['site_group','site_hwadam','site_room','site_ski','temp_mean','temp_range','rain_sum','group_store']:\n",
        "            feats[f'meta_{c}_r7_mean']=0.0; feats[f'meta_{c}_r28_mean']=0.0; feats[f'meta_{c}_last']=0.0\n",
        "\n",
        "    # 가격 피처\n",
        "    key_name = f\"{store_name}_{menu_name}\"\n",
        "    p = PRICE_MAP.get(key_name, STORE_PRICE_MEAN.get(store_name, GLOBAL_PRICE_MEAN))\n",
        "    mu_s = STORE_PRICE_MEAN.get(store_name, GLOBAL_PRICE_MEAN if GLOBAL_PRICE_MEAN>0 else 1.0)\n",
        "    feats['avg_price']   = float(p if p is not None else 0.0)\n",
        "    feats['price_index'] = float((p/(mu_s+1e-6)) if p and mu_s else 1.0)\n",
        "\n",
        "    feats['store_name']=store_name; feats['menu_name']=menu_name\n",
        "    row = pd.DataFrame([feats])\n",
        "    row = add_kw(row)\n",
        "    s_in = store_name if store_name in le_store.classes_ else \"<UNK>\"\n",
        "    m_in = menu_name  if menu_name  in le_menu.classes_  else \"<UNK>\"\n",
        "    row['store_le'] = le_store.transform([s_in])[0]\n",
        "    row['menu_le']  = le_menu.transform([m_in])[0]\n",
        "    row = row.drop(columns=['store_name','menu_name'])\n",
        "\n",
        "    anchor_date = pd.to_datetime(last_date)\n",
        "    preds=[]\n",
        "    for h in range(7):\n",
        "        cal = horizon_calendar(pd.Series([anchor_date]), h+1)\n",
        "        prof = _profile_block(pd.Series([anchor_date]), h, pd.Series([store_name]))\n",
        "        Xh = pd.concat([row.reset_index(drop=True), cal.reset_index(drop=True), prof.reset_index(drop=True)], axis=1)\n",
        "\n",
        "        dow = int(cal[f'dow_h{h+1}'].iloc[0])\n",
        "        is_wk = int(cal[f'is_weekend_h{h+1}'].iloc[0])\n",
        "        is_h  = int(cal[f'is_holiday_h{h+1}'].iloc[0])\n",
        "        is_winter= int(cal[f'is_winter_h{h+1}'].iloc[0])\n",
        "        g_sid = int(cal[f'season_id_h{h+1}'].iloc[0])\n",
        "\n",
        "        q = Q_HDS.get((h, dow, g_sid), Q_HD.get((h, dow), 0.50))\n",
        "\n",
        "        seed_preds_lgb=[]; seed_preds_xgb=[]\n",
        "        for (model, cols, cat_cols) in final_models_lgb[h]:\n",
        "            Xuse = Xh.reindex(columns=cols, fill_value=0)\n",
        "            yhat = float(model.predict(Xuse)[0])\n",
        "            seed_preds_lgb.append(max(0.0, yhat))\n",
        "        for (model, cols) in final_models_xgb[h]:\n",
        "            Xuse = Xh.reindex(columns=cols, fill_value=0)\n",
        "            dm = xgb.DMatrix(Xuse, feature_names=cols)\n",
        "            yhat = float(xgb_predict_compat(model, dm)[0])\n",
        "            seed_preds_xgb.append(max(0.0, yhat))\n",
        "\n",
        "        y_lgb = float(np.quantile(seed_preds_lgb, q)) if seed_preds_lgb else 0.0\n",
        "        y_xgb = float(np.quantile(seed_preds_xgb, q)) if seed_preds_xgb else 0.0\n",
        "\n",
        "        a = alpha_hds.get((h, dow, g_sid), alpha_hd.get((h, dow), ALPHA_XGB))\n",
        "        a = float(np.clip(a, 0.0, MAX_AX_SUM))\n",
        "        yhat  = (1.0-a)*y_lgb + a*y_xgb\n",
        "\n",
        "        g = _shrink(gamma_hd.get((h,dow), h_gamma[h]), TAU['gamma'])\n",
        "        b = beta_hd.get((h,dow), h_beta[h])\n",
        "\n",
        "        naive = weekly_naive_from_history(history28, h)\n",
        "        if naive <= 1e-6: b = 0.0\n",
        "\n",
        "        nz_idx = np.where(np.asarray(history28) > 0)[0]\n",
        "        days_since_last_nz = 28 - (nz_idx[-1]+1) if len(nz_idx)>0 else 28\n",
        "        if days_since_last_nz >= 21: b *= 0.75\n",
        "        elif days_since_last_nz >= 14: b *= 0.90\n",
        "\n",
        "        yhat = yhat * g * (1.0 - b) + naive * b\n",
        "\n",
        "        # 시즌/어피니티 보정\n",
        "        m_season = _shrink(store_season_corr.get((store_name, g_sid), 1.0), TAU['season'])\n",
        "        g_name = {0:'spring',1:'summer',2:'autumn',3:'winter'}[g_sid]\n",
        "        m_aff = _shrink(STORE_SEASON_AFFINITY.get((store_name, g_name), 1.0), TAU['aff'])\n",
        "        if is_hwadam_store(store_name) and int(cal[f'spring_autumn_active_h{h+1}'].iloc[0])==1:\n",
        "            m_season *= 1.02\n",
        "        m_season = float(np.clip(m_season * m_aff, 0.90, 1.15))\n",
        "        yhat *= m_season\n",
        "\n",
        "        m_store = _shrink(store_corr.get(store_name, 1.0), TAU['store'])\n",
        "        m_sdow  = _shrink(store_dow_corr.get((store_name, dow), 1.0), TAU['sdow'])\n",
        "        m_sh    = _shrink(store_h_corr.get((store_name, h), 1.0), TAU['sh'])\n",
        "        m_winter   = _shrink(store_winter_corr.get((store_name, is_winter), 1.0), TAU['winter'])\n",
        "        m_wk    = _shrink(store_weekend_corr.get((store_name, int(is_wk or is_h)), 1.0), TAU['wk'])\n",
        "        mult = m_store * m_sdow * m_sh * m_winter * m_wk\n",
        "        yhat = yhat * mult\n",
        "\n",
        "        floor_val = get_floor(store_name, menu_name)\n",
        "        if floor_val > 0: yhat = max(yhat, floor_val)\n",
        "        yhat = apply_cap(store_name, dow, is_wk, is_h, yhat)\n",
        "\n",
        "        preds.append(max(0.0, yhat))\n",
        "    return np.array(preds, dtype=float)\n",
        "\n",
        "# ---------------- Build submission ----------------\n",
        "print(\"[Predict] build submission from TEST files ...\")\n",
        "pred_rows=[]\n",
        "for tfp in TEST_FILES:\n",
        "    tid = int(re.findall(r'TEST_(\\d+)\\.csv', os.path.basename(tfp))[0])\n",
        "    tdf = pd.read_csv(tfp)\n",
        "    tdf = _clean_columns(tdf); tdf = _force_kor_cols(tdf); tdf = _normalize_key_cols(tdf)\n",
        "    tdf = tdf.rename(columns={\"영업일자\":\"date\",\"영업장명_메뉴명\":\"key\",\"매출수량\":\"y_raw\"})\n",
        "    tdf[\"date\"] = pd.to_datetime(tdf[\"date\"])\n",
        "    tdf[\"store\"], tdf[\"menu\"] = zip(*tdf[\"key\"].map(split_store_menu))\n",
        "\n",
        "    # TEST 파일 내 28일 (store,date) 합계로 피어 집계 생성\n",
        "    tdf['_yc'] = tdf['y_raw'].clip(lower=0.0)\n",
        "    test_store_total_ser = tdf.groupby(['store','date'])['_yc'].sum().sort_index()\n",
        "\n",
        "    # TEST meta 로더: 이 파일의 28일만\n",
        "    TMB = _load_test_meta_bundle(tid)\n",
        "    site_df_28 = TMB['site_df'] if (TMB and ('site_df' in TMB)) else pd.DataFrame()\n",
        "    group_df_28= TMB['group_df'] if (TMB and ('group_df' in TMB)) else None\n",
        "\n",
        "    # group_store_map은 TRAIN 기준으로 고정(명칭 매칭)\n",
        "    group_store_map = TRAIN_META['group_store_map']\n",
        "\n",
        "    for name,g in tdf.groupby(\"key\"):\n",
        "        g = g.sort_values(\"date\")\n",
        "        vals_raw = g[\"y_raw\"].values.astype(float)\n",
        "        dates_28 = g[\"date\"].values\n",
        "        assert len(vals_raw)==28, f\"{name} in {tfp} is not 28 days\"\n",
        "        store,menu = split_store_menu(name)\n",
        "        preds = infer_single_item(\n",
        "            vals_raw, store, menu, g[\"date\"].max(),\n",
        "            history_dates=dates_28,\n",
        "            store_total_lookup=test_store_total_ser,\n",
        "            site_df_28=site_df_28,\n",
        "            group_df_28=group_df_28,\n",
        "            group_store_map=group_store_map\n",
        "        )\n",
        "        for h in range(7):\n",
        "            pred_rows.append({\n",
        "                \"영업일자\": f\"TEST_{tid:02d}+{h+1}일\",\n",
        "                \"영업장명_메뉴명\": name,\n",
        "                \"매출수량\": float(preds[h])\n",
        "            })\n",
        "\n",
        "pred_df = pd.DataFrame(pred_rows)\n",
        "pred_df = _clean_columns(pred_df)\n",
        "pred_df = _force_kor_cols(pred_df)\n",
        "pred_df = _normalize_key_cols(pred_df)\n",
        "\n",
        "pred_wide_clean = pred_df.pivot_table(index='영업일자',\n",
        "                                      columns='영업장명_메뉴명',\n",
        "                                      values='매출수량',\n",
        "                                      aggfunc='first').reset_index()\n",
        "\n",
        "raw_sample = pd.read_csv(SAMPLE_SUB_PATH)\n",
        "raw_cols = raw_sample.columns.tolist()\n",
        "raw_dates = raw_sample['영업일자'].astype(str)\n",
        "\n",
        "def _canon(s: str) -> str:\n",
        "    return (unicodedata.normalize('NFKC', str(s))\n",
        "            .replace('\\ufeff','').replace('\\u200b','').replace('\\xa0','').strip())\n",
        "clean_cols_target = ['영업일자'] + [_canon(c) for c in raw_cols if c != '영업일자']\n",
        "clean_to_raw = { _canon(c): c for c in raw_cols }\n",
        "\n",
        "submission_clean = pd.DataFrame({'영업일자': raw_dates})\n",
        "submission_clean = submission_clean.merge(pred_wide_clean, on='영업일자', how='left')\n",
        "\n",
        "for c in clean_cols_target:\n",
        "    if c not in submission_clean.columns and c != '영업일자':\n",
        "        submission_clean[c] = 0.0\n",
        "extra_cols = [c for c in submission_clean.columns if c not in clean_cols_target]\n",
        "if extra_cols:\n",
        "    submission_clean = submission_clean.drop(columns=extra_cols, errors='ignore')\n",
        "submission_clean = submission_clean[clean_cols_target]\n",
        "\n",
        "submission_final = submission_clean.rename(columns=clean_to_raw)\n",
        "submission_final = submission_final[raw_cols]\n",
        "if submission_final.isna().any().any():\n",
        "    print(\"[Warn] NaN detected in submission; filling 0.0\")\n",
        "    submission_final = submission_final.fillna(0.0)\n",
        "\n",
        "submission_final.to_csv(OUT_PATH, index=False, encoding='utf-8-sig')\n",
        "print(f\"[DONE] saved: {OUT_PATH}\")\n",
        "print(submission_final.head(3))\n",
        "\n",
        "s2 = pd.read_csv(SAMPLE_SUB_PATH)\n",
        "print('[Check] same columns? ', set(submission_final.columns) == set(s2.columns))\n",
        "print('[Check] same order?   ', list(submission_final.columns) == list(s2.columns))\n",
        "print('[Check] same dates?   ', submission_final['영업일자'].tolist() == s2['영업일자'].tolist())\n",
        "print('[Check] NaNs count:   ', int(submission_final.isna().sum().sum()))\n",
        "print('[Check] shape:        ', submission_final.shape, ' / sample:', s2.shape)"
      ]
    }
  ]
}