{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#LG Aimers 7기 식음업장 메뉴 수요 예측 AI 온라인 해커톤  \n",
        "#private 22등 / 수달팀\n",
        "#최종 제출본  "
      ],
      "metadata": {
        "id": "pbnV-jQeImE-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "실행환경 : Google Colab CPU / 고용량 RAM 환경(꼭 CPU)  \n",
        "\n",
        "colab 사용한 PC사양:  \n",
        "CPU : Ryzen7 5700x  \n",
        "GPU : GTX1660 Super(4gb)  \n",
        "\n",
        "데이터는 코랩 런타임에 넣어 실행하였음."
      ],
      "metadata": {
        "id": "KatH39M775t3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "라이브러리 버전  \n",
        "lightgbm == 4.6.0  \n",
        "xgboost == 3.0.4  \n",
        "numpy == 2.0.2  \n",
        "pandas == 2.2.2  \n",
        "tensorflow == 2.19.0  \n",
        "scikit-learn == 1.6.1  "
      ],
      "metadata": {
        "id": "4mevZAHQ7uNw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import"
      ],
      "metadata": {
        "id": "bB0Pm_PdzQho"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMLZezOdzLxA"
      },
      "outputs": [],
      "source": [
        "import os, glob, re, math, gc, warnings, random, unicodedata\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# TensorFlow 설치확인\n",
        "TF_OK = False\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    from tensorflow import keras\n",
        "    from tensorflow.keras import layers\n",
        "    TF_OK = True\n",
        "    print(\"[Info] TensorFlow detected. LSTM head ENABLED.\")\n",
        "except Exception as e:\n",
        "    print(\"[Info] TensorFlow not available. LSTM head DISABLED.\", e)\n",
        "\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Repro & blend knobs"
      ],
      "metadata": {
        "id": "246mucc6zTjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED)\n",
        "if TF_OK:\n",
        "    tf.random.set_seed(SEED)\n",
        "\n",
        "# OOF 보정은 튜닝된 α/τ(h,DOW) 블렌드 예측에 맞춤 (기본 수축 기준값)\n",
        "OOB_BLEND = 0.40   # OOF에서 XGB 글로벌 비중(튜닝·수축의 기준점)\n",
        "OOB_TAU   = 0.10   # OOF에서 LSTM 글로벌 비중(튜닝·수축의 기준점, 소폭)\n",
        "\n",
        "# 최종 추론에서 XGB/LSTM 글로벌 비중(튜닝·수축의 기준점)\n",
        "ALPHA_XGB = 0.40\n",
        "TAU_LSTM  = 0.10\n",
        "\n",
        "# α+τ 상한(과도한 트리·딥 비중을 방지)\n",
        "MAX_AX_SUM = 0.70"
      ],
      "metadata": {
        "id": "ZjmVpwzIzVuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "path"
      ],
      "metadata": {
        "id": "yPkL1JaYzZr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR = './data' if os.path.exists('./data/train.csv') else './'\n",
        "TRAIN_PATH = os.path.join(DATA_DIR, 'train.csv')\n",
        "SAMPLE_SUB_PATH = os.path.join(DATA_DIR, 'sample_submission.csv')\n",
        "TEST_FILES = sorted(glob.glob(os.path.join(DATA_DIR, 'TEST_*.csv')))\n",
        "OUT_PATH = os.path.join(DATA_DIR, 'submission.csv')\n",
        "\n",
        "print(f\"[Path] DATA_DIR = {DATA_DIR}\")"
      ],
      "metadata": {
        "id": "oq5M1gvlzXLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utils"
      ],
      "metadata": {
        "id": "EXWMIREKzbtd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _canon_text(s: str) -> str:\n",
        "    if s is None: return ''\n",
        "    s = unicodedata.normalize('NFKC', str(s))\n",
        "    return s.replace('\\ufeff','').replace('\\u200b','').replace('\\xa0','').strip()\n",
        "\n",
        "def _clean_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    return df.rename(columns={c:_canon_text(c) for c in df.columns})\n",
        "\n",
        "def _force_kor_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    m = { 'date':'영업일자', 'key':'영업장명_메뉴명', 'y':'매출수량' }\n",
        "    return df.rename(columns={k:v for k,v in m.items() if k in df.columns})\n",
        "\n",
        "def _normalize_key_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    if '영업장명_메뉴명' in df.columns:\n",
        "        df['영업장명_메뉴명'] = df['영업장명_메뉴명'].map(_canon_text)\n",
        "    return df"
      ],
      "metadata": {
        "id": "gtc7lVfizc5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calibration shrink"
      ],
      "metadata": {
        "id": "NpsQziuszfin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _shrink(x, tau=0.5):\n",
        "    return 1.0 + float(tau) * (float(x) - 1.0)\n",
        "\n",
        "TAU = dict(\n",
        "    gamma=1.05,  # γ는 살짝 과감히(다운 바이어스 완화)\n",
        "    store=0.40,\n",
        "    sdow=0.40,\n",
        "    sh=0.40,\n",
        "    winter=0.50,\n",
        "    wk=0.25,\n",
        "    season=0.30,\n",
        "    aff=0.20\n",
        ")"
      ],
      "metadata": {
        "id": "mEryy1p7zfIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Domain: stores/holidays"
      ],
      "metadata": {
        "id": "61OW7VSYzlB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HEAVY_STORES = {'담하','미라시아'}\n",
        "HEAVY_W = 1.35  # (담하/미라시아 중요도 상향 : 공개데이터)\n",
        "def store_weight(name: str) -> float:\n",
        "    for s in HEAVY_STORES:\n",
        "        if str(name).startswith(s):\n",
        "            return HEAVY_W\n",
        "    return 1.0\n",
        "\n",
        "# 화담매장 feature helpers\n",
        "def is_hwadam_store(s: str) -> bool:\n",
        "    s = str(s)\n",
        "    return s.startswith('화담숲주막') or s.startswith('화담숲카페')\n",
        "\n",
        "def _season_id_from_dt(dt: pd.Timestamp) -> int:\n",
        "    \"\"\"0: spring(3~5), 1: summer(6~8), 2: autumn(9~11), 3: winter(12~2)\"\"\"\n",
        "    m = int(dt.month)\n",
        "    return 1 if 6<=m<=8 else (2 if 9<=m<=11 else (0 if 3<=m<=5 else 3))\n",
        "\n",
        "def _spring_autumn_active(dt: pd.Timestamp) -> int:\n",
        "    \"\"\" 봄/가을 (4–6, 9–11)\"\"\"\n",
        "    m = int(dt.month)\n",
        "    return int(m in [4,5,6,9,10,11])\n",
        "\n",
        "def _summer_family(dt: pd.Timestamp) -> int:\n",
        "    \"\"\"여름 시즌 (방학·휴가)\"\"\"\n",
        "    return int(int(dt.month) in [7,8])\n",
        "\n",
        "# 업장-계절 도메인 prior(train데이터 분석)\n",
        "STORE_SEASON_AFFINITY = {\n",
        "    ('화담숲주막','spring'): 1.15, ('화담숲주막','autumn'): 1.20,\n",
        "    ('화담숲카페','spring'): 1.15, ('화담숲카페','autumn'): 1.20,\n",
        "    ('미라시아','summer'): 1.08, ('느티나무 셀프BBQ','summer'): 1.10,\n",
        "}\n",
        "\n",
        "K_HOLS = set(pd.to_datetime([\n",
        "    # 2023\n",
        "    \"2023-01-01\",\"2023-01-21\",\"2023-01-22\",\"2023-01-23\",\"2023-01-24\",\n",
        "    \"2023-03-01\",\"2023-05-05\",\"2023-05-27\",\"2023-06-06\",\"2023-08-15\",\n",
        "    \"2023-09-28\",\"2023-09-29\",\"2023-09-30\",\"2023-10-03\",\"2023-10-09\",\n",
        "    \"2023-12-25\",\n",
        "    # 2024\n",
        "    \"2024-01-01\",\"2024-02-09\",\"2024-02-10\",\"2024-02-11\",\"2024-02-12\",\n",
        "    \"2024-03-01\",\"2024-05-05\",\"2024-05-06\",\"2024-05-15\",\"2024-06-06\",\n",
        "    \"2024-08-15\",\"2024-09-16\",\"2024-09-17\",\"2024-09-18\",\"2024-10-03\",\n",
        "    \"2024-10-09\",\"2024-12-25\",\n",
        "    # 2025\n",
        "    \"2025-01-01\",\"2025-01-27\",\"2025-01-28\",\"2025-01-29\",\n",
        "    \"2025-03-01\",\"2025-05-05\",\"2025-05-06\",\"2025-06-06\",\"2025-08-15\",\n",
        "    \"2025-10-03\",\"2025-10-06\",\"2025-10-07\",\"2025-10-08\",\"2025-10-09\",\n",
        "    \"2025-12-25\",\n",
        "]).date)\n",
        "\n",
        "def is_holiday(ts) -> pd.Series:\n",
        "    td = pd.to_datetime(ts)\n",
        "    if not isinstance(td, pd.Series):\n",
        "        td = pd.Series(td)\n",
        "    return td.dt.date.map(lambda d: int(d in K_HOLS)).astype(int)\n",
        "\n",
        "def season_code(m: int) -> int:\n",
        "    if m in [12,1,2]: return 0   # 겨울=0, 봄=1, 여름=2, 가을=3\n",
        "    if m in [3,4,5]:  return 1\n",
        "    if m in [6,7,8]:  return 2\n",
        "    return 3\n",
        "\n",
        "def _is_winter_date(dt: pd.Timestamp) -> int:\n",
        "    m = dt.month; d = dt.day\n",
        "    return int((m==12 and d>=10) or (m in [1,2]) or (m==3 and d<=10))"
      ],
      "metadata": {
        "id": "G4Y_X09czmry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Store/menu ecosystem priors"
      ],
      "metadata": {
        "id": "EbuohHXK0a71"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#train데이터에서 업장별 총/평균 일 매출수량을 집계 후, 전체 평균으로 나눈 상대지수에서 개별 조정\n",
        "_STORE_PRIOR_RAW = {\n",
        "    '포레스트릿': 47.84,\n",
        "    '화담숲주막': 34.38,\n",
        "    '카페테리아': 18.86,\n",
        "    '담하': 5.50,\n",
        "    '미라시아': 5.50,\n",
        "    '라그로타': 1.31\n",
        "}\n",
        "_sp_mean = float(np.mean(list(_STORE_PRIOR_RAW.values()))) if _STORE_PRIOR_RAW else 1.0\n",
        "STORE_PRIOR = {k: (v/_sp_mean if _sp_mean>0 else 1.0) for k,v in _STORE_PRIOR_RAW.items()}\n",
        "def store_prior_index(store: str) -> float:\n",
        "    return float(STORE_PRIOR.get(str(store), 1.0))"
      ],
      "metadata": {
        "id": "_cC1Uuev0Ymk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time-pattern priors (월/요일)"
      ],
      "metadata": {
        "id": "sIsZkHjN0dh0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#train데이터에서 뽑은 월/요일별 평균 수요 편향을 약한 사전값(개별조정) -> 캘린더 효과 방향성\n",
        "_MONTH_PRIOR_RAW = {1:21.64, 2:17.37, 3:2.64, 10:15.47, 12:13.71}\n",
        "_m_mean = float(np.mean(list(_MONTH_PRIOR_RAW.values()))) if _MONTH_PRIOR_RAW else 1.0\n",
        "MONTH_PRIOR = {m: (v/_m_mean if _m_mean>0 else 1.0) for m,v in _MONTH_PRIOR_RAW.items()}\n",
        "\n",
        "_DOW_PRIOR_RAW = {0:7.76, 4:12.26, 5:15.28, 6:12.72}\n",
        "_d_mean = float(np.mean(list(_DOW_PRIOR_RAW.values()))) if _DOW_PRIOR_RAW else 1.0\n",
        "DOW_PRIOR = {d: (v/_d_mean if _d_mean>0 else 1.0) for d,v in _DOW_PRIOR_RAW.items()}\n",
        "\n",
        "def month_prior(m: int) -> float:\n",
        "    return float(MONTH_PRIOR.get(int(m), 1.0))\n",
        "\n",
        "def dow_prior(d: int) -> float:\n",
        "    return float(DOW_PRIOR.get(int(d), 1.0))"
      ],
      "metadata": {
        "id": "eg06QwVX0foB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "General helpers"
      ],
      "metadata": {
        "id": "GOqe-XmZ1Fxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_store_menu(x: str):\n",
        "    x = str(x)\n",
        "    if \"_\" in x:\n",
        "        p = x.find(\"_\"); return x[:p], x[p+1:]\n",
        "    return \"UNKNOWN\", x\n",
        "\n",
        "def ensure_full_daily_index_multi(df, value_cols):\n",
        "    df = df.copy()\n",
        "    df['영업일자'] = pd.to_datetime(df['영업일자'])\n",
        "    out=[]\n",
        "    for name,g in df.groupby('영업장명_메뉴명'):\n",
        "        g = g.sort_values('영업일자')\n",
        "        idx = pd.date_range(g['영업일자'].min(), g['영업일자'].max(), freq=\"D\")\n",
        "        gg = g.set_index('영업일자').reindex(idx).rename_axis('영업일자').reset_index()\n",
        "        gg['영업장명_메뉴명'] = name\n",
        "        for c in value_cols:\n",
        "            gg[c] = gg[c].fillna(0.0)\n",
        "        out.append(gg)\n",
        "    return pd.concat(out, ignore_index=True)\n",
        "\n",
        "def build_feature_from_window(y28: np.ndarray, y28_raw: np.ndarray):\n",
        "    f={}\n",
        "    y = np.clip(y28.astype(float), 0.0, None)\n",
        "    yr = y28_raw.astype(float)\n",
        "\n",
        "    for i in range(28):\n",
        "        f[f'lag_{i+1}'] = float(y[-(i+1)])\n",
        "\n",
        "    def stats(prefix, arr):\n",
        "        f[f'{prefix}_mean'] = float(arr.mean())\n",
        "        f[f'{prefix}_std']  = float(arr.std(ddof=0))\n",
        "        f[f'{prefix}_max']  = float(arr.max())\n",
        "        f[f'{prefix}_sum']  = float(arr.sum())\n",
        "        f[f'{prefix}_zero_share'] = float((arr==0).mean())\n",
        "        f[f'{prefix}_nz_share']   = float((arr>0).mean())\n",
        "    stats('r7',  y[-7:])\n",
        "    stats('r14', y[-14:])\n",
        "    stats('r28', y[-28:])\n",
        "\n",
        "    a7 = y[-7:].mean(); a14 = y[-14:-7].mean() if y[-14:-7].size>0 else 0.0\n",
        "    f['mom_7_7']  = float(a7/(a14+1e-6))\n",
        "    f['mom_7_28'] = float(a7/(y.mean()+1e-6))\n",
        "    x = np.arange(1,29); ylog = np.log1p(y)\n",
        "    denom = np.sum((x-x.mean())**2); slope = 0.0 if denom==0 else np.sum((x-x.mean())*(ylog-ylog.mean()))/denom\n",
        "    f['trend_slope'] = float(slope)\n",
        "\n",
        "    nz_idx = np.where(y>0)[0]\n",
        "    f['days_since_last_nz'] = float(28 - (nz_idx[-1]+1) if len(nz_idx)>0 else 28)\n",
        "    f['last_nz_val'] = float(y[nz_idx[-1]] if len(nz_idx)>0 else 0.0)\n",
        "\n",
        "    neg_mask = (yr < 0)\n",
        "    f['neg_cnt_7']  = float(neg_mask[-7:].sum())\n",
        "    f['neg_cnt_14'] = float(neg_mask[-14:].sum())\n",
        "    f['neg_cnt_28'] = float(neg_mask[-28:].sum())\n",
        "    f['neg_sum_abs_28'] = float(np.abs(yr[neg_mask][-28:]).sum()) if neg_mask.any() else 0.0\n",
        "    f['neg_min_28'] = float(yr[-28:].min())\n",
        "    f['neg_share_28'] = float((yr[-28:]<0).mean())\n",
        "    neg_idx = np.where(yr<0)[0]\n",
        "    f['days_since_last_neg'] = float(28 - (neg_idx[-1]+1) if len(neg_idx)>0 else 28)\n",
        "    return f"
      ],
      "metadata": {
        "id": "EFsi2Y_F1GHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load"
      ],
      "metadata": {
        "id": "PMy6Cem81LUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"[Load] {TRAIN_PATH}\")\n",
        "train_raw = pd.read_csv(TRAIN_PATH)\n",
        "train_raw = _clean_columns(train_raw); train_raw = _force_kor_cols(train_raw)\n",
        "train_raw = train_raw.rename(columns={\"영업일자\":\"date\",\"영업장명_메뉴명\":\"key\",\"매출수량\":\"y_raw\"})\n",
        "train_raw[\"date\"] = pd.to_datetime(train_raw[\"date\"])\n",
        "train_raw[\"y_raw\"] = train_raw[\"y_raw\"].astype(float)\n",
        "\n",
        "train = train_raw.copy()\n",
        "train[\"y\"] = train[\"y_raw\"].clip(lower=0.0)\n",
        "train[\"store\"], train[\"menu\"] = zip(*train[\"key\"].map(split_store_menu))\n",
        "\n",
        "print(\"Train:\", train.shape, \"| Period:\", train[\"date\"].min().date(), \"~\", train[\"date\"].max().date())\n",
        "print(\"Unique series:\", train[\"key\"].nunique(), \"| Stores:\", train[\"store\"].nunique())\n",
        "print(f\"Zero share: {(train['y']==0).mean():.3f}\")\n",
        "\n",
        "train_full = ensure_full_daily_index_multi(\n",
        "    train[[\"date\",\"key\",\"y\",\"y_raw\"]].rename(columns={\"date\":\"영업일자\",\"key\":\"영업장명_메뉴명\"}),\n",
        "    value_cols=[\"y\",\"y_raw\"]\n",
        ")\n",
        "train_full = train_full.rename(columns={\"영업일자\":\"date\",\"영업장명_메뉴명\":\"key\"})\n",
        "train_full[\"store\"], train_full[\"menu\"] = zip(*train_full[\"key\"].map(split_store_menu))"
      ],
      "metadata": {
        "id": "U75mYckx1J3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Store totals (for peer/ecosystem)"
      ],
      "metadata": {
        "id": "ZOZGpRLV1OQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_store_tot_ser = train_full.groupby(['date','store'])['y'].sum()\n",
        "STORE_TOTAL_SER = _store_tot_ser.rename_axis(['date','store']).swaplevel(0,1).sort_index()\n",
        "\n",
        "def _lookup_store_total_series(store_name, dates, ser):\n",
        "    idx = pd.MultiIndex.from_arrays(\n",
        "        [np.array([store_name]*len(dates), dtype=object), pd.to_datetime(dates)],\n",
        "        names=['store','date']\n",
        "    )\n",
        "    return ser.reindex(idx, fill_value=0.0).values"
      ],
      "metadata": {
        "id": "yAUXCqoh1Qn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build windows (Tabular + Seq for LSTM)"
      ],
      "metadata": {
        "id": "FIxbkDlH1VC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"[Windows] 28->7 supervised windows ...\")\n",
        "\n",
        "SEQ_FEAT_NAMES = ['y_clip', 'store_tot', 'peer_tot', 'dow_norm', 'is_weekend', 'is_holiday', 'is_winter']\n",
        "\n",
        "def make_supervised_windows(df_item, store_name, menu_name):\n",
        "    vals     = df_item['y'].values.astype(float)\n",
        "    vals_raw = df_item['y_raw'].values.astype(float)\n",
        "    dates    = pd.to_datetime(df_item['영업일자']).values\n",
        "    st28_all = _lookup_store_total_series(store_name, dates, STORE_TOTAL_SER)\n",
        "\n",
        "    X_rows=[]; Y_rows=[]; meta=[]; SEQ_rows=[]\n",
        "    if len(vals)<35: return X_rows, Y_rows, meta, SEQ_rows\n",
        "    for end in range(27, len(vals)-7):\n",
        "        y28  = vals[end-27:end+1]\n",
        "        y28r = vals_raw[end-27:end+1]\n",
        "        st28 = st28_all[end-27:end+1]\n",
        "        peer28 = np.clip(st28 - y28, 0.0, None)\n",
        "        dts = pd.to_datetime(dates[end-27:end+1])\n",
        "        dows = dts.dayofweek.values.astype(float)\n",
        "        wknd = (dows>=5).astype(float)\n",
        "        hol  = is_holiday(dts).values.astype(float)\n",
        "        winter  = np.array([_is_winter_date(pd.Timestamp(x)) for x in dts], dtype=float)\n",
        "        seq = np.stack([\n",
        "            np.clip(y28, 0.0, None),\n",
        "            st28,\n",
        "            peer28,\n",
        "            dows/6.0,\n",
        "            wknd,\n",
        "            hol,\n",
        "            winter\n",
        "        ], axis=1).astype(float)  # shape (28, n_seq_feat)\n",
        "\n",
        "        yf  = vals[end+1:end+8]\n",
        "        feats = build_feature_from_window(y28, y28r)\n",
        "\n",
        "        feats['store_prior']    = store_prior_index(store_name)\n",
        "        feats['store_r7_mean']  = float(st28[-7:].mean())\n",
        "        feats['store_r28_mean'] = float(st28[-28:].mean())\n",
        "        feats['peer_r7_mean']   = float(peer28[-7:].mean())\n",
        "        feats['peer_r28_mean']  = float(peer28[-28:].mean())\n",
        "        feats['share_in_store_r7']  = float(y28[-7:].sum()/(st28[-7:].sum()+1e-6))\n",
        "        feats['share_in_store_r28'] = float(y28[-28:].sum()/(st28[-28:].sum()+1e-6))\n",
        "\n",
        "        feats['store_name'] = store_name\n",
        "        feats['menu_name']  = menu_name\n",
        "        X_rows.append(feats); Y_rows.append(yf); SEQ_rows.append(seq)\n",
        "        meta.append({'anchor_date': pd.to_datetime(dates[end]),\n",
        "                     'store_name': store_name,\n",
        "                     'menu_name' : menu_name})\n",
        "    return X_rows, Y_rows, meta, SEQ_rows\n",
        "\n",
        "def df_to_windows(df_full):\n",
        "    X_all=[]; Y_all=[]; M_all=[]; S_all=[]\n",
        "    for name,g in df_full.groupby('영업장명_메뉴명'):\n",
        "        store, menu = split_store_menu(name)\n",
        "        g = g.sort_values('영업일자')\n",
        "        X,Y,M,S = make_supervised_windows(g, store, menu)\n",
        "        if X:\n",
        "            X_all.extend(X); Y_all.extend(Y); M_all.extend(M); S_all.extend(S)\n",
        "    return pd.DataFrame(X_all), np.array(Y_all), pd.DataFrame(M_all), np.array(S_all)\n",
        "\n",
        "X_base, Y, META, X_SEQ = df_to_windows(\n",
        "    train_full[[\"date\",\"key\",\"y\",\"y_raw\"]].rename(columns={\"date\":\"영업일자\",\"key\":\"영업장명_메뉴명\"})\n",
        ")\n",
        "META = META.reset_index(drop=True); X_base = X_base.reset_index(drop=True)\n",
        "\n",
        "# Encoders\n",
        "def fit_le_with_unk(values):\n",
        "    le = LabelEncoder()\n",
        "    uniq = pd.Series(values).astype(str).unique().tolist()\n",
        "    uniq = sorted(list(set(uniq + [\"<UNK>\"])))\n",
        "    le.fit(uniq)\n",
        "    return le\n",
        "\n",
        "le_store = fit_le_with_unk(META['store_name'])\n",
        "le_menu  = fit_le_with_unk(META['menu_name'])\n",
        "\n",
        "X_base[\"store_le\"] = le_store.transform(X_base[\"store_name\"].astype(str).fillna(\"<UNK>\"))\n",
        "X_base[\"menu_le\"]  = le_menu.transform(X_base[\"menu_name\"].astype(str).fillna(\"<UNK>\"))\n",
        "\n",
        "def add_kw(df):\n",
        "    MENU_KW = ['세트','라떼','커피','아메리카노','맥주','소주','와인','막걸리','사케','피자','파스타','국수','라면','우동',\n",
        "               '볶음','탕','스테이크','버거','샐러드','밥','비빔','디저트','케이크','아이스','주스','티','차','빵',\n",
        "               '샌드위치','BBQ','꼬치','튀김','만두','키즈']\n",
        "    STORE_KW = ['카페','주막','BBQ','라그로타','담하','미라시아','연회장','포레스트릿','화담','카페테리아']\n",
        "    df = df.copy()\n",
        "    df['store_name'] = df['store_name'].astype(str).fillna('')\n",
        "    df['menu_name']  = df['menu_name'].astype(str).fillna('')\n",
        "    for w in MENU_KW:\n",
        "        df[f'kw_m_{w}'] = df['menu_name'].str.contains(w).astype(int)\n",
        "    for w in STORE_KW:\n",
        "        df[f'kw_s_{w}'] = df['store_name'].str.contains(w).astype(int)\n",
        "    df['menu_len'] = df['menu_name'].str.len().astype(int)\n",
        "    return df\n",
        "\n",
        "X_base = add_kw(X_base)\n",
        "\n",
        "X_feat = X_base.drop(columns=['store_name','menu_name']).copy()\n",
        "anchor = META['anchor_date']\n",
        "stores_for_rows = META['store_name'].values\n",
        "\n",
        "print(\"Supervised X:\", X_feat.shape, \"Y:\", Y.shape, \"| Seq:\", X_SEQ.shape)"
      ],
      "metadata": {
        "id": "L5Akl-Gn1UqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Site/Store calendar profiles"
      ],
      "metadata": {
        "id": "x2jz62JO1asw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"[Profiles] build Site/Store calendar indices (DOW & WOY) ...\")\n",
        "tf = train_full[['date','store','y']].copy()\n",
        "tf['date'] = pd.to_datetime(tf['date'])\n",
        "\n",
        "site_daily = tf.groupby('date')['y'].sum().sort_index()\n",
        "site_mu = site_daily.mean()\n",
        "SITE_DOW_IDX = {d: (site_daily[site_daily.index.dayofweek==d].mean()/site_mu if site_mu>0 else 1.0) for d in range(7)}\n",
        "SITE_WOY_IDX = {}\n",
        "tmp = site_daily.to_frame('y')\n",
        "tmp['woy'] = tmp.index.isocalendar().week.astype(int)\n",
        "for w, g in tmp.groupby('woy'):\n",
        "    SITE_WOY_IDX[int(w)] = (g['y'].mean()/site_mu if site_mu>0 else 1.0)\n",
        "\n",
        "store_daily = tf.groupby(['date','store'])['y'].sum().rename('y').reset_index()\n",
        "STORE_DOW_IDX = {}\n",
        "STORE_WOY_IDX = {}\n",
        "for s, g in store_daily.groupby('store'):\n",
        "    g = g.sort_values('date').copy()\n",
        "    mu = g['y'].mean()\n",
        "    if mu<=0:\n",
        "        for d in range(7): STORE_DOW_IDX[(s,d)] = 1.0\n",
        "        continue\n",
        "    g['dow'] = g['date'].dt.dayofweek\n",
        "    for d, gg in g.groupby('dow'):\n",
        "        STORE_DOW_IDX[(s,int(d))] = gg['y'].mean()/mu\n",
        "    g['woy'] = g['date'].dt.isocalendar().week.astype(int)\n",
        "    for w, gg in g.groupby('woy'):\n",
        "        STORE_WOY_IDX[(s,int(w))] = gg['y'].mean()/mu\n",
        "\n",
        "def horizon_calendar(anchor_dates: pd.Series, h: int) -> pd.DataFrame:\n",
        "    td = pd.to_datetime(anchor_dates) + pd.to_timedelta(h, unit='D') + pd.to_timedelta(1, unit='D')\n",
        "    dow = td.dt.dayofweek\n",
        "    mon = td.dt.month\n",
        "    woy = td.dt.isocalendar().week.astype(int)\n",
        "    hol = is_holiday(td)\n",
        "    hol_adj = ((td - pd.Timedelta(days=1)).dt.date.map(lambda d: int(d in K_HOLS)) |\n",
        "               (td + pd.Timedelta(days=1)).dt.date.map(lambda d: int(d in K_HOLS))).astype(int)\n",
        "    is_winter = td.map(_is_winter_date).astype(int)\n",
        "    season_id = td.map(_season_id_from_dt).astype(int)      # 0봄 1여름 2가을 3겨울\n",
        "    spring_autumn_act = td.map(_spring_autumn_active).astype(int)\n",
        "    summer_fam = td.map(_summer_family).astype(int)\n",
        "\n",
        "    dow_prior_vals = dow.map(dow_prior).astype(float)\n",
        "    mon_prior_vals = mon.map(month_prior).astype(float)\n",
        "    return pd.DataFrame({\n",
        "        f'dow_h{h}': dow.astype(int),\n",
        "        f'month_h{h}': mon.astype(int),\n",
        "        f'woy_h{h}': woy.astype(int),\n",
        "        f'is_weekend_h{h}': (dow>=5).astype(int),\n",
        "        f'is_holiday_h{h}': hol,\n",
        "        f'is_hol_adj_h{h}': hol_adj,\n",
        "        f'season_h{h}': mon.map(season_code).astype(int),\n",
        "        f'season_id_h{h}': season_id,                       # 0봄 1여름 2가을 3겨울\n",
        "        f'is_winter_h{h}': is_winter,\n",
        "        f'spring_autumn_active_h{h}': spring_autumn_act,\n",
        "        f'summer_family_h{h}': summer_fam,\n",
        "        f'is_winter_weekend_h{h}': (is_winter & (dow>=5)).astype(int),\n",
        "        f'dow_prior_h{h}': dow_prior_vals.values,\n",
        "        f'month_prior_h{h}': mon_prior_vals.values,\n",
        "    })\n",
        "\n",
        "def _profile_block(anchor_series: pd.Series, h: int, store_series: pd.Series) -> pd.DataFrame:\n",
        "    cal = horizon_calendar(anchor_series, h+1)\n",
        "    dow = cal[f'dow_h{h+1}'].values\n",
        "    woy = cal[f'woy_h{h+1}'].values\n",
        "    stores = store_series.values\n",
        "    site_dow = np.array([SITE_DOW_IDX.get(int(d),1.0) for d in dow], dtype=float)\n",
        "    site_woy = np.array([SITE_WOY_IDX.get(int(w),1.0) for w in woy], dtype=float)\n",
        "    store_dow= np.array([STORE_DOW_IDX.get((str(s), int(d)),1.0) for s,d in zip(stores,dow)], dtype=float)\n",
        "    store_woy= np.array([STORE_WOY_IDX.get((str(s), int(w)),1.0) for s,w in zip(stores,woy)], dtype=float)\n",
        "    return pd.DataFrame({\n",
        "        f'site_dow_idx_h{h+1}': site_dow,\n",
        "        f'site_woy_idx_h{h+1}': site_woy,\n",
        "        f'store_dow_idx_h{h+1}': store_dow,\n",
        "        f'store_woy_idx_h{h+1}': store_woy,\n",
        "    })\n",
        "\n",
        "def add_h_feats(Xb: pd.DataFrame, anchor_series: pd.Series, h: int, store_series: pd.Series):\n",
        "    cal = horizon_calendar(anchor_series, h+1)  # +1..+7\n",
        "    prof = _profile_block(anchor_series, h, store_series)\n",
        "    return pd.concat([Xb.reset_index(drop=True), cal.reset_index(drop=True), prof.reset_index(drop=True)], axis=1)\n",
        "\n",
        "# weighted SMAPE\n",
        "def smape_store_weighted(y_true, y_pred, stores_arr):\n",
        "    idx = y_true>0\n",
        "    if not np.any(idx): return 0.0\n",
        "    A = y_true[idx]; P = y_pred[idx]; S = stores_arr[idx]\n",
        "    sm = 2.0*np.abs(A-P)/(np.abs(A)+np.abs(P)+1e-8)\n",
        "    score=0.0; tot=0.0\n",
        "    for s in np.unique(S):\n",
        "        m = (S==s)\n",
        "        w = store_weight(s)\n",
        "        score += w * sm[m].mean()\n",
        "        tot   += w\n",
        "    return float(score/tot)\n",
        "\n",
        "def lgb_wsmape_feval(preds, dset):\n",
        "    y = dset.get_label()\n",
        "    w = dset.get_weight()\n",
        "    if w is None: w = np.ones_like(y)\n",
        "    mask = y>0\n",
        "    if not np.any(mask):\n",
        "        return ('wSMAPE', 0.0, False)\n",
        "    s = 2.0*np.abs(y - preds) / (np.abs(y) + np.abs(preds) + 1e-8)\n",
        "    return ('wSMAPE', float(np.average(s[mask], weights=w[mask])), False)\n",
        "\n",
        "def xgb_wsmape_feval(preds, dtrain):\n",
        "    y = dtrain.get_label()\n",
        "    w = dtrain.get_weight()\n",
        "    if w is None or len(w)==0:\n",
        "        w = np.ones_like(y)\n",
        "    mask = y>0\n",
        "    if not np.any(mask):\n",
        "        return 'wSMAPE', 0.0\n",
        "    s = 2.0*np.abs(y - preds) / (np.abs(y) + np.abs(preds) + 1e-8)\n",
        "    return 'wSMAPE', float(np.average(s[mask], weights=w[mask]))"
      ],
      "metadata": {
        "id": "7YgAO51h1d40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost compatibility helpers (feval / prediction API)"
      ],
      "metadata": {
        "id": "87Q-7_Db1hMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def xgb_train_compat(params, dtrain, num_boost_round, evals, early_stopping_rounds=200, verbose_eval=200):\n",
        "    \"\"\"Try custom feval; if not supported, fallback to built-in eval_metric.\"\"\"\n",
        "    try:\n",
        "        return xgb.train(params, dtrain, num_boost_round=num_boost_round, evals=evals,\n",
        "                         feval=xgb_wsmape_feval, early_stopping_rounds=early_stopping_rounds,\n",
        "                         verbose_eval=verbose_eval)\n",
        "    except TypeError:\n",
        "        params2 = dict(params)\n",
        "        params2.setdefault('eval_metric', 'rmse')\n",
        "        return xgb.train(params2, dtrain, num_boost_round=num_boost_round, evals=evals,\n",
        "                         early_stopping_rounds=early_stopping_rounds, verbose_eval=verbose_eval)\n",
        "\n",
        "def xgb_best_iteration(model):\n",
        "    bi = getattr(model, 'best_iteration', None)\n",
        "    if bi is not None:\n",
        "        return int(bi)\n",
        "    bn = getattr(model, 'best_ntree_limit', None)\n",
        "    if bn is not None and bn > 0:\n",
        "        return int(bn)\n",
        "    return None\n",
        "\n",
        "def xgb_predict_compat(model, dmatrix):\n",
        "    bi = xgb_best_iteration(model)\n",
        "    try:\n",
        "        if bi is not None:\n",
        "            return model.predict(dmatrix, iteration_range=(0, bi))\n",
        "        else:\n",
        "            return model.predict(dmatrix)\n",
        "    except TypeError:\n",
        "        bn = getattr(model, 'best_ntree_limit', None)\n",
        "        if bn is not None and bn > 0:\n",
        "            return model.predict(dmatrix, ntree_limit=bn)\n",
        "        return model.predict(dmatrix)"
      ],
      "metadata": {
        "id": "fnVlC7aE1isN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Params"
      ],
      "metadata": {
        "id": "LL4CezRU13f8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LGB poisson\n",
        "lgb_params = dict(\n",
        "    objective=\"poisson\",\n",
        "    learning_rate=0.05,\n",
        "    num_leaves=45,\n",
        "    feature_fraction=0.80,\n",
        "    bagging_fraction=0.70,\n",
        "    bagging_freq=1,\n",
        "    min_data_in_leaf=80,\n",
        "    max_depth=-1,\n",
        "    lambda_l1=1e-3,\n",
        "    lambda_l2=5e-2,\n",
        "    metric=\"None\",\n",
        "    n_estimators=3000,\n",
        "    verbosity=-1\n",
        ")\n",
        "\n",
        "# XGBoost Tweedie\n",
        "XGB_TREE_METHOD = 'hist'\n",
        "xgb_params_base = dict(\n",
        "    objective='reg:tweedie',\n",
        "    tweedie_variance_power=1.2,\n",
        "    eta=0.05,\n",
        "    max_depth=7,\n",
        "    min_child_weight=8,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    reg_lambda=1.2,\n",
        "    reg_alpha=0.0,\n",
        "    tree_method=XGB_TREE_METHOD,\n",
        "    disable_default_eval_metric=1\n",
        ")\n",
        "\n",
        "def get_cat_cols(cols):\n",
        "    cats = []\n",
        "    base_cats = ['store_le','menu_le']\n",
        "    hol_cats  = [c for c in cols if c.startswith((\n",
        "        'dow_h','month_h','woy_h','is_weekend_h','is_holiday_h','is_hol_adj_h',\n",
        "        'season_h','season_id_h','is_winter_h','is_winter_weekend_h',\n",
        "        'spring_autumn_active_h','summer_family_h'\n",
        "    ))]\n",
        "    kw_cats   = [c for c in cols if c.startswith(('kw_m_','kw_s_'))]\n",
        "    cats.extend(base_cats + hol_cats + kw_cats)\n",
        "    return sorted(list(set([c for c in cats if c in cols])))"
      ],
      "metadata": {
        "id": "M6xwzljY157a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train 7 heads with KFold (LGB + XGB + (opt) LSTM OOF)"
      ],
      "metadata": {
        "id": "vwpO_JMX18Fw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"[Train] LGB, XGB & (opt) LSTM heads with weighted early stopping ...\")\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "oof_lgb = np.zeros_like(Y, dtype=float)\n",
        "oof_xgb = np.zeros_like(Y, dtype=float)\n",
        "oof_lstm = np.zeros_like(Y, dtype=float)\n",
        "models_cv_lgb = []\n",
        "models_cv_xgb = []\n",
        "models_cv_lstm = []  # list of tuples per h: [(model, mu, std), ...]\n",
        "avg_iters_lgb=[]; avg_iters_xgb=[]\n",
        "best_epochs_lstm=[]\n",
        "\n",
        "META_store_series = META['store_name']\n",
        "\n",
        "def standardize_seq(x, mu, std):\n",
        "    return (x - mu) / (std + 1e-6)\n",
        "\n",
        "def compute_seq_mu_std(x_seq):\n",
        "    # x_seq: (N, T, F)\n",
        "    mu  = x_seq.reshape(-1, x_seq.shape[-1]).mean(axis=0)\n",
        "    std = x_seq.reshape(-1, x_seq.shape[-1]).std(axis=0)\n",
        "    return mu.astype('float32'), (std+1e-6).astype('float32')\n",
        "\n",
        "def build_lstm_model(seq_len, n_feat, seed=42):\n",
        "    keras.utils.set_random_seed(seed)\n",
        "    inp = keras.Input(shape=(seq_len, n_feat))\n",
        "    x = layers.Masking(mask_value=0.0)(inp)\n",
        "    x = layers.LSTM(32, return_sequences=False)(x)\n",
        "    x = layers.Dropout(0.10)(x)\n",
        "    x = layers.Dense(16, activation='relu')(x)\n",
        "    out = layers.Dense(1, activation='relu')(x)  # non-neg\n",
        "    m = keras.Model(inp, out)\n",
        "    m.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
        "              loss='mae')\n",
        "    return m\n",
        "\n",
        "for h in range(7):\n",
        "    print(f\"  - Horizon +{h+1}d\")\n",
        "    yh = Y[:,h].astype(float).clip(min=0.0)\n",
        "    Xh = add_h_feats(X_feat, anchor, h, META_store_series)\n",
        "\n",
        "    cal_h = horizon_calendar(anchor, h+1)\n",
        "    is_weekend = cal_h[f'is_weekend_h{h+1}'].values\n",
        "    mon = cal_h[f'month_h{h+1}'].values\n",
        "    is_winter = cal_h[f'is_winter_h{h+1}'].values\n",
        "\n",
        "    # recency & weights\n",
        "    max_anchor = pd.to_datetime(anchor).max()\n",
        "    days_from_edge = (pd.to_datetime(anchor) - (max_anchor - pd.Timedelta(days=240))).dt.days.clip(lower=0, upper=240).values\n",
        "    rec = 0.5 + 0.8*(days_from_edge/240.0)**2\n",
        "\n",
        "    pos = yh[yh>0]\n",
        "    thr_low = np.quantile(pos, 0.25) if pos.size>50 else (pos.mean() if pos.size>0 else 0.0)\n",
        "    lowpos_bonus = np.where((yh>0) & (yh<=thr_low), 1.10, 1.0)\n",
        "\n",
        "    sw = np.array([store_weight(s) for s in stores_for_rows]) * rec * lowpos_bonus\n",
        "    sw = sw * (1.15**(is_weekend)) * (1.10**(pd.Series(mon).isin([12,1,2]).astype(int).values))\n",
        "    sw = sw * (1.10**(is_winter)) * (1.05**(is_winter & is_weekend))\n",
        "    sw = sw * (yh>0)\n",
        "\n",
        "    # 봄/여름/가을철 가중\n",
        "    spring_autumn_act = cal_h[f'spring_autumn_active_h{h+1}'].values.astype(int)\n",
        "    summer_fam = cal_h[f'summer_family_h{h+1}'].values.astype(int)\n",
        "    is_hwadam_arr = np.array([is_hwadam_store(s) for s in stores_for_rows]).astype(int)\n",
        "    summer_aff = np.array([STORE_SEASON_AFFINITY.get((s,'summer'),1.0) for s in stores_for_rows])\n",
        "    sw = sw * (1.05 ** (is_hwadam_arr & spring_autumn_act))\n",
        "    sw = sw * np.where((summer_fam==1) & (summer_aff>1.0), 1.03, 1.0)\n",
        "\n",
        "    cat_cols = get_cat_cols(Xh.columns.tolist())\n",
        "    fold_models_lgb=[]; fold_models_xgb=[]; fold_models_lstm=[];\n",
        "    oof_col_lgb=np.zeros_like(yh); oof_col_xgb=np.zeros_like(yh); oof_col_lstm=np.zeros_like(yh)\n",
        "\n",
        "    for fold,(tr,va) in enumerate(kf.split(Xh)):\n",
        "        # LGB\n",
        "        dtr = lgb.Dataset(Xh.iloc[tr], label=yh[tr], weight=sw[tr], categorical_feature=cat_cols, free_raw_data=False)\n",
        "        dva = lgb.Dataset(Xh.iloc[va], label=yh[va], weight=sw[va], categorical_feature=cat_cols, free_raw_data=False)\n",
        "        callbacks=[lgb.early_stopping(200), lgb.log_evaluation(200)]\n",
        "        model_lgb = lgb.train(lgb_params, dtr, valid_sets=[dtr,dva], valid_names=['train','valid'],\n",
        "                              feval=lgb_wsmape_feval, num_boost_round=5000, callbacks=callbacks)\n",
        "        best_it_lgb = model_lgb.best_iteration or model_lgb.current_iteration()\n",
        "        avg_iters_lgb.append(best_it_lgb)\n",
        "        oof_col_lgb[va] = model_lgb.predict(Xh.iloc[va], num_iteration=best_it_lgb)\n",
        "        fold_models_lgb.append(model_lgb)\n",
        "\n",
        "        # XGB\n",
        "        xtr = xgb.DMatrix(Xh.iloc[tr], label=yh[tr], weight=sw[tr], feature_names=Xh.columns.tolist())\n",
        "        xva = xgb.DMatrix(Xh.iloc[va], label=yh[va], weight=sw[va], feature_names=Xh.columns.tolist())\n",
        "        params = dict(xgb_params_base); params['seed'] = SEED + fold + 100*h\n",
        "        evals = [(xtr,'train'), (xva,'valid')]\n",
        "        model_xgb = xgb_train_compat(params, xtr, num_boost_round=5000, evals=evals,\n",
        "                                     early_stopping_rounds=200, verbose_eval=200)\n",
        "        best_it_xgb = xgb_best_iteration(model_xgb)\n",
        "        avg_iters_xgb.append(best_it_xgb if best_it_xgb is not None else model_xgb.num_boosted_rounds())\n",
        "        oof_col_xgb[va] = xgb_predict_compat(model_xgb, xva)\n",
        "        fold_models_xgb.append(model_xgb)\n",
        "\n",
        "        # LSTM\n",
        "        if TF_OK:\n",
        "            Xtr_seq = X_SEQ[tr]  # (Ntr, 28, F)\n",
        "            Xva_seq = X_SEQ[va]\n",
        "            mu,std = compute_seq_mu_std(Xtr_seq)\n",
        "            Xtr_s = standardize_seq(Xtr_seq, mu, std)\n",
        "            Xva_s = standardize_seq(Xva_seq, mu, std)\n",
        "\n",
        "            model_lstm = build_lstm_model(seq_len=Xtr_s.shape[1], n_feat=Xtr_s.shape[2], seed=SEED+fold+1000*h)\n",
        "            es = keras.callbacks.EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True, verbose=0)\n",
        "            hist = model_lstm.fit(\n",
        "                Xtr_s, yh[tr],\n",
        "                validation_data=(Xva_s, yh[va], sw[va]),\n",
        "                sample_weight=sw[tr],\n",
        "                epochs=120,\n",
        "                batch_size=256,\n",
        "                verbose=0,\n",
        "                callbacks=[es]\n",
        "            )\n",
        "            be = np.argmin(hist.history['val_loss']) if 'val_loss' in hist.history else len(hist.history.get('loss',[]))\n",
        "            best_epochs_lstm.append(be)\n",
        "            pred_l = model_lstm.predict(Xva_s, verbose=0).reshape(-1)\n",
        "            oof_col_lstm[va] = np.maximum(0.0, pred_l)\n",
        "            fold_models_lstm.append((model_lstm, mu, std))\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "    oof_lgb[:,h] = oof_col_lgb\n",
        "    oof_xgb[:,h] = oof_col_xgb\n",
        "    if TF_OK:\n",
        "        oof_lstm[:,h] = oof_col_lstm\n",
        "    models_cv_lgb.append((fold_models_lgb, Xh.columns.tolist(), cat_cols))\n",
        "    models_cv_xgb.append((fold_models_xgb, Xh.columns.tolist()))\n",
        "    if TF_OK:\n",
        "        models_cv_lstm.append((fold_models_lstm, list(SEQ_FEAT_NAMES)))\n",
        "    else:\n",
        "        models_cv_lstm.append(([], list(SEQ_FEAT_NAMES)))\n",
        "    gc.collect()"
      ],
      "metadata": {
        "id": "YaGZdZAN1-Jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OOF dynamic blend"
      ],
      "metadata": {
        "id": "DE2OBe412Bc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stores_rep = np.repeat(stores_for_rows, 7)\n",
        "\n",
        "# (h, DOW)별 XGB 가중치 α & LSTM 가중치 τ 튜닝(+수축)\n",
        "alpha_hd = {}  # (h,d) -> alpha for XGB\n",
        "tau_hd   = {}  # (h,d) -> tau for LSTM\n",
        "for h in range(7):\n",
        "    y   = Y[:, h].astype(float)\n",
        "    pL  = oof_lgb[:, h].astype(float)\n",
        "    pX  = oof_xgb[:, h].astype(float)\n",
        "    pS  = oof_lstm[:, h].astype(float) if TF_OK else np.zeros_like(pL)\n",
        "    dws = horizon_calendar(anchor, h+1)[f'dow_h{h+1}'].values\n",
        "    for d in range(7):\n",
        "        m = (y>0) & (dws==d)\n",
        "        if m.sum() < 300:\n",
        "            continue\n",
        "        best_s, best_a, best_t = 1e9, OOB_BLEND, (OOB_TAU if TF_OK else 0.0)\n",
        "        a_grid = [0.25,0.30,0.35,0.40,0.45,0.50,0.55]\n",
        "        t_grid = ([0.00,0.05,0.10,0.15,0.20,0.25] if TF_OK else [0.00])\n",
        "        for a in a_grid:\n",
        "            for t in t_grid:\n",
        "                if a + t > MAX_AX_SUM:\n",
        "                    continue\n",
        "                pr = pL[m]*(1-a-t) + pX[m]*a + pS[m]*t\n",
        "                s  = smape_store_weighted(y[m], pr, stores_for_rows[m])\n",
        "                if s < best_s:\n",
        "                    best_s, best_a, best_t = s, a, t\n",
        "        # 글로벌로 50% 수축\n",
        "        alpha_hd[(h,d)] = OOB_BLEND + 0.5*(best_a - OOB_BLEND)\n",
        "        base_tau = (OOB_TAU if TF_OK else 0.0)\n",
        "        tau_hd[(h,d)]   = base_tau + 0.5*(best_t - base_tau)\n",
        "\n",
        "# 튜닝된 α,τ로 OOF 블렌드 재계산\n",
        "oof_blend = np.zeros_like(oof_lgb)\n",
        "for h in range(7):\n",
        "    pL  = oof_lgb[:, h].astype(float)\n",
        "    pX  = oof_xgb[:, h].astype(float)\n",
        "    pS  = oof_lstm[:, h].astype(float) if TF_OK else np.zeros_like(pL)\n",
        "    dws = horizon_calendar(anchor, h+1)[f'dow_h{h+1}'].values\n",
        "    a_arr = np.array([alpha_hd.get((h, int(dd)), OOB_BLEND) for dd in dws], dtype=float)\n",
        "    t_arr = np.array([tau_hd.get((h, int(dd)), (OOB_TAU if TF_OK else 0.0)) for dd in dws], dtype=float)\n",
        "    a_arr = np.clip(a_arr, 0.0, 1.0)\n",
        "    t_arr = np.clip(t_arr, 0.0, 1.0)\n",
        "    sum_at = np.clip(a_arr + t_arr, None, MAX_AX_SUM)\n",
        "    excess = (a_arr + t_arr) - sum_at\n",
        "    t_arr = np.clip(t_arr - np.maximum(0.0, excess), 0.0, 1.0)\n",
        "    oof_blend[:, h] = pL*(1.0 - a_arr - t_arr) + pX*a_arr + pS*t_arr\n",
        "\n",
        "# (h, DOW)별 시드 앙상블 분위수 q 결정\n",
        "Q_HD = {}  # (h,d) -> 0.50 or 0.60\n",
        "for h in range(7):\n",
        "    y  = Y[:, h].astype(float)\n",
        "    p  = oof_blend[:, h].astype(float)\n",
        "    dws= horizon_calendar(anchor, h+1)[f'dow_h{h+1}'].values\n",
        "    for d in range(7):\n",
        "        m = (y>0) & (dws==d)\n",
        "        if m.sum() < 300:\n",
        "            continue\n",
        "        r = np.median(y[m] / (p[m] + 1e-6))\n",
        "        Q_HD[(h,d)] = 0.60 if r > 1.03 else 0.50\n",
        "\n",
        "val_score = smape_store_weighted(Y.flatten(), oof_blend.flatten(), stores_rep)\n",
        "print(f\"[OOF] weighted SMAPE (LGB+XGB+LSTM dyn α/τ, OOB_BLEND={OOB_BLEND}, OOB_TAU={OOB_TAU}) ≈ {val_score:.4f}\")\n",
        "print(f\"       tuned α buckets: {len(alpha_hd)}  | τ buckets: {len(tau_hd)} | q buckets: {len(Q_HD)}\")"
      ],
      "metadata": {
        "id": "AZjQTuDr2Cv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Horizon gamma/beta from blended OOF"
      ],
      "metadata": {
        "id": "vuX1kDrg2LjH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"[Calibrate] gamma(h,dow) & beta(h,dow) from blended OOF ...\")\n",
        "eps=1e-6\n",
        "h_gamma = np.ones(7, dtype=float)\n",
        "h_beta  = np.zeros(7, dtype=float)\n",
        "gamma_hd = {}\n",
        "beta_hd  = {}\n",
        "\n",
        "for h in range(7):\n",
        "    y  = Y[:,h].astype(float)\n",
        "    p  = oof_blend[:,h].astype(float)\n",
        "    nz = y>0\n",
        "    if np.any(nz):\n",
        "        ratio = np.median(y[nz]/(p[nz]+eps))\n",
        "        h_gamma[h] = float(np.clip(ratio, 0.90, 1.08))\n",
        "    naive = X_feat[f'lag_{7-h}'].values.astype(float)\n",
        "    best_s, best_b = 1e9, 0.0\n",
        "    for b in [0.00,0.02,0.04,0.06,0.08,0.10,0.12,0.15,0.18,0.20]:\n",
        "        pr = p*h_gamma[h]*(1-b) + naive*b\n",
        "        s = smape_store_weighted(y, pr, stores_for_rows)\n",
        "        if s < best_s:\n",
        "            best_s, best_b = s, b\n",
        "    h_beta[h] = best_b\n",
        "\n",
        "    dows = horizon_calendar(anchor, h+1)[f'dow_h{h+1}'].values\n",
        "    for d in range(7):\n",
        "        m = (y>0) & (dows==d)\n",
        "        if m.sum() >= 200:\n",
        "            r = np.median(y[m]/(p[m]+eps))\n",
        "            gamma_hd[(h,d)] = float(np.clip(r, 0.88, 1.10))\n",
        "            n = naive[m]\n",
        "            best_s, best_b = 1e9, h_beta[h]\n",
        "            for b in [0.00,0.02,0.04,0.06,0.08,0.10,0.12,0.15,0.18]:\n",
        "                pr = p[m]*gamma_hd[(h,d)]*(1-b) + n*b\n",
        "                s = smape_store_weighted(y[m], pr, stores_for_rows[m])\n",
        "                if s < best_s:\n",
        "                    best_s, best_b = s, b\n",
        "            beta_hd[(h,d)] = best_b\n",
        "\n",
        "print(\"  gamma_h (fallback):\", np.round(h_gamma,3))\n",
        "print(\"  beta_h  (fallback):\", np.round(h_beta,3))\n",
        "print(f\"  gamma_hd keys: {len(gamma_hd)}  beta_hd keys: {len(beta_hd)}\")"
      ],
      "metadata": {
        "id": "bmmoCew12Pii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OOF-based calibration maps (store/*)"
      ],
      "metadata": {
        "id": "et7uQk112R-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"[Calibrate] store / (store,dow) / (store,h) ratios from blended OOF ...\")\n",
        "all_dows=[]; h_idx=[]\n",
        "for h in range(7):\n",
        "    d = horizon_calendar(anchor, h+1)[f'dow_h{h+1}'].values\n",
        "    all_dows.append(d)\n",
        "    h_idx.append(np.full(Y.shape[0], h, dtype=int))\n",
        "all_dows = np.concatenate(all_dows)\n",
        "h_idx    = np.concatenate(h_idx)\n",
        "\n",
        "y_flat = Y.flatten(); p_flat = oof_blend.flatten()\n",
        "mask = y_flat>0\n",
        "y_pos, p_pos = y_flat[mask], p_flat[mask]\n",
        "s_pos = stores_rep[mask]\n",
        "dow_pos = all_dows[mask]\n",
        "h_pos = h_idx[mask]\n",
        "\n",
        "store_corr = {}; store_dow_corr = {}; store_h_corr = {}\n",
        "for s in np.unique(s_pos):\n",
        "    m = (s_pos==s)\n",
        "    if m.sum()>=50:\n",
        "        r = np.median(y_pos[m]/(p_pos[m]+eps))\n",
        "        store_corr[s] = float(np.clip(r, 0.75, 1.35))\n",
        "    else:\n",
        "        store_corr[s] = 1.0\n",
        "\n",
        "for s in np.unique(s_pos):\n",
        "    for d in range(7):\n",
        "        m = (s_pos==s) & (dow_pos==d)\n",
        "        if m.sum()>=30:\n",
        "            r = np.median(y_pos[m]/(p_pos[m]+eps))\n",
        "            store_dow_corr[(s,d)] = float(np.clip(r, 0.80, 1.30))\n",
        "\n",
        "for s in np.unique(s_pos):\n",
        "    for hh in range(7):\n",
        "        m = (s_pos==s) & (h_pos==hh)\n",
        "        if m.sum()>=30:\n",
        "            r = np.median(y_pos[m]/(p_pos[m]+eps))\n",
        "            store_h_corr[(s,hh)] = float(np.clip(r, 0.80, 1.25))\n",
        "\n",
        "print(\"[Calibrate] (store, winter_flag) & (store, weekend_or_holiday) ratios ...\")\n",
        "winter_flags=[]; wk_or_h_flags=[]\n",
        "for h in range(7):\n",
        "    cal = horizon_calendar(anchor, h+1)\n",
        "    winter_flags.append(cal[f'is_winter_h{h+1}'].values)\n",
        "    wk_or_h_flags.append((cal[f'is_weekend_h{h+1}'].values | cal[f'is_holiday_h{h+1}'].values).astype(int))\n",
        "winter_flags = np.concatenate(winter_flags)\n",
        "wk_or_h_flags = np.concatenate(wk_or_h_flags)\n",
        "\n",
        "winter_pos = winter_flags[mask]\n",
        "wk_pos  = wk_or_h_flags[mask]\n",
        "\n",
        "store_winter_corr = {}\n",
        "for s in np.unique(s_pos):\n",
        "    for flag in [0,1]:\n",
        "        m = (s_pos==s) & (winter_pos==flag)\n",
        "        if m.sum()>=30:\n",
        "            r = np.median(y_pos[m]/(p_pos[m]+eps))\n",
        "            store_winter_corr[(s,int(flag))] = float(np.clip(r, 0.80, 1.25))\n",
        "\n",
        "store_weekend_corr = {}\n",
        "for s in np.unique(s_pos):\n",
        "    for flag in [0,1]:\n",
        "        m = (s_pos==s) & (wk_pos==flag)\n",
        "        if m.sum()>=30:\n",
        "            r = np.median(y_pos[m]/(p_pos[m]+eps))\n",
        "            store_weekend_corr[(s,int(flag))] = float(np.clip(r, 0.80, 1.25))\n",
        "\n",
        "# (store×season) OOF 보정\n",
        "print(\"[Calibrate] (store, season_id) ratios from blended OOF ...\")\n",
        "all_season_ids=[]\n",
        "for h in range(7):\n",
        "    all_season_ids.append(horizon_calendar(anchor, h+1)[f'season_id_h{h+1}'].values)\n",
        "season_ids = np.concatenate(all_season_ids)\n",
        "g_pos = season_ids[mask]\n",
        "\n",
        "store_season_corr = {}\n",
        "for s in np.unique(s_pos):\n",
        "    for g in [0,1,2,3]:  # 0spring 1summer 2autumn 3winter\n",
        "        m = (s_pos==s) & (g_pos==g)\n",
        "        if m.sum()>=30:\n",
        "            r = np.median(y_pos[m]/(p_pos[m]+eps))\n",
        "            store_season_corr[(s,int(g))] = float(np.clip(r, 0.85, 1.20))\n",
        "print(\"  calibration maps ready.\")\n",
        "\n",
        "# Winsorized caps & floors\n",
        "print(\"[Calibrate] winsor caps & positive floors ...\")\n",
        "caps = {}\n",
        "floor_sm = {}\n",
        "floor_s  = {}\n",
        "GLOBAL_POS_Q05 = 0.0\n",
        "\n",
        "train_full2 = train_full.copy()\n",
        "train_full2['dow'] = pd.to_datetime(train_full2['date']).dt.dayofweek\n",
        "train_full2['is_weekend'] = (train_full2['dow']>=5).astype(int)\n",
        "train_full2['is_hol'] = is_holiday(train_full2['date'])\n",
        "\n",
        "pos_all = train_full2.loc[train_full2['y']>0, 'y'].values\n",
        "if pos_all.size>0:\n",
        "    GLOBAL_POS_Q05 = float(np.quantile(pos_all, 0.05))\n",
        "\n",
        "for (s,m), g in train_full2.groupby(['store','menu']):\n",
        "    pos = g.loc[g['y']>0, 'y'].values\n",
        "    if pos.size>=20:\n",
        "        floor_sm[(s,m)] = float(np.quantile(pos, 0.05))\n",
        "\n",
        "for s, g in train_full2.groupby('store'):\n",
        "    pos = g.loc[g['y']>0, 'y'].values\n",
        "    if pos.size>=20:\n",
        "        floor_s[s] = float(np.quantile(pos, 0.05))\n",
        "\n",
        "for s, g1 in train_full2.groupby('store'):\n",
        "    for d, g2 in g1.groupby('dow'):\n",
        "        for flag, g3 in g2.groupby((g2['is_weekend'] | g2['is_hol']).astype(int)):\n",
        "            arr = g3['y'].values.astype(float)\n",
        "            if len(arr)>=50:\n",
        "                q95 = float(np.quantile(arr, 0.95))\n",
        "                q99 = float(np.quantile(arr, 0.99))\n",
        "                caps[(s,d,int(flag))] = (q95, q99)\n",
        "\n",
        "def get_floor(store, menu):\n",
        "    return 0.0\n",
        "\n",
        "def apply_cap(store, dow, is_weekend, is_holiday, yhat):\n",
        "    flag = int((is_weekend==1) or (is_holiday==1))\n",
        "    q = caps.get((store,dow,flag))\n",
        "    if q is None: return yhat\n",
        "    q95,q99 = q\n",
        "    lim = (1.05*q99) if flag else (1.00*q99)\n",
        "    return float(min(yhat, max(q95, lim)))"
      ],
      "metadata": {
        "id": "FV86kiJX2Tkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrain on FULL with multi-seed bagging (LGB+XGB+(opt)LSTM)"
      ],
      "metadata": {
        "id": "_1ddImuc2alB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"[Retrain] full data with multi-seed bagging (LGB+XGB+(opt)LSTM) ...\")\n",
        "num_rounds_lgb = int(1.05*(np.mean(avg_iters_lgb) if len(avg_iters_lgb)>0 else 1500))\n",
        "num_rounds_xgb = int(1.05*(np.mean(avg_iters_xgb) if len(avg_iters_xgb)>0 else 1500))\n",
        "if TF_OK and len(best_epochs_lstm)>0:\n",
        "    num_epochs_lstm = int(np.clip(1.15*np.mean(best_epochs_lstm), 40, 150))\n",
        "else:\n",
        "    num_epochs_lstm = 0\n",
        "\n",
        "SEEDS = [42, 777, 1201, 2025, 314159]\n",
        "SEEDS_LSTM = [42, 777, 1201] if TF_OK else []\n",
        "\n",
        "final_models_lgb = []\n",
        "final_models_xgb = []\n",
        "final_models_lstm = []  # per h: list of (keras_model, mu, std)\n",
        "\n",
        "for h in range(7):\n",
        "    yh = Y[:,h].astype(float).clip(min=0.0)\n",
        "    Xh = add_h_feats(X_feat, anchor, h, META_store_series)\n",
        "    cal_h = horizon_calendar(anchor, h+1)\n",
        "    is_weekend = cal_h[f'is_weekend_h{h+1}'].values\n",
        "    mon = cal_h[f'month_h{h+1}'].values\n",
        "    is_winter = cal_h[f'is_winter_h{h+1}'].values\n",
        "\n",
        "    max_anchor = pd.to_datetime(anchor).max()\n",
        "    days_from_edge = (pd.to_datetime(anchor) - (max_anchor - pd.Timedelta(days=240))).dt.days.clip(lower=0, upper=240).values\n",
        "    rec = 0.5 + 0.8*(days_from_edge/240.0)**2\n",
        "\n",
        "    pos = yh[yh>0]\n",
        "    thr_low = np.quantile(pos, 0.25) if pos.size>50 else (pos.mean() if pos.size>0 else 0.0)\n",
        "    lowpos_bonus = np.where((yh>0) & (yh<=thr_low), 1.10, 1.0)\n",
        "\n",
        "    sw = np.array([store_weight(s) for s in stores_for_rows]) * rec * lowpos_bonus\n",
        "    sw = sw * (1.15**(is_weekend)) * (1.10**(pd.Series(mon).isin([12,1,2]).astype(int).values))\n",
        "    sw = sw * (1.10**(is_winter)) * (1.05**(is_winter & is_weekend))\n",
        "    sw = sw * (yh>0)\n",
        "\n",
        "    # 봄/여름/가을철 가중\n",
        "    spring_autumn_act = cal_h[f'spring_autumn_active_h{h+1}'].values.astype(int)\n",
        "    summer_fam = cal_h[f'summer_family_h{h+1}'].values.astype(int)\n",
        "    is_hwadam_arr = np.array([is_hwadam_store(s) for s in stores_for_rows]).astype(int)\n",
        "    summer_aff = np.array([STORE_SEASON_AFFINITY.get((s,'summer'),1.0) for s in stores_for_rows])\n",
        "    sw = sw * (1.05 ** (is_hwadam_arr & spring_autumn_act))\n",
        "    sw = sw * np.where((summer_fam==1) & (summer_aff>1.0), 1.03, 1.0)\n",
        "\n",
        "    cat_cols = get_cat_cols(Xh.columns.tolist())\n",
        "\n",
        "    # LGB multi-seed\n",
        "    models_h_lgb=[]\n",
        "    for sd in SEEDS:\n",
        "        params = dict(lgb_params); params['random_state']=sd\n",
        "        dtr = lgb.Dataset(Xh, label=yh, weight=sw, categorical_feature=cat_cols)\n",
        "        model = lgb.train(params, dtr, num_boost_round=num_rounds_lgb, feval=lgb_wsmape_feval,\n",
        "                          callbacks=[lgb.log_evaluation(250)])\n",
        "        models_h_lgb.append((model, Xh.columns.tolist(), cat_cols))\n",
        "    final_models_lgb.append(models_h_lgb)\n",
        "\n",
        "    # XGB multi-seed\n",
        "    models_h_xgb=[]\n",
        "    for sd in SEEDS:\n",
        "        params = dict(xgb_params_base); params['seed']=sd\n",
        "        dtr = xgb.DMatrix(Xh, label=yh, weight=sw, feature_names=Xh.columns.tolist())\n",
        "        model = xgb.train(params, dtr, num_boost_round=num_rounds_xgb, verbose_eval=250)\n",
        "        models_h_xgb.append((model, Xh.columns.tolist()))\n",
        "    final_models_xgb.append(models_h_xgb)\n",
        "\n",
        "    # LSTM multi-seed\n",
        "    models_h_lstm=[]\n",
        "    if TF_OK and num_epochs_lstm>0:\n",
        "        mu,std = compute_seq_mu_std(X_SEQ)\n",
        "        Xs = standardize_seq(X_SEQ, mu, std)\n",
        "        for sd in SEEDS_LSTM:\n",
        "            m = build_lstm_model(seq_len=Xs.shape[1], n_feat=Xs.shape[2], seed=sd+1000*h)\n",
        "            m.fit(Xs, yh, sample_weight=sw, epochs=num_epochs_lstm, batch_size=256, verbose=0)\n",
        "            models_h_lstm.append((m, mu, std))\n",
        "    final_models_lstm.append(models_h_lstm)\n",
        "    gc.collect()\n",
        "\n",
        "# Inference helpers\n",
        "def weekly_naive_from_history(history28, h):\n",
        "    return float(history28[-(7 - h)])\n",
        "\n",
        "def build_seq_from_history(history28_raw, store_name, history_dates, store_total_lookup):\n",
        "    history28 = np.clip(np.asarray(history28_raw, dtype=float), 0.0, None)\n",
        "    st28 = _lookup_store_total_series(store_name, pd.to_datetime(history_dates), store_total_lookup)\n",
        "    peer28 = np.clip(st28 - history28, 0.0, None)\n",
        "    dts = pd.to_datetime(history_dates)\n",
        "    dows = dts.dayofweek.values.astype(float)\n",
        "    wknd = (dows>=5).astype(float)\n",
        "    hol  = is_holiday(dts).values.astype(float)\n",
        "    winter  = np.array([_is_winter_date(pd.Timestamp(x)) for x in dts], dtype=float)\n",
        "    seq = np.stack([\n",
        "        history28,\n",
        "        st28,\n",
        "        peer28,\n",
        "        dows/6.0,\n",
        "        wknd,\n",
        "        hol,\n",
        "        winter\n",
        "    ], axis=1).astype(float)\n",
        "    return seq  # (28, F)\n",
        "\n",
        "def infer_single_item(history28_raw, store_name, menu_name, last_date,\n",
        "                      history_dates=None, store_total_lookup=None):\n",
        "    history28 = np.clip(np.asarray(history28_raw, dtype=float), 0.0, None)\n",
        "    feats = build_feature_from_window(history28, np.asarray(history28_raw, dtype=float))\n",
        "\n",
        "    feats['store_prior'] = store_prior_index(store_name)\n",
        "    if (history_dates is not None) and (store_total_lookup is not None):\n",
        "        st28 = _lookup_store_total_series(store_name, pd.to_datetime(history_dates), store_total_lookup)\n",
        "        peer28 = np.clip(st28 - history28, 0.0, None)\n",
        "        feats['store_r7_mean']  = float(np.mean(st28[-7:]))\n",
        "        feats['store_r28_mean'] = float(np.mean(st28[-28:]))\n",
        "        feats['peer_r7_mean']   = float(np.mean(peer28[-7:]))\n",
        "        feats['peer_r28_mean']  = float(np.mean(peer28[-28:]))\n",
        "        feats['share_in_store_r7']  = float(history28[-7:].sum()/(st28[-7:].sum()+1e-6))\n",
        "        feats['share_in_store_r28'] = float(history28[-28:].sum()/(st28[-28:].sum()+1e-6))\n",
        "    else:\n",
        "        feats['store_r7_mean']=feats['store_r28_mean']=0.0\n",
        "        feats['peer_r7_mean']=feats['peer_r28_mean']=0.0\n",
        "        feats['share_in_store_r7']=feats['share_in_store_r28']=0.0\n",
        "\n",
        "    feats['store_name']=store_name; feats['menu_name']=menu_name\n",
        "    row = pd.DataFrame([feats])\n",
        "    row = add_kw(row)\n",
        "    s_in = store_name if store_name in le_store.classes_ else \"<UNK>\"\n",
        "    m_in = menu_name  if menu_name  in le_menu.classes_  else \"<UNK>\"\n",
        "    row['store_le'] = le_store.transform([s_in])[0]\n",
        "    row['menu_le']  = le_menu.transform([m_in])[0]\n",
        "    row = row.drop(columns=['store_name','menu_name'])\n",
        "\n",
        "    # LSTM seq build\n",
        "    seq28 = None\n",
        "    if TF_OK and (history_dates is not None) and (store_total_lookup is not None):\n",
        "        seq28 = build_seq_from_history(np.asarray(history28_raw, dtype=float), store_name, history_dates, store_total_lookup)\n",
        "\n",
        "    anchor_date = pd.to_datetime(last_date)\n",
        "    preds=[]\n",
        "    for h in range(7):\n",
        "        cal = horizon_calendar(pd.Series([anchor_date]), h+1)\n",
        "        prof = _profile_block(pd.Series([anchor_date]), h, pd.Series([store_name]))\n",
        "        Xh = pd.concat([row.reset_index(drop=True), cal.reset_index(drop=True), prof.reset_index(drop=True)], axis=1)\n",
        "\n",
        "        dow = int(cal[f'dow_h{h+1}'].iloc[0])\n",
        "        is_wk = int(cal[f'is_weekend_h{h+1}'].iloc[0])\n",
        "        is_h  = int(cal[f'is_holiday_h{h+1}'].iloc[0])\n",
        "        is_winter= int(cal[f'is_winter_h{h+1}'].iloc[0])\n",
        "\n",
        "        # (h,DOW)별 분위수\n",
        "        q = Q_HD.get((h, dow), 0.50)\n",
        "\n",
        "        seed_preds_lgb=[]; seed_preds_xgb=[]; seed_preds_lstm=[]\n",
        "        # LGB\n",
        "        for (model, cols, cat_cols) in final_models_lgb[h]:\n",
        "            Xuse = Xh.reindex(columns=cols, fill_value=0)\n",
        "            yhat = float(model.predict(Xuse)[0])\n",
        "            seed_preds_lgb.append(max(0.0, yhat))\n",
        "        # XGB\n",
        "        for (model, cols) in final_models_xgb[h]:\n",
        "            Xuse = Xh.reindex(columns=cols, fill_value=0)\n",
        "            dm = xgb.DMatrix(Xuse, feature_names=cols)\n",
        "            yhat = float(xgb_predict_compat(model, dm)[0])\n",
        "            seed_preds_xgb.append(max(0.0, yhat))\n",
        "        # LSTM\n",
        "        if TF_OK and final_models_lstm[h]:\n",
        "            for (m, mu, std) in final_models_lstm[h]:\n",
        "                xs = standardize_seq(seq28[np.newaxis, ...], mu, std)\n",
        "                yhat = float(m.predict(xs, verbose=0).reshape(-1)[0])\n",
        "                seed_preds_lstm.append(max(0.0, yhat))\n",
        "\n",
        "        y_lgb = float(np.quantile(seed_preds_lgb, q)) if seed_preds_lgb else 0.0\n",
        "        y_xgb = float(np.quantile(seed_preds_xgb, q)) if seed_preds_xgb else 0.0\n",
        "        y_lstm= float(np.quantile(seed_preds_lstm, q)) if seed_preds_lstm else 0.0\n",
        "\n",
        "        # (h,DOW)별 α/τ로 LGB/XGB/LSTM 블렌드\n",
        "        a = alpha_hd.get((h, dow), ALPHA_XGB)\n",
        "        t = tau_hd.get((h, dow), (TAU_LSTM if TF_OK else 0.0))\n",
        "        a = max(0.0, min(1.0, a)); t = max(0.0, min(1.0, t))\n",
        "        if a + t > MAX_AX_SUM:\n",
        "            excess = a + t - MAX_AX_SUM\n",
        "            t = max(0.0, t - excess)\n",
        "        yhat  = (1.0-a-t)*y_lgb + a*y_xgb + t*y_lstm\n",
        "\n",
        "        # (h,d)-specific gamma/beta + shrink\n",
        "        g = _shrink(gamma_hd.get((h,dow), h_gamma[h]), TAU['gamma'])\n",
        "        b = beta_hd.get((h,dow), h_beta[h])\n",
        "\n",
        "        naive = weekly_naive_from_history(history28, h)\n",
        "        if naive <= 1e-6:\n",
        "            b = 0.0\n",
        "\n",
        "        nz_idx = np.where(np.asarray(history28) > 0)[0]\n",
        "        days_since_last_nz = 28 - (nz_idx[-1]+1) if len(nz_idx)>0 else 28\n",
        "        if days_since_last_nz >= 21:\n",
        "            b *= 0.75\n",
        "        elif days_since_last_nz >= 14:\n",
        "            b *= 0.90\n",
        "\n",
        "        yhat = yhat * g * (1.0 - b) + naive * b\n",
        "\n",
        "        # (store×season) OOF 보정 + 도메인 prior\n",
        "        g_sid = int(cal[f'season_id_h{h+1}'].iloc[0])      # 0-봄, 1-여름, 2-가을,3-겨울\n",
        "        m_season = _shrink(store_season_corr.get((store_name, g_sid), 1.0), TAU['season'])\n",
        "        g_name = {0:'spring',1:'summer',2:'autumn',3:'winter'}[g_sid]\n",
        "        m_aff = _shrink(STORE_SEASON_AFFINITY.get((store_name, g_name), 1.0), TAU['aff'])\n",
        "        if is_hwadam_store(store_name) and int(cal[f'spring_autumn_active_h{h+1}'].iloc[0])==1:\n",
        "            m_season *= 1.02\n",
        "        m_season = float(np.clip(m_season * m_aff, 0.90, 1.15))\n",
        "        yhat *= m_season\n",
        "\n",
        "        # 기존 store/* 보정\n",
        "        m_store = _shrink(store_corr.get(store_name, 1.0), TAU['store'])\n",
        "        m_sdow  = _shrink(store_dow_corr.get((store_name, dow), 1.0), TAU['sdow'])\n",
        "        m_sh    = _shrink(store_h_corr.get((store_name, h), 1.0), TAU['sh'])\n",
        "        m_winter   = _shrink(store_winter_corr.get((store_name, is_winter), 1.0), TAU['winter'])\n",
        "        m_wk    = _shrink(store_weekend_corr.get((store_name, int(is_wk or is_h)), 1.0), TAU['wk'])\n",
        "        mult = m_store * m_sdow * m_sh * m_winter * m_wk\n",
        "        yhat = yhat * mult\n",
        "\n",
        "        floor_val = get_floor(store_name, menu_name)\n",
        "        if floor_val > 0:\n",
        "            yhat = max(yhat, floor_val)\n",
        "\n",
        "        yhat = apply_cap(store_name, dow, is_wk, is_h, yhat)\n",
        "        preds.append(max(0.0, yhat))\n",
        "    return np.array(preds, dtype=float)"
      ],
      "metadata": {
        "id": "eYrm3Dfy2c_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build submission"
      ],
      "metadata": {
        "id": "a34x_FNY2jfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"[Predict] build submission from TEST files ...\")\n",
        "pred_rows=[]\n",
        "for tfp in TEST_FILES:\n",
        "    tid = int(re.findall(r'TEST_(\\d+)\\.csv', os.path.basename(tfp))[0])\n",
        "    tdf = pd.read_csv(tfp)\n",
        "    tdf = _clean_columns(tdf); tdf = _force_kor_cols(tdf); tdf = _normalize_key_cols(tdf)\n",
        "    tdf = tdf.rename(columns={\"영업일자\":\"date\",\"영업장명_메뉴명\":\"key\",\"매출수량\":\"y_raw\"})\n",
        "    tdf[\"date\"] = pd.to_datetime(tdf[\"date\"])\n",
        "    tdf[\"store\"], tdf[\"menu\"] = zip(*tdf[\"key\"].map(split_store_menu))\n",
        "\n",
        "    # 피어집계 생성\n",
        "    tdf['_yc'] = tdf['y_raw'].clip(lower=0.0)\n",
        "    test_store_total_ser = tdf.groupby(['store','date'])['_yc'].sum().sort_index()\n",
        "\n",
        "    for name,g in tdf.groupby(\"key\"):\n",
        "        g = g.sort_values(\"date\")\n",
        "        vals_raw = g[\"y_raw\"].values.astype(float)\n",
        "        dates_28 = g[\"date\"].values\n",
        "        assert len(vals_raw)==28, f\"{name} in {tfp} is not 28 days\"\n",
        "        store,menu = split_store_menu(name)\n",
        "        preds = infer_single_item(vals_raw, store, menu, g[\"date\"].max(),\n",
        "                                  history_dates=dates_28,\n",
        "                                  store_total_lookup=test_store_total_ser)\n",
        "        for h in range(7):\n",
        "            pred_rows.append({\n",
        "                \"영업일자\": f\"TEST_{tid:02d}+{h+1}일\",\n",
        "                \"영업장명_메뉴명\": name,\n",
        "                \"매출수량\": float(preds[h])\n",
        "            })\n",
        "\n",
        "pred_df = pd.DataFrame(pred_rows)\n",
        "pred_df = _clean_columns(pred_df)\n",
        "pred_df = _force_kor_cols(pred_df)\n",
        "pred_df = _normalize_key_cols(pred_df)\n",
        "\n",
        "pred_wide_clean = pred_df.pivot_table(index='영업일자',\n",
        "                                      columns='영업장명_메뉴명',\n",
        "                                      values='매출수량',\n",
        "                                      aggfunc='first').reset_index()\n",
        "\n",
        "raw_sample = pd.read_csv(SAMPLE_SUB_PATH)\n",
        "raw_cols = raw_sample.columns.tolist()\n",
        "raw_dates = raw_sample['영업일자'].astype(str)\n",
        "\n",
        "def _canon(s: str) -> str:\n",
        "    return (unicodedata.normalize('NFKC', str(s))\n",
        "            .replace('\\ufeff','').replace('\\u200b','').replace('\\xa0','').strip())\n",
        "clean_cols_target = ['영업일자'] + [_canon(c) for c in raw_cols if c != '영업일자']\n",
        "clean_to_raw = { _canon(c): c for c in raw_cols }\n",
        "\n",
        "submission_clean = pd.DataFrame({'영업일자': raw_dates})\n",
        "submission_clean = submission_clean.merge(pred_wide_clean, on='영업일자', how='left')\n",
        "\n",
        "for c in clean_cols_target:\n",
        "    if c not in submission_clean.columns and c != '영업일자':\n",
        "        submission_clean[c] = 0.0\n",
        "extra_cols = [c for c in submission_clean.columns if c not in clean_cols_target]\n",
        "if extra_cols:\n",
        "    submission_clean = submission_clean.drop(columns=extra_cols, errors='ignore')\n",
        "submission_clean = submission_clean[clean_cols_target]\n",
        "\n",
        "submission_final = submission_clean.rename(columns=clean_to_raw)\n",
        "submission_final = submission_final[raw_cols]\n",
        "if submission_final.isna().any().any():\n",
        "    print(\"[Warn] NaN detected in submission; filling 0.0\")\n",
        "    submission_final = submission_final.fillna(0.0)\n",
        "\n",
        "submission_final.to_csv(OUT_PATH, index=False, encoding='utf-8-sig')\n",
        "print(f\"[DONE] saved: {OUT_PATH}\")\n",
        "print(submission_final.head(3))\n",
        "\n",
        "s2 = pd.read_csv(SAMPLE_SUB_PATH)"
      ],
      "metadata": {
        "id": "GEOTgUq22nAC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}