{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdfwueBRLhng",
        "outputId": "e39a73d5-f015-4e1d-b24d-1e11c0da6e9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Info] TensorFlow detected. LSTM head ENABLED.\n",
            "[Path] BASE_DIR=data\n",
            "       TRAIN_DIR=data/train\n",
            "       TEST_DIR=data/test\n",
            "       TRAIN_PATH=data/train/train.csv\n",
            "       SAMPLE_SUB_PATH=data/sample_submission.csv\n",
            "       #TEST_FILES=10\n",
            "[Load] data/train/train.csv\n",
            "Train: (88844, 6) | Period: 2023-01-01 ~ 2024-06-15\n",
            "Unique series: 167 | Stores: 8\n",
            "Zero share: 0.506\n",
            "[Windows] 28->7 supervised windows ...\n",
            "Supervised X: (83166, 112) Y: (83166, 7) | Seq: (83166, 28, 7)\n",
            "[Profiles] build Site/Store calendar indices (DOW & WOY) ...\n",
            "[Train] LGB, XGB & (opt) LSTM heads with weighted early stopping ...\n",
            "  - Horizon +1d\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\ttrain's wSMAPE: 0.504299\tvalid's wSMAPE: 0.522979\n",
            "[400]\ttrain's wSMAPE: 0.461827\tvalid's wSMAPE: 0.500798\n",
            "[600]\ttrain's wSMAPE: 0.438067\tvalid's wSMAPE: 0.49376\n",
            "[800]\ttrain's wSMAPE: 0.418202\tvalid's wSMAPE: 0.488957\n",
            "[1000]\ttrain's wSMAPE: 0.400286\tvalid's wSMAPE: 0.485719\n",
            "[1200]\ttrain's wSMAPE: 0.384193\tvalid's wSMAPE: 0.483491\n",
            "[1400]\ttrain's wSMAPE: 0.369369\tvalid's wSMAPE: 0.481269\n",
            "[1600]\ttrain's wSMAPE: 0.35543\tvalid's wSMAPE: 0.479796\n",
            "[1800]\ttrain's wSMAPE: 0.342436\tvalid's wSMAPE: 0.478377\n",
            "[2000]\ttrain's wSMAPE: 0.330338\tvalid's wSMAPE: 0.47743\n",
            "[2200]\ttrain's wSMAPE: 0.318795\tvalid's wSMAPE: 0.476452\n",
            "[2400]\ttrain's wSMAPE: 0.307923\tvalid's wSMAPE: 0.475865\n",
            "[2600]\ttrain's wSMAPE: 0.297388\tvalid's wSMAPE: 0.475396\n",
            "[2800]\ttrain's wSMAPE: 0.287766\tvalid's wSMAPE: 0.475031\n",
            "[3000]\ttrain's wSMAPE: 0.278202\tvalid's wSMAPE: 0.474693\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[3000]\ttrain's wSMAPE: 0.278202\tvalid's wSMAPE: 0.474693\n",
            "[0]\ttrain-rmse:56.48143\tvalid-rmse:52.72324\n",
            "[200]\ttrain-rmse:9.47284\tvalid-rmse:18.97447\n",
            "[400]\ttrain-rmse:6.96789\tvalid-rmse:18.81226\n",
            "[600]\ttrain-rmse:5.51446\tvalid-rmse:18.79169\n",
            "[702]\ttrain-rmse:4.95129\tvalid-rmse:18.80987\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\ttrain's wSMAPE: 0.501516\tvalid's wSMAPE: 0.542146\n",
            "[400]\ttrain's wSMAPE: 0.458827\tvalid's wSMAPE: 0.519813\n",
            "[600]\ttrain's wSMAPE: 0.434668\tvalid's wSMAPE: 0.512231\n",
            "[800]\ttrain's wSMAPE: 0.41437\tvalid's wSMAPE: 0.506991\n",
            "[1000]\ttrain's wSMAPE: 0.396074\tvalid's wSMAPE: 0.503676\n",
            "[1200]\ttrain's wSMAPE: 0.380071\tvalid's wSMAPE: 0.501144\n",
            "[1400]\ttrain's wSMAPE: 0.365113\tvalid's wSMAPE: 0.498928\n",
            "[1600]\ttrain's wSMAPE: 0.351079\tvalid's wSMAPE: 0.496799\n",
            "[1800]\ttrain's wSMAPE: 0.338161\tvalid's wSMAPE: 0.495493\n",
            "[2000]\ttrain's wSMAPE: 0.32602\tvalid's wSMAPE: 0.494201\n",
            "[2200]\ttrain's wSMAPE: 0.314777\tvalid's wSMAPE: 0.493146\n",
            "[2400]\ttrain's wSMAPE: 0.304271\tvalid's wSMAPE: 0.492293\n",
            "[2600]\ttrain's wSMAPE: 0.294062\tvalid's wSMAPE: 0.491423\n",
            "[2800]\ttrain's wSMAPE: 0.284195\tvalid's wSMAPE: 0.490585\n",
            "[3000]\ttrain's wSMAPE: 0.274939\tvalid's wSMAPE: 0.489957\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2999]\ttrain's wSMAPE: 0.274977\tvalid's wSMAPE: 0.489954\n",
            "[0]\ttrain-rmse:56.78302\tvalid-rmse:51.47248\n",
            "[200]\ttrain-rmse:9.64052\tvalid-rmse:19.59216\n",
            "[400]\ttrain-rmse:7.05406\tvalid-rmse:19.30836\n",
            "[600]\ttrain-rmse:5.56830\tvalid-rmse:19.22009\n",
            "[800]\ttrain-rmse:4.59502\tvalid-rmse:19.20352\n",
            "[1000]\ttrain-rmse:3.88043\tvalid-rmse:19.19314\n",
            "[1200]\ttrain-rmse:3.33604\tvalid-rmse:19.21425\n",
            "[1216]\ttrain-rmse:3.28957\tvalid-rmse:19.22158\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\ttrain's wSMAPE: 0.502024\tvalid's wSMAPE: 0.532896\n",
            "[400]\ttrain's wSMAPE: 0.459656\tvalid's wSMAPE: 0.508404\n",
            "[600]\ttrain's wSMAPE: 0.436063\tvalid's wSMAPE: 0.501712\n",
            "[800]\ttrain's wSMAPE: 0.415714\tvalid's wSMAPE: 0.498056\n",
            "[1000]\ttrain's wSMAPE: 0.397954\tvalid's wSMAPE: 0.495199\n",
            "[1200]\ttrain's wSMAPE: 0.381867\tvalid's wSMAPE: 0.493095\n",
            "[1400]\ttrain's wSMAPE: 0.366841\tvalid's wSMAPE: 0.491404\n",
            "[1600]\ttrain's wSMAPE: 0.352786\tvalid's wSMAPE: 0.490101\n",
            "[1800]\ttrain's wSMAPE: 0.339483\tvalid's wSMAPE: 0.488644\n",
            "[2000]\ttrain's wSMAPE: 0.327263\tvalid's wSMAPE: 0.48764\n",
            "[2200]\ttrain's wSMAPE: 0.315783\tvalid's wSMAPE: 0.487143\n",
            "[2400]\ttrain's wSMAPE: 0.304859\tvalid's wSMAPE: 0.486287\n",
            "[2600]\ttrain's wSMAPE: 0.29479\tvalid's wSMAPE: 0.485987\n",
            "[2800]\ttrain's wSMAPE: 0.285215\tvalid's wSMAPE: 0.485279\n",
            "[3000]\ttrain's wSMAPE: 0.275561\tvalid's wSMAPE: 0.484541\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[3000]\ttrain's wSMAPE: 0.275561\tvalid's wSMAPE: 0.484541\n",
            "[0]\ttrain-rmse:54.51701\tvalid-rmse:60.49176\n",
            "[200]\ttrain-rmse:9.49135\tvalid-rmse:19.94528\n",
            "[400]\ttrain-rmse:6.91269\tvalid-rmse:19.78339\n",
            "[600]\ttrain-rmse:5.49366\tvalid-rmse:19.62959\n",
            "[797]\ttrain-rmse:4.53353\tvalid-rmse:19.68603\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\ttrain's wSMAPE: 0.502306\tvalid's wSMAPE: 0.529811\n",
            "[400]\ttrain's wSMAPE: 0.45934\tvalid's wSMAPE: 0.506528\n",
            "[600]\ttrain's wSMAPE: 0.435596\tvalid's wSMAPE: 0.500247\n",
            "[800]\ttrain's wSMAPE: 0.415873\tvalid's wSMAPE: 0.496674\n",
            "[1000]\ttrain's wSMAPE: 0.397973\tvalid's wSMAPE: 0.494065\n",
            "[1200]\ttrain's wSMAPE: 0.381531\tvalid's wSMAPE: 0.491767\n",
            "[1400]\ttrain's wSMAPE: 0.366827\tvalid's wSMAPE: 0.490201\n",
            "[1600]\ttrain's wSMAPE: 0.353074\tvalid's wSMAPE: 0.489273\n",
            "[1800]\ttrain's wSMAPE: 0.340135\tvalid's wSMAPE: 0.488046\n",
            "[2000]\ttrain's wSMAPE: 0.327758\tvalid's wSMAPE: 0.486832\n",
            "[2200]\ttrain's wSMAPE: 0.316391\tvalid's wSMAPE: 0.486094\n",
            "[2400]\ttrain's wSMAPE: 0.30563\tvalid's wSMAPE: 0.485307\n",
            "[2600]\ttrain's wSMAPE: 0.295455\tvalid's wSMAPE: 0.484616\n",
            "[2800]\ttrain's wSMAPE: 0.285767\tvalid's wSMAPE: 0.483885\n",
            "[3000]\ttrain's wSMAPE: 0.276292\tvalid's wSMAPE: 0.483331\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[3000]\ttrain's wSMAPE: 0.276292\tvalid's wSMAPE: 0.483331\n",
            "[0]\ttrain-rmse:54.67846\tvalid-rmse:59.95034\n",
            "[200]\ttrain-rmse:9.39546\tvalid-rmse:22.23503\n",
            "[400]\ttrain-rmse:6.96092\tvalid-rmse:22.26184\n",
            "[440]\ttrain-rmse:6.57295\tvalid-rmse:22.27040\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\ttrain's wSMAPE: 0.503958\tvalid's wSMAPE: 0.534417\n",
            "[400]\ttrain's wSMAPE: 0.462196\tvalid's wSMAPE: 0.512776\n",
            "[600]\ttrain's wSMAPE: 0.437239\tvalid's wSMAPE: 0.506056\n",
            "[800]\ttrain's wSMAPE: 0.416165\tvalid's wSMAPE: 0.501617\n",
            "[1000]\ttrain's wSMAPE: 0.398065\tvalid's wSMAPE: 0.498836\n",
            "[1200]\ttrain's wSMAPE: 0.382036\tvalid's wSMAPE: 0.496715\n",
            "[1400]\ttrain's wSMAPE: 0.366776\tvalid's wSMAPE: 0.494487\n",
            "[1600]\ttrain's wSMAPE: 0.353065\tvalid's wSMAPE: 0.492658\n",
            "[1800]\ttrain's wSMAPE: 0.34005\tvalid's wSMAPE: 0.491073\n",
            "[2000]\ttrain's wSMAPE: 0.328031\tvalid's wSMAPE: 0.48963\n",
            "[2200]\ttrain's wSMAPE: 0.316702\tvalid's wSMAPE: 0.488482\n",
            "[2400]\ttrain's wSMAPE: 0.305781\tvalid's wSMAPE: 0.487376\n",
            "[2600]\ttrain's wSMAPE: 0.295752\tvalid's wSMAPE: 0.486483\n",
            "[2800]\ttrain's wSMAPE: 0.286026\tvalid's wSMAPE: 0.485243\n",
            "[3000]\ttrain's wSMAPE: 0.277001\tvalid's wSMAPE: 0.484656\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2999]\ttrain's wSMAPE: 0.277045\tvalid's wSMAPE: 0.484653\n",
            "[0]\ttrain-rmse:56.28960\tvalid-rmse:53.54986\n",
            "[200]\ttrain-rmse:9.52146\tvalid-rmse:20.30310\n",
            "[400]\ttrain-rmse:6.96419\tvalid-rmse:20.11951\n",
            "[600]\ttrain-rmse:5.52172\tvalid-rmse:20.00409\n",
            "[800]\ttrain-rmse:4.57990\tvalid-rmse:19.98518\n",
            "[1000]\ttrain-rmse:3.86891\tvalid-rmse:20.04836\n",
            "[1061]\ttrain-rmse:3.68184\tvalid-rmse:20.06086\n",
            "  - Horizon +2d\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\ttrain's wSMAPE: 0.507825\tvalid's wSMAPE: 0.537227\n",
            "[400]\ttrain's wSMAPE: 0.463074\tvalid's wSMAPE: 0.512263\n",
            "[600]\ttrain's wSMAPE: 0.43878\tvalid's wSMAPE: 0.504466\n",
            "[800]\ttrain's wSMAPE: 0.418249\tvalid's wSMAPE: 0.500021\n",
            "[1000]\ttrain's wSMAPE: 0.400661\tvalid's wSMAPE: 0.496695\n",
            "[1200]\ttrain's wSMAPE: 0.384514\tvalid's wSMAPE: 0.494124\n",
            "[1400]\ttrain's wSMAPE: 0.369606\tvalid's wSMAPE: 0.491586\n",
            "[1600]\ttrain's wSMAPE: 0.355695\tvalid's wSMAPE: 0.489716\n",
            "[1800]\ttrain's wSMAPE: 0.34327\tvalid's wSMAPE: 0.488207\n",
            "[2000]\ttrain's wSMAPE: 0.331254\tvalid's wSMAPE: 0.487218\n",
            "[2200]\ttrain's wSMAPE: 0.319914\tvalid's wSMAPE: 0.485978\n",
            "[2400]\ttrain's wSMAPE: 0.309337\tvalid's wSMAPE: 0.484958\n",
            "[2600]\ttrain's wSMAPE: 0.299426\tvalid's wSMAPE: 0.484569\n",
            "[2800]\ttrain's wSMAPE: 0.28961\tvalid's wSMAPE: 0.483897\n",
            "[3000]\ttrain's wSMAPE: 0.280547\tvalid's wSMAPE: 0.483645\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2993]\ttrain's wSMAPE: 0.280817\tvalid's wSMAPE: 0.483613\n",
            "[0]\ttrain-rmse:56.33444\tvalid-rmse:52.46368\n",
            "[200]\ttrain-rmse:10.02208\tvalid-rmse:19.78648\n",
            "[400]\ttrain-rmse:7.22023\tvalid-rmse:19.37219\n",
            "[600]\ttrain-rmse:5.71091\tvalid-rmse:19.34873\n",
            "[800]\ttrain-rmse:4.71075\tvalid-rmse:19.38900\n",
            "[884]\ttrain-rmse:4.36147\tvalid-rmse:19.37264\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\ttrain's wSMAPE: 0.509014\tvalid's wSMAPE: 0.534322\n",
            "[400]\ttrain's wSMAPE: 0.464947\tvalid's wSMAPE: 0.509379\n",
            "[600]\ttrain's wSMAPE: 0.439238\tvalid's wSMAPE: 0.501118\n",
            "[800]\ttrain's wSMAPE: 0.418914\tvalid's wSMAPE: 0.496698\n",
            "[1000]\ttrain's wSMAPE: 0.400878\tvalid's wSMAPE: 0.493415\n",
            "[1200]\ttrain's wSMAPE: 0.384427\tvalid's wSMAPE: 0.490564\n",
            "[1400]\ttrain's wSMAPE: 0.369476\tvalid's wSMAPE: 0.488601\n",
            "[1600]\ttrain's wSMAPE: 0.355858\tvalid's wSMAPE: 0.487119\n",
            "[1800]\ttrain's wSMAPE: 0.343076\tvalid's wSMAPE: 0.485823\n",
            "[2000]\ttrain's wSMAPE: 0.330968\tvalid's wSMAPE: 0.484684\n",
            "[2200]\ttrain's wSMAPE: 0.319665\tvalid's wSMAPE: 0.483648\n",
            "[2400]\ttrain's wSMAPE: 0.308834\tvalid's wSMAPE: 0.482849\n",
            "[2600]\ttrain's wSMAPE: 0.298704\tvalid's wSMAPE: 0.482082\n",
            "[2800]\ttrain's wSMAPE: 0.289335\tvalid's wSMAPE: 0.481596\n",
            "[3000]\ttrain's wSMAPE: 0.280067\tvalid's wSMAPE: 0.481288\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2991]\ttrain's wSMAPE: 0.280431\tvalid's wSMAPE: 0.481242\n",
            "[0]\ttrain-rmse:56.53790\tvalid-rmse:51.56256\n",
            "[200]\ttrain-rmse:9.98272\tvalid-rmse:20.29968\n",
            "[400]\ttrain-rmse:7.17901\tvalid-rmse:19.85913\n",
            "[600]\ttrain-rmse:5.71153\tvalid-rmse:19.71171\n",
            "[800]\ttrain-rmse:4.69410\tvalid-rmse:19.73490\n",
            "[828]\ttrain-rmse:4.54707\tvalid-rmse:19.72581\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\ttrain's wSMAPE: 0.507176\tvalid's wSMAPE: 0.534854\n",
            "[400]\ttrain's wSMAPE: 0.46312\tvalid's wSMAPE: 0.51068\n",
            "[600]\ttrain's wSMAPE: 0.438283\tvalid's wSMAPE: 0.503279\n",
            "[800]\ttrain's wSMAPE: 0.417736\tvalid's wSMAPE: 0.498557\n",
            "[1000]\ttrain's wSMAPE: 0.399688\tvalid's wSMAPE: 0.495486\n",
            "[1200]\ttrain's wSMAPE: 0.383704\tvalid's wSMAPE: 0.492995\n",
            "[1400]\ttrain's wSMAPE: 0.368601\tvalid's wSMAPE: 0.491127\n",
            "[1600]\ttrain's wSMAPE: 0.355078\tvalid's wSMAPE: 0.48986\n",
            "[1800]\ttrain's wSMAPE: 0.342233\tvalid's wSMAPE: 0.488308\n",
            "[2000]\ttrain's wSMAPE: 0.329999\tvalid's wSMAPE: 0.486946\n",
            "[2200]\ttrain's wSMAPE: 0.318531\tvalid's wSMAPE: 0.486191\n",
            "[2400]\ttrain's wSMAPE: 0.308173\tvalid's wSMAPE: 0.485451\n",
            "[2600]\ttrain's wSMAPE: 0.298064\tvalid's wSMAPE: 0.484634\n",
            "[2800]\ttrain's wSMAPE: 0.288678\tvalid's wSMAPE: 0.483938\n",
            "[3000]\ttrain's wSMAPE: 0.279451\tvalid's wSMAPE: 0.483649\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2966]\ttrain's wSMAPE: 0.280918\tvalid's wSMAPE: 0.483519\n",
            "[0]\ttrain-rmse:53.60998\tvalid-rmse:62.83065\n",
            "[200]\ttrain-rmse:9.65961\tvalid-rmse:21.92549\n",
            "[400]\ttrain-rmse:6.99250\tvalid-rmse:21.65982\n",
            "[600]\ttrain-rmse:5.43746\tvalid-rmse:21.64065\n",
            "[744]\ttrain-rmse:4.72581\tvalid-rmse:21.62929\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\ttrain's wSMAPE: 0.507093\tvalid's wSMAPE: 0.539627\n",
            "[400]\ttrain's wSMAPE: 0.462419\tvalid's wSMAPE: 0.513021\n",
            "[600]\ttrain's wSMAPE: 0.43821\tvalid's wSMAPE: 0.504673\n",
            "[800]\ttrain's wSMAPE: 0.418087\tvalid's wSMAPE: 0.499626\n",
            "[1000]\ttrain's wSMAPE: 0.400663\tvalid's wSMAPE: 0.496156\n",
            "[1200]\ttrain's wSMAPE: 0.384405\tvalid's wSMAPE: 0.493253\n",
            "[1400]\ttrain's wSMAPE: 0.369745\tvalid's wSMAPE: 0.491063\n",
            "[1600]\ttrain's wSMAPE: 0.355878\tvalid's wSMAPE: 0.489202\n",
            "[1800]\ttrain's wSMAPE: 0.343083\tvalid's wSMAPE: 0.487556\n",
            "[2000]\ttrain's wSMAPE: 0.331011\tvalid's wSMAPE: 0.486434\n",
            "[2200]\ttrain's wSMAPE: 0.319437\tvalid's wSMAPE: 0.485163\n",
            "[2400]\ttrain's wSMAPE: 0.308893\tvalid's wSMAPE: 0.483769\n",
            "[2600]\ttrain's wSMAPE: 0.298761\tvalid's wSMAPE: 0.482695\n",
            "[2800]\ttrain's wSMAPE: 0.288988\tvalid's wSMAPE: 0.481632\n",
            "[3000]\ttrain's wSMAPE: 0.280054\tvalid's wSMAPE: 0.480883\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2982]\ttrain's wSMAPE: 0.2808\tvalid's wSMAPE: 0.480788\n",
            "[0]\ttrain-rmse:55.53394\tvalid-rmse:55.89396\n",
            "[200]\ttrain-rmse:9.75479\tvalid-rmse:21.89573\n",
            "[400]\ttrain-rmse:7.04508\tvalid-rmse:21.49755\n",
            "[600]\ttrain-rmse:5.49845\tvalid-rmse:21.42360\n",
            "[794]\ttrain-rmse:4.60176\tvalid-rmse:21.45470\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\ttrain's wSMAPE: 0.508559\tvalid's wSMAPE: 0.542131\n",
            "[400]\ttrain's wSMAPE: 0.464788\tvalid's wSMAPE: 0.517436\n",
            "[600]\ttrain's wSMAPE: 0.439744\tvalid's wSMAPE: 0.508421\n",
            "[800]\ttrain's wSMAPE: 0.41847\tvalid's wSMAPE: 0.502734\n",
            "[1000]\ttrain's wSMAPE: 0.400008\tvalid's wSMAPE: 0.499143\n",
            "[1200]\ttrain's wSMAPE: 0.383734\tvalid's wSMAPE: 0.496137\n",
            "[1400]\ttrain's wSMAPE: 0.369011\tvalid's wSMAPE: 0.493786\n",
            "[1600]\ttrain's wSMAPE: 0.355119\tvalid's wSMAPE: 0.491998\n",
            "[1800]\ttrain's wSMAPE: 0.342259\tvalid's wSMAPE: 0.490048\n",
            "[2000]\ttrain's wSMAPE: 0.330064\tvalid's wSMAPE: 0.488977\n",
            "[2200]\ttrain's wSMAPE: 0.318631\tvalid's wSMAPE: 0.48771\n",
            "[2400]\ttrain's wSMAPE: 0.308101\tvalid's wSMAPE: 0.486715\n",
            "[2600]\ttrain's wSMAPE: 0.298169\tvalid's wSMAPE: 0.485972\n",
            "[2800]\ttrain's wSMAPE: 0.288798\tvalid's wSMAPE: 0.485127\n",
            "[3000]\ttrain's wSMAPE: 0.279709\tvalid's wSMAPE: 0.48435\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2996]\ttrain's wSMAPE: 0.279907\tvalid's wSMAPE: 0.484333\n",
            "[0]\ttrain-rmse:55.86715\tvalid-rmse:54.44902\n",
            "[200]\ttrain-rmse:9.73541\tvalid-rmse:20.94447\n",
            "[400]\ttrain-rmse:7.09427\tvalid-rmse:20.85659\n",
            "[600]\ttrain-rmse:5.54131\tvalid-rmse:20.79492\n",
            "[655]\ttrain-rmse:5.25560\tvalid-rmse:20.85315\n",
            "  - Horizon +3d\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\ttrain's wSMAPE: 0.506933\tvalid's wSMAPE: 0.538893\n",
            "[400]\ttrain's wSMAPE: 0.462003\tvalid's wSMAPE: 0.513326\n",
            "[600]\ttrain's wSMAPE: 0.437787\tvalid's wSMAPE: 0.50616\n",
            "[800]\ttrain's wSMAPE: 0.418035\tvalid's wSMAPE: 0.501582\n",
            "[1000]\ttrain's wSMAPE: 0.400746\tvalid's wSMAPE: 0.498501\n",
            "[1200]\ttrain's wSMAPE: 0.384848\tvalid's wSMAPE: 0.496204\n",
            "[1400]\ttrain's wSMAPE: 0.37062\tvalid's wSMAPE: 0.494103\n",
            "[1600]\ttrain's wSMAPE: 0.357221\tvalid's wSMAPE: 0.49222\n",
            "[1800]\ttrain's wSMAPE: 0.344473\tvalid's wSMAPE: 0.490776\n",
            "[2000]\ttrain's wSMAPE: 0.332564\tvalid's wSMAPE: 0.489466\n",
            "[2200]\ttrain's wSMAPE: 0.321462\tvalid's wSMAPE: 0.488208\n",
            "[2400]\ttrain's wSMAPE: 0.311223\tvalid's wSMAPE: 0.487085\n",
            "[2600]\ttrain's wSMAPE: 0.301341\tvalid's wSMAPE: 0.48629\n",
            "[2800]\ttrain's wSMAPE: 0.29207\tvalid's wSMAPE: 0.485511\n",
            "[3000]\ttrain's wSMAPE: 0.282955\tvalid's wSMAPE: 0.484937\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[3000]\ttrain's wSMAPE: 0.282955\tvalid's wSMAPE: 0.484937\n",
            "[0]\ttrain-rmse:56.06203\tvalid-rmse:53.07772\n",
            "[200]\ttrain-rmse:9.79660\tvalid-rmse:19.43009\n",
            "[400]\ttrain-rmse:7.00774\tvalid-rmse:19.19311\n",
            "[600]\ttrain-rmse:5.54071\tvalid-rmse:19.14423\n",
            "[747]\ttrain-rmse:4.82019\tvalid-rmse:19.19714\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\ttrain's wSMAPE: 0.508515\tvalid's wSMAPE: 0.538543\n",
            "[400]\ttrain's wSMAPE: 0.463724\tvalid's wSMAPE: 0.511263\n",
            "[600]\ttrain's wSMAPE: 0.439242\tvalid's wSMAPE: 0.502689\n",
            "[800]\ttrain's wSMAPE: 0.418646\tvalid's wSMAPE: 0.497852\n",
            "[1000]\ttrain's wSMAPE: 0.400772\tvalid's wSMAPE: 0.494933\n",
            "[1200]\ttrain's wSMAPE: 0.384826\tvalid's wSMAPE: 0.492\n",
            "[1400]\ttrain's wSMAPE: 0.370195\tvalid's wSMAPE: 0.489654\n",
            "[1600]\ttrain's wSMAPE: 0.356549\tvalid's wSMAPE: 0.487584\n",
            "[1800]\ttrain's wSMAPE: 0.344045\tvalid's wSMAPE: 0.486035\n",
            "[2000]\ttrain's wSMAPE: 0.33228\tvalid's wSMAPE: 0.484767\n",
            "[2200]\ttrain's wSMAPE: 0.32095\tvalid's wSMAPE: 0.483731\n",
            "[2400]\ttrain's wSMAPE: 0.310503\tvalid's wSMAPE: 0.483131\n",
            "[2600]\ttrain's wSMAPE: 0.30043\tvalid's wSMAPE: 0.482393\n",
            "[2800]\ttrain's wSMAPE: 0.290746\tvalid's wSMAPE: 0.481881\n",
            "[3000]\ttrain's wSMAPE: 0.281848\tvalid's wSMAPE: 0.481257\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2996]\ttrain's wSMAPE: 0.282048\tvalid's wSMAPE: 0.481244\n",
            "[0]\ttrain-rmse:55.89370\tvalid-rmse:53.81063\n",
            "[200]\ttrain-rmse:9.88991\tvalid-rmse:19.38842\n",
            "[400]\ttrain-rmse:7.10063\tvalid-rmse:18.97595\n",
            "[600]\ttrain-rmse:5.53886\tvalid-rmse:18.80631\n",
            "[800]\ttrain-rmse:4.59479\tvalid-rmse:18.81840\n",
            "[838]\ttrain-rmse:4.47036\tvalid-rmse:18.82675\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\ttrain's wSMAPE: 0.509093\tvalid's wSMAPE: 0.540021\n",
            "[400]\ttrain's wSMAPE: 0.463848\tvalid's wSMAPE: 0.513301\n",
            "[600]\ttrain's wSMAPE: 0.438767\tvalid's wSMAPE: 0.503926\n",
            "[800]\ttrain's wSMAPE: 0.418383\tvalid's wSMAPE: 0.498655\n",
            "[1000]\ttrain's wSMAPE: 0.400596\tvalid's wSMAPE: 0.494868\n",
            "[1200]\ttrain's wSMAPE: 0.38507\tvalid's wSMAPE: 0.492203\n",
            "[1400]\ttrain's wSMAPE: 0.370348\tvalid's wSMAPE: 0.490145\n",
            "[1600]\ttrain's wSMAPE: 0.356791\tvalid's wSMAPE: 0.487991\n",
            "[1800]\ttrain's wSMAPE: 0.344224\tvalid's wSMAPE: 0.486315\n",
            "[2000]\ttrain's wSMAPE: 0.332252\tvalid's wSMAPE: 0.484788\n",
            "[2200]\ttrain's wSMAPE: 0.321302\tvalid's wSMAPE: 0.483609\n",
            "[2400]\ttrain's wSMAPE: 0.310678\tvalid's wSMAPE: 0.482677\n",
            "[2600]\ttrain's wSMAPE: 0.300774\tvalid's wSMAPE: 0.481631\n",
            "[2800]\ttrain's wSMAPE: 0.291643\tvalid's wSMAPE: 0.480826\n",
            "[3000]\ttrain's wSMAPE: 0.282632\tvalid's wSMAPE: 0.480173\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2983]\ttrain's wSMAPE: 0.283421\tvalid's wSMAPE: 0.480122\n",
            "[0]\ttrain-rmse:53.86536\tvalid-rmse:61.61442\n",
            "[200]\ttrain-rmse:9.76068\tvalid-rmse:23.18966\n",
            "[400]\ttrain-rmse:6.97296\tvalid-rmse:22.81989\n",
            "[600]\ttrain-rmse:5.45064\tvalid-rmse:22.88459\n",
            "[676]\ttrain-rmse:5.08875\tvalid-rmse:22.81710\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\ttrain's wSMAPE: 0.509145\tvalid's wSMAPE: 0.535046\n",
            "[400]\ttrain's wSMAPE: 0.464191\tvalid's wSMAPE: 0.509543\n",
            "[600]\ttrain's wSMAPE: 0.439153\tvalid's wSMAPE: 0.501414\n",
            "[800]\ttrain's wSMAPE: 0.418158\tvalid's wSMAPE: 0.496054\n",
            "[1000]\ttrain's wSMAPE: 0.400363\tvalid's wSMAPE: 0.492702\n",
            "[1200]\ttrain's wSMAPE: 0.384565\tvalid's wSMAPE: 0.490293\n",
            "[1400]\ttrain's wSMAPE: 0.369745\tvalid's wSMAPE: 0.48834\n",
            "[1600]\ttrain's wSMAPE: 0.356006\tvalid's wSMAPE: 0.486689\n",
            "[1800]\ttrain's wSMAPE: 0.343371\tvalid's wSMAPE: 0.485513\n",
            "[2000]\ttrain's wSMAPE: 0.331453\tvalid's wSMAPE: 0.48446\n",
            "[2200]\ttrain's wSMAPE: 0.320621\tvalid's wSMAPE: 0.483507\n",
            "[2400]\ttrain's wSMAPE: 0.310043\tvalid's wSMAPE: 0.482939\n",
            "[2600]\ttrain's wSMAPE: 0.300051\tvalid's wSMAPE: 0.482225\n",
            "[2800]\ttrain's wSMAPE: 0.290996\tvalid's wSMAPE: 0.481715\n",
            "[3000]\ttrain's wSMAPE: 0.281948\tvalid's wSMAPE: 0.481169\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2991]\ttrain's wSMAPE: 0.282312\tvalid's wSMAPE: 0.481072\n",
            "[0]\ttrain-rmse:55.09258\tvalid-rmse:57.06589\n",
            "[200]\ttrain-rmse:9.96328\tvalid-rmse:20.30589\n",
            "[400]\ttrain-rmse:7.19242\tvalid-rmse:20.21627\n",
            "[483]\ttrain-rmse:6.43146\tvalid-rmse:20.18480\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\ttrain's wSMAPE: 0.510526\tvalid's wSMAPE: 0.534202\n",
            "[400]\ttrain's wSMAPE: 0.466457\tvalid's wSMAPE: 0.510168\n",
            "[600]\ttrain's wSMAPE: 0.441264\tvalid's wSMAPE: 0.501462\n",
            "[800]\ttrain's wSMAPE: 0.421046\tvalid's wSMAPE: 0.49656\n",
            "[1000]\ttrain's wSMAPE: 0.403278\tvalid's wSMAPE: 0.493461\n",
            "[1200]\ttrain's wSMAPE: 0.387168\tvalid's wSMAPE: 0.491209\n",
            "[1400]\ttrain's wSMAPE: 0.37231\tvalid's wSMAPE: 0.489391\n",
            "[1600]\ttrain's wSMAPE: 0.358448\tvalid's wSMAPE: 0.488001\n",
            "[1800]\ttrain's wSMAPE: 0.345572\tvalid's wSMAPE: 0.486622\n",
            "[2000]\ttrain's wSMAPE: 0.33391\tvalid's wSMAPE: 0.485602\n",
            "[2200]\ttrain's wSMAPE: 0.322397\tvalid's wSMAPE: 0.48471\n",
            "[2400]\ttrain's wSMAPE: 0.311761\tvalid's wSMAPE: 0.484365\n",
            "[2600]\ttrain's wSMAPE: 0.301897\tvalid's wSMAPE: 0.483861\n",
            "[2800]\ttrain's wSMAPE: 0.292281\tvalid's wSMAPE: 0.483038\n",
            "[3000]\ttrain's wSMAPE: 0.283283\tvalid's wSMAPE: 0.482263\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2996]\ttrain's wSMAPE: 0.283397\tvalid's wSMAPE: 0.482236\n",
            "[0]\ttrain-rmse:56.47086\tvalid-rmse:51.28013\n",
            "[200]\ttrain-rmse:9.83839\tvalid-rmse:20.53918\n",
            "[400]\ttrain-rmse:7.11995\tvalid-rmse:20.30047\n",
            "[600]\ttrain-rmse:5.67185\tvalid-rmse:20.36242\n",
            "[658]\ttrain-rmse:5.32974\tvalid-rmse:20.34263\n",
            "  - Horizon +4d\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\ttrain's wSMAPE: 0.510818\tvalid's wSMAPE: 0.537472\n",
            "[400]\ttrain's wSMAPE: 0.465897\tvalid's wSMAPE: 0.511587\n",
            "[600]\ttrain's wSMAPE: 0.441666\tvalid's wSMAPE: 0.504022\n",
            "[800]\ttrain's wSMAPE: 0.421261\tvalid's wSMAPE: 0.498955\n",
            "[1000]\ttrain's wSMAPE: 0.403316\tvalid's wSMAPE: 0.495745\n",
            "[1200]\ttrain's wSMAPE: 0.387526\tvalid's wSMAPE: 0.493285\n",
            "[1400]\ttrain's wSMAPE: 0.372614\tvalid's wSMAPE: 0.490906\n",
            "[1600]\ttrain's wSMAPE: 0.358875\tvalid's wSMAPE: 0.4895\n",
            "[1800]\ttrain's wSMAPE: 0.346468\tvalid's wSMAPE: 0.488054\n",
            "[2000]\ttrain's wSMAPE: 0.334471\tvalid's wSMAPE: 0.486726\n",
            "[2200]\ttrain's wSMAPE: 0.323243\tvalid's wSMAPE: 0.485898\n",
            "[2400]\ttrain's wSMAPE: 0.312948\tvalid's wSMAPE: 0.485332\n",
            "[2600]\ttrain's wSMAPE: 0.303178\tvalid's wSMAPE: 0.484593\n",
            "[2800]\ttrain's wSMAPE: 0.293866\tvalid's wSMAPE: 0.483774\n",
            "[3000]\ttrain's wSMAPE: 0.284693\tvalid's wSMAPE: 0.483165\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2989]\ttrain's wSMAPE: 0.285143\tvalid's wSMAPE: 0.483102\n",
            "[0]\ttrain-rmse:56.31672\tvalid-rmse:51.43139\n",
            "[200]\ttrain-rmse:10.23810\tvalid-rmse:18.80007\n",
            "[400]\ttrain-rmse:7.23853\tvalid-rmse:18.42030\n",
            "[600]\ttrain-rmse:5.75632\tvalid-rmse:18.29725\n",
            "[800]\ttrain-rmse:4.74334\tvalid-rmse:18.23873\n",
            "[1000]\ttrain-rmse:4.01680\tvalid-rmse:18.30521\n",
            "[1140]\ttrain-rmse:3.64073\tvalid-rmse:18.33020\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\ttrain's wSMAPE: 0.508969\tvalid's wSMAPE: 0.543835\n",
            "[400]\ttrain's wSMAPE: 0.464173\tvalid's wSMAPE: 0.517892\n",
            "[600]\ttrain's wSMAPE: 0.438384\tvalid's wSMAPE: 0.508941\n",
            "[800]\ttrain's wSMAPE: 0.41814\tvalid's wSMAPE: 0.503314\n",
            "[1000]\ttrain's wSMAPE: 0.400182\tvalid's wSMAPE: 0.499058\n",
            "[1200]\ttrain's wSMAPE: 0.384348\tvalid's wSMAPE: 0.496087\n",
            "[1400]\ttrain's wSMAPE: 0.369641\tvalid's wSMAPE: 0.494308\n",
            "[1600]\ttrain's wSMAPE: 0.356153\tvalid's wSMAPE: 0.492546\n",
            "[1800]\ttrain's wSMAPE: 0.343231\tvalid's wSMAPE: 0.49101\n",
            "[2000]\ttrain's wSMAPE: 0.331225\tvalid's wSMAPE: 0.489803\n",
            "[2200]\ttrain's wSMAPE: 0.320429\tvalid's wSMAPE: 0.488786\n",
            "[2400]\ttrain's wSMAPE: 0.309637\tvalid's wSMAPE: 0.487811\n",
            "[2600]\ttrain's wSMAPE: 0.29999\tvalid's wSMAPE: 0.487331\n",
            "[2800]\ttrain's wSMAPE: 0.290676\tvalid's wSMAPE: 0.486543\n",
            "[3000]\ttrain's wSMAPE: 0.281733\tvalid's wSMAPE: 0.485956\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[3000]\ttrain's wSMAPE: 0.281733\tvalid's wSMAPE: 0.485956\n",
            "[0]\ttrain-rmse:55.47972\tvalid-rmse:55.10971\n",
            "[200]\ttrain-rmse:9.78593\tvalid-rmse:24.44946\n",
            "[400]\ttrain-rmse:7.04766\tvalid-rmse:23.74297\n",
            "[600]\ttrain-rmse:5.54810\tvalid-rmse:23.46855\n",
            "[800]\ttrain-rmse:4.52220\tvalid-rmse:23.37175\n",
            "[1000]\ttrain-rmse:3.85957\tvalid-rmse:23.32614\n",
            "[1200]\ttrain-rmse:3.33929\tvalid-rmse:23.31602\n",
            "[1400]\ttrain-rmse:2.92065\tvalid-rmse:23.29944\n",
            "[1445]\ttrain-rmse:2.83366\tvalid-rmse:23.28281\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\ttrain's wSMAPE: 0.510845\tvalid's wSMAPE: 0.535155\n",
            "[400]\ttrain's wSMAPE: 0.464911\tvalid's wSMAPE: 0.510074\n",
            "[600]\ttrain's wSMAPE: 0.440097\tvalid's wSMAPE: 0.502014\n",
            "[800]\ttrain's wSMAPE: 0.420178\tvalid's wSMAPE: 0.496918\n",
            "[1000]\ttrain's wSMAPE: 0.402628\tvalid's wSMAPE: 0.493097\n",
            "[1200]\ttrain's wSMAPE: 0.386807\tvalid's wSMAPE: 0.490235\n",
            "[1400]\ttrain's wSMAPE: 0.372172\tvalid's wSMAPE: 0.488063\n",
            "[1600]\ttrain's wSMAPE: 0.358951\tvalid's wSMAPE: 0.486546\n",
            "[1800]\ttrain's wSMAPE: 0.345749\tvalid's wSMAPE: 0.485109\n",
            "[2000]\ttrain's wSMAPE: 0.333921\tvalid's wSMAPE: 0.48395\n",
            "[2200]\ttrain's wSMAPE: 0.322677\tvalid's wSMAPE: 0.482972\n",
            "[2400]\ttrain's wSMAPE: 0.312018\tvalid's wSMAPE: 0.481922\n",
            "[2600]\ttrain's wSMAPE: 0.301906\tvalid's wSMAPE: 0.481016\n",
            "[2800]\ttrain's wSMAPE: 0.292486\tvalid's wSMAPE: 0.480438\n",
            "[3000]\ttrain's wSMAPE: 0.283685\tvalid's wSMAPE: 0.480061\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2986]\ttrain's wSMAPE: 0.284245\tvalid's wSMAPE: 0.479983\n",
            "[0]\ttrain-rmse:54.85968\tvalid-rmse:57.46551\n",
            "[200]\ttrain-rmse:10.00479\tvalid-rmse:20.91959\n",
            "[400]\ttrain-rmse:7.10935\tvalid-rmse:20.59316\n",
            "[600]\ttrain-rmse:5.62431\tvalid-rmse:20.54504\n",
            "[739]\ttrain-rmse:4.93305\tvalid-rmse:20.53113\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\ttrain's wSMAPE: 0.509112\tvalid's wSMAPE: 0.541245\n",
            "[400]\ttrain's wSMAPE: 0.463669\tvalid's wSMAPE: 0.514013\n",
            "[600]\ttrain's wSMAPE: 0.439222\tvalid's wSMAPE: 0.505425\n",
            "[800]\ttrain's wSMAPE: 0.418883\tvalid's wSMAPE: 0.499934\n",
            "[1000]\ttrain's wSMAPE: 0.401171\tvalid's wSMAPE: 0.496759\n",
            "[1200]\ttrain's wSMAPE: 0.385417\tvalid's wSMAPE: 0.493678\n",
            "[1400]\ttrain's wSMAPE: 0.370859\tvalid's wSMAPE: 0.491414\n",
            "[1600]\ttrain's wSMAPE: 0.357163\tvalid's wSMAPE: 0.489474\n",
            "[1800]\ttrain's wSMAPE: 0.344863\tvalid's wSMAPE: 0.488012\n",
            "[2000]\ttrain's wSMAPE: 0.332851\tvalid's wSMAPE: 0.487079\n",
            "[2200]\ttrain's wSMAPE: 0.321947\tvalid's wSMAPE: 0.486203\n",
            "[2400]\ttrain's wSMAPE: 0.311544\tvalid's wSMAPE: 0.485519\n",
            "[2600]\ttrain's wSMAPE: 0.30187\tvalid's wSMAPE: 0.484994\n",
            "[2800]\ttrain's wSMAPE: 0.292875\tvalid's wSMAPE: 0.484357\n",
            "[3000]\ttrain's wSMAPE: 0.283845\tvalid's wSMAPE: 0.48379\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2998]\ttrain's wSMAPE: 0.283916\tvalid's wSMAPE: 0.483779\n",
            "[0]\ttrain-rmse:54.59011\tvalid-rmse:58.54085\n",
            "[200]\ttrain-rmse:9.88545\tvalid-rmse:22.36810\n",
            "[400]\ttrain-rmse:7.07093\tvalid-rmse:22.21552\n",
            "[600]\ttrain-rmse:5.52760\tvalid-rmse:22.27191\n",
            "[658]\ttrain-rmse:5.22535\tvalid-rmse:22.26392\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\ttrain's wSMAPE: 0.510594\tvalid's wSMAPE: 0.539666\n",
            "[400]\ttrain's wSMAPE: 0.465844\tvalid's wSMAPE: 0.513411\n",
            "[600]\ttrain's wSMAPE: 0.441652\tvalid's wSMAPE: 0.50511\n",
            "[800]\ttrain's wSMAPE: 0.421605\tvalid's wSMAPE: 0.499703\n",
            "[1000]\ttrain's wSMAPE: 0.40395\tvalid's wSMAPE: 0.495718\n",
            "[1200]\ttrain's wSMAPE: 0.387913\tvalid's wSMAPE: 0.492939\n",
            "[1400]\ttrain's wSMAPE: 0.37332\tvalid's wSMAPE: 0.490532\n",
            "[1600]\ttrain's wSMAPE: 0.360138\tvalid's wSMAPE: 0.488345\n",
            "[1800]\ttrain's wSMAPE: 0.347557\tvalid's wSMAPE: 0.486427\n",
            "[2000]\ttrain's wSMAPE: 0.335788\tvalid's wSMAPE: 0.484692\n",
            "[2200]\ttrain's wSMAPE: 0.324802\tvalid's wSMAPE: 0.48352\n",
            "[2400]\ttrain's wSMAPE: 0.314345\tvalid's wSMAPE: 0.482812\n",
            "[2600]\ttrain's wSMAPE: 0.304487\tvalid's wSMAPE: 0.482272\n",
            "[2800]\ttrain's wSMAPE: 0.295079\tvalid's wSMAPE: 0.48148\n",
            "[3000]\ttrain's wSMAPE: 0.286015\tvalid's wSMAPE: 0.480855\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[3000]\ttrain's wSMAPE: 0.286015\tvalid's wSMAPE: 0.480855\n",
            "[0]\ttrain-rmse:55.68009\tvalid-rmse:54.22426\n",
            "[200]\ttrain-rmse:9.96999\tvalid-rmse:20.90421\n",
            "[400]\ttrain-rmse:7.17565\tvalid-rmse:20.53023\n",
            "[592]\ttrain-rmse:5.76479\tvalid-rmse:20.54810\n",
            "  - Horizon +5d\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\ttrain's wSMAPE: 0.509767\tvalid's wSMAPE: 0.536685\n",
            "[400]\ttrain's wSMAPE: 0.465414\tvalid's wSMAPE: 0.510235\n",
            "[600]\ttrain's wSMAPE: 0.441024\tvalid's wSMAPE: 0.501682\n",
            "[800]\ttrain's wSMAPE: 0.421084\tvalid's wSMAPE: 0.496181\n",
            "[1000]\ttrain's wSMAPE: 0.403312\tvalid's wSMAPE: 0.492709\n",
            "[1200]\ttrain's wSMAPE: 0.387266\tvalid's wSMAPE: 0.490325\n",
            "[1400]\ttrain's wSMAPE: 0.372916\tvalid's wSMAPE: 0.488196\n",
            "[1600]\ttrain's wSMAPE: 0.359711\tvalid's wSMAPE: 0.486639\n",
            "[1800]\ttrain's wSMAPE: 0.3472\tvalid's wSMAPE: 0.485375\n",
            "[2000]\ttrain's wSMAPE: 0.335681\tvalid's wSMAPE: 0.484151\n",
            "[2200]\ttrain's wSMAPE: 0.324217\tvalid's wSMAPE: 0.483153\n",
            "[2400]\ttrain's wSMAPE: 0.313901\tvalid's wSMAPE: 0.482413\n",
            "[2600]\ttrain's wSMAPE: 0.304001\tvalid's wSMAPE: 0.481779\n",
            "[2800]\ttrain's wSMAPE: 0.294641\tvalid's wSMAPE: 0.481092\n",
            "[3000]\ttrain's wSMAPE: 0.285817\tvalid's wSMAPE: 0.48085\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2964]\ttrain's wSMAPE: 0.287379\tvalid's wSMAPE: 0.480834\n",
            "[0]\ttrain-rmse:55.89125\tvalid-rmse:52.95550\n",
            "[200]\ttrain-rmse:9.82598\tvalid-rmse:19.05767\n",
            "[400]\ttrain-rmse:7.02930\tvalid-rmse:18.74823\n",
            "[600]\ttrain-rmse:5.65004\tvalid-rmse:18.60249\n",
            "[800]\ttrain-rmse:4.74020\tvalid-rmse:18.59657\n",
            "[874]\ttrain-rmse:4.45440\tvalid-rmse:18.57960\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\ttrain's wSMAPE: 0.510133\tvalid's wSMAPE: 0.539979\n",
            "[400]\ttrain's wSMAPE: 0.465394\tvalid's wSMAPE: 0.514706\n",
            "[600]\ttrain's wSMAPE: 0.439932\tvalid's wSMAPE: 0.50618\n",
            "[800]\ttrain's wSMAPE: 0.419908\tvalid's wSMAPE: 0.501021\n",
            "[1000]\ttrain's wSMAPE: 0.402176\tvalid's wSMAPE: 0.497333\n",
            "[1200]\ttrain's wSMAPE: 0.38622\tvalid's wSMAPE: 0.494573\n",
            "[1400]\ttrain's wSMAPE: 0.37132\tvalid's wSMAPE: 0.491752\n",
            "[1600]\ttrain's wSMAPE: 0.3577\tvalid's wSMAPE: 0.490182\n",
            "[1800]\ttrain's wSMAPE: 0.34513\tvalid's wSMAPE: 0.48882\n",
            "[2000]\ttrain's wSMAPE: 0.332784\tvalid's wSMAPE: 0.487478\n",
            "[2200]\ttrain's wSMAPE: 0.321772\tvalid's wSMAPE: 0.486245\n",
            "[2400]\ttrain's wSMAPE: 0.310998\tvalid's wSMAPE: 0.485226\n",
            "[2600]\ttrain's wSMAPE: 0.301067\tvalid's wSMAPE: 0.484209\n",
            "[2800]\ttrain's wSMAPE: 0.291383\tvalid's wSMAPE: 0.483562\n",
            "[3000]\ttrain's wSMAPE: 0.282345\tvalid's wSMAPE: 0.482995\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[3000]\ttrain's wSMAPE: 0.282345\tvalid's wSMAPE: 0.482995\n",
            "[0]\ttrain-rmse:55.63160\tvalid-rmse:54.09464\n",
            "[200]\ttrain-rmse:10.00193\tvalid-rmse:20.29524\n",
            "[400]\ttrain-rmse:7.09078\tvalid-rmse:19.98687\n",
            "[600]\ttrain-rmse:5.54692\tvalid-rmse:19.95275\n",
            "[689]\ttrain-rmse:5.08474\tvalid-rmse:19.94255\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\ttrain's wSMAPE: 0.512192\tvalid's wSMAPE: 0.534607\n",
            "[400]\ttrain's wSMAPE: 0.466804\tvalid's wSMAPE: 0.50867\n",
            "[600]\ttrain's wSMAPE: 0.442059\tvalid's wSMAPE: 0.500436\n",
            "[800]\ttrain's wSMAPE: 0.421086\tvalid's wSMAPE: 0.495119\n",
            "[1000]\ttrain's wSMAPE: 0.403172\tvalid's wSMAPE: 0.491419\n",
            "[1200]\ttrain's wSMAPE: 0.387108\tvalid's wSMAPE: 0.488522\n",
            "[1400]\ttrain's wSMAPE: 0.371841\tvalid's wSMAPE: 0.486319\n",
            "[1600]\ttrain's wSMAPE: 0.357973\tvalid's wSMAPE: 0.484661\n",
            "[1800]\ttrain's wSMAPE: 0.345409\tvalid's wSMAPE: 0.483536\n",
            "[2000]\ttrain's wSMAPE: 0.333558\tvalid's wSMAPE: 0.482979\n",
            "[2200]\ttrain's wSMAPE: 0.322457\tvalid's wSMAPE: 0.482038\n",
            "[2400]\ttrain's wSMAPE: 0.311838\tvalid's wSMAPE: 0.481316\n",
            "[2600]\ttrain's wSMAPE: 0.301923\tvalid's wSMAPE: 0.480749\n",
            "[2800]\ttrain's wSMAPE: 0.292423\tvalid's wSMAPE: 0.480289\n",
            "[3000]\ttrain's wSMAPE: 0.283356\tvalid's wSMAPE: 0.479759\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2995]\ttrain's wSMAPE: 0.283557\tvalid's wSMAPE: 0.479721\n",
            "[0]\ttrain-rmse:54.14490\tvalid-rmse:59.82011\n",
            "[200]\ttrain-rmse:10.00005\tvalid-rmse:22.48197\n",
            "[400]\ttrain-rmse:7.15042\tvalid-rmse:22.12559\n",
            "[600]\ttrain-rmse:5.60888\tvalid-rmse:22.15557\n",
            "[624]\ttrain-rmse:5.45240\tvalid-rmse:22.12927\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\ttrain's wSMAPE: 0.506882\tvalid's wSMAPE: 0.546501\n",
            "[400]\ttrain's wSMAPE: 0.46244\tvalid's wSMAPE: 0.521154\n",
            "[600]\ttrain's wSMAPE: 0.43811\tvalid's wSMAPE: 0.513136\n",
            "[800]\ttrain's wSMAPE: 0.418059\tvalid's wSMAPE: 0.508037\n",
            "[1000]\ttrain's wSMAPE: 0.400176\tvalid's wSMAPE: 0.504792\n",
            "[1200]\ttrain's wSMAPE: 0.384262\tvalid's wSMAPE: 0.502037\n",
            "[1400]\ttrain's wSMAPE: 0.369692\tvalid's wSMAPE: 0.499533\n",
            "[1600]\ttrain's wSMAPE: 0.356609\tvalid's wSMAPE: 0.498076\n",
            "[1800]\ttrain's wSMAPE: 0.344226\tvalid's wSMAPE: 0.496537\n",
            "[2000]\ttrain's wSMAPE: 0.332432\tvalid's wSMAPE: 0.494709\n",
            "[2200]\ttrain's wSMAPE: 0.321521\tvalid's wSMAPE: 0.493753\n",
            "[2400]\ttrain's wSMAPE: 0.31107\tvalid's wSMAPE: 0.492797\n",
            "[2600]\ttrain's wSMAPE: 0.301608\tvalid's wSMAPE: 0.491809\n",
            "[2800]\ttrain's wSMAPE: 0.292438\tvalid's wSMAPE: 0.491198\n",
            "[3000]\ttrain's wSMAPE: 0.283312\tvalid's wSMAPE: 0.490388\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[3000]\ttrain's wSMAPE: 0.283312\tvalid's wSMAPE: 0.490388\n",
            "[0]\ttrain-rmse:54.52730\tvalid-rmse:58.51689\n",
            "[200]\ttrain-rmse:9.86571\tvalid-rmse:23.57852\n",
            "[400]\ttrain-rmse:7.09265\tvalid-rmse:23.42165\n",
            "[600]\ttrain-rmse:5.59908\tvalid-rmse:23.47516\n",
            "[633]\ttrain-rmse:5.41485\tvalid-rmse:23.48987\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\ttrain's wSMAPE: 0.513414\tvalid's wSMAPE: 0.541131\n",
            "[400]\ttrain's wSMAPE: 0.468842\tvalid's wSMAPE: 0.514598\n",
            "[600]\ttrain's wSMAPE: 0.44373\tvalid's wSMAPE: 0.505361\n",
            "[800]\ttrain's wSMAPE: 0.422833\tvalid's wSMAPE: 0.500045\n",
            "[1000]\ttrain's wSMAPE: 0.40453\tvalid's wSMAPE: 0.496357\n",
            "[1200]\ttrain's wSMAPE: 0.388584\tvalid's wSMAPE: 0.493763\n",
            "[1400]\ttrain's wSMAPE: 0.373843\tvalid's wSMAPE: 0.491511\n",
            "[1600]\ttrain's wSMAPE: 0.360129\tvalid's wSMAPE: 0.489728\n",
            "[1800]\ttrain's wSMAPE: 0.347243\tvalid's wSMAPE: 0.488198\n",
            "[2000]\ttrain's wSMAPE: 0.335478\tvalid's wSMAPE: 0.487023\n",
            "[2200]\ttrain's wSMAPE: 0.324117\tvalid's wSMAPE: 0.486278\n",
            "[2400]\ttrain's wSMAPE: 0.313438\tvalid's wSMAPE: 0.485449\n",
            "[2600]\ttrain's wSMAPE: 0.303202\tvalid's wSMAPE: 0.484846\n",
            "[2800]\ttrain's wSMAPE: 0.293854\tvalid's wSMAPE: 0.484246\n",
            "[3000]\ttrain's wSMAPE: 0.285031\tvalid's wSMAPE: 0.483779\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2991]\ttrain's wSMAPE: 0.285386\tvalid's wSMAPE: 0.483724\n",
            "[0]\ttrain-rmse:56.40258\tvalid-rmse:50.79850\n",
            "[200]\ttrain-rmse:10.10302\tvalid-rmse:20.32594\n",
            "[400]\ttrain-rmse:7.21364\tvalid-rmse:19.99713\n",
            "[600]\ttrain-rmse:5.70407\tvalid-rmse:19.92813\n",
            "[800]\ttrain-rmse:4.71268\tvalid-rmse:19.88954\n",
            "[922]\ttrain-rmse:4.27703\tvalid-rmse:19.89362\n",
            "  - Horizon +6d\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\ttrain's wSMAPE: 0.511054\tvalid's wSMAPE: 0.543162\n",
            "[400]\ttrain's wSMAPE: 0.465913\tvalid's wSMAPE: 0.519688\n",
            "[600]\ttrain's wSMAPE: 0.441082\tvalid's wSMAPE: 0.511577\n",
            "[800]\ttrain's wSMAPE: 0.420694\tvalid's wSMAPE: 0.507078\n",
            "[1000]\ttrain's wSMAPE: 0.402852\tvalid's wSMAPE: 0.50449\n",
            "[1200]\ttrain's wSMAPE: 0.386668\tvalid's wSMAPE: 0.501479\n",
            "[1400]\ttrain's wSMAPE: 0.371923\tvalid's wSMAPE: 0.499159\n",
            "[1600]\ttrain's wSMAPE: 0.358415\tvalid's wSMAPE: 0.497487\n",
            "[1800]\ttrain's wSMAPE: 0.345577\tvalid's wSMAPE: 0.495582\n",
            "[2000]\ttrain's wSMAPE: 0.333694\tvalid's wSMAPE: 0.494109\n",
            "[2200]\ttrain's wSMAPE: 0.322722\tvalid's wSMAPE: 0.493086\n",
            "[2400]\ttrain's wSMAPE: 0.312328\tvalid's wSMAPE: 0.492295\n",
            "[2600]\ttrain's wSMAPE: 0.302638\tvalid's wSMAPE: 0.49189\n",
            "[2800]\ttrain's wSMAPE: 0.293374\tvalid's wSMAPE: 0.491395\n",
            "[3000]\ttrain's wSMAPE: 0.284292\tvalid's wSMAPE: 0.490744\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2985]\ttrain's wSMAPE: 0.284921\tvalid's wSMAPE: 0.490676\n",
            "[0]\ttrain-rmse:56.14811\tvalid-rmse:51.31034\n",
            "[200]\ttrain-rmse:10.05314\tvalid-rmse:19.34053\n",
            "[400]\ttrain-rmse:7.21516\tvalid-rmse:18.82772\n",
            "[600]\ttrain-rmse:5.74648\tvalid-rmse:18.68488\n",
            "[800]\ttrain-rmse:4.79104\tvalid-rmse:18.65117\n",
            "[1000]\ttrain-rmse:4.11671\tvalid-rmse:18.65479\n",
            "[1011]\ttrain-rmse:4.07862\tvalid-rmse:18.65037\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\ttrain's wSMAPE: 0.510688\tvalid's wSMAPE: 0.541758\n",
            "[400]\ttrain's wSMAPE: 0.465143\tvalid's wSMAPE: 0.514724\n",
            "[600]\ttrain's wSMAPE: 0.440267\tvalid's wSMAPE: 0.506624\n",
            "[800]\ttrain's wSMAPE: 0.419977\tvalid's wSMAPE: 0.50121\n",
            "[1000]\ttrain's wSMAPE: 0.402223\tvalid's wSMAPE: 0.49778\n",
            "[1200]\ttrain's wSMAPE: 0.385933\tvalid's wSMAPE: 0.494909\n",
            "[1400]\ttrain's wSMAPE: 0.37157\tvalid's wSMAPE: 0.492727\n",
            "[1600]\ttrain's wSMAPE: 0.357772\tvalid's wSMAPE: 0.490718\n",
            "[1800]\ttrain's wSMAPE: 0.34492\tvalid's wSMAPE: 0.48917\n",
            "[2000]\ttrain's wSMAPE: 0.332971\tvalid's wSMAPE: 0.488063\n",
            "[2200]\ttrain's wSMAPE: 0.3218\tvalid's wSMAPE: 0.48749\n",
            "[2400]\ttrain's wSMAPE: 0.311106\tvalid's wSMAPE: 0.486928\n",
            "[2600]\ttrain's wSMAPE: 0.301063\tvalid's wSMAPE: 0.486326\n",
            "[2800]\ttrain's wSMAPE: 0.291498\tvalid's wSMAPE: 0.485382\n",
            "[3000]\ttrain's wSMAPE: 0.282709\tvalid's wSMAPE: 0.484613\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2992]\ttrain's wSMAPE: 0.283064\tvalid's wSMAPE: 0.484599\n",
            "[0]\ttrain-rmse:55.20114\tvalid-rmse:55.35463\n",
            "[200]\ttrain-rmse:9.96538\tvalid-rmse:21.58739\n",
            "[400]\ttrain-rmse:7.17858\tvalid-rmse:21.29970\n",
            "[568]\ttrain-rmse:5.83536\tvalid-rmse:21.33408\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\ttrain's wSMAPE: 0.511266\tvalid's wSMAPE: 0.538982\n",
            "[400]\ttrain's wSMAPE: 0.466233\tvalid's wSMAPE: 0.511983\n",
            "[600]\ttrain's wSMAPE: 0.441459\tvalid's wSMAPE: 0.502922\n",
            "[800]\ttrain's wSMAPE: 0.421446\tvalid's wSMAPE: 0.49729\n",
            "[1000]\ttrain's wSMAPE: 0.403402\tvalid's wSMAPE: 0.493023\n",
            "[1200]\ttrain's wSMAPE: 0.387189\tvalid's wSMAPE: 0.490127\n",
            "[1400]\ttrain's wSMAPE: 0.372267\tvalid's wSMAPE: 0.487721\n",
            "[1600]\ttrain's wSMAPE: 0.35861\tvalid's wSMAPE: 0.485261\n",
            "[1800]\ttrain's wSMAPE: 0.346064\tvalid's wSMAPE: 0.484001\n",
            "[2000]\ttrain's wSMAPE: 0.33401\tvalid's wSMAPE: 0.482723\n",
            "[2200]\ttrain's wSMAPE: 0.322453\tvalid's wSMAPE: 0.481593\n",
            "[2400]\ttrain's wSMAPE: 0.312186\tvalid's wSMAPE: 0.481035\n",
            "[2600]\ttrain's wSMAPE: 0.302234\tvalid's wSMAPE: 0.480191\n",
            "[2800]\ttrain's wSMAPE: 0.293216\tvalid's wSMAPE: 0.479457\n",
            "[3000]\ttrain's wSMAPE: 0.284518\tvalid's wSMAPE: 0.479117\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2991]\ttrain's wSMAPE: 0.284884\tvalid's wSMAPE: 0.47908\n",
            "[0]\ttrain-rmse:54.97238\tvalid-rmse:56.25314\n",
            "[200]\ttrain-rmse:9.99597\tvalid-rmse:19.79449\n",
            "[400]\ttrain-rmse:7.19749\tvalid-rmse:19.48750\n",
            "[600]\ttrain-rmse:5.71136\tvalid-rmse:19.39853\n",
            "[800]\ttrain-rmse:4.73868\tvalid-rmse:19.36929\n",
            "[979]\ttrain-rmse:4.12853\tvalid-rmse:19.39463\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\ttrain's wSMAPE: 0.510195\tvalid's wSMAPE: 0.544215\n",
            "[400]\ttrain's wSMAPE: 0.464458\tvalid's wSMAPE: 0.516542\n",
            "[600]\ttrain's wSMAPE: 0.440035\tvalid's wSMAPE: 0.507878\n",
            "[800]\ttrain's wSMAPE: 0.419777\tvalid's wSMAPE: 0.502179\n",
            "[1000]\ttrain's wSMAPE: 0.402141\tvalid's wSMAPE: 0.498346\n",
            "[1200]\ttrain's wSMAPE: 0.386366\tvalid's wSMAPE: 0.495485\n",
            "[1400]\ttrain's wSMAPE: 0.371566\tvalid's wSMAPE: 0.493409\n",
            "[1600]\ttrain's wSMAPE: 0.358406\tvalid's wSMAPE: 0.491387\n",
            "[1800]\ttrain's wSMAPE: 0.345814\tvalid's wSMAPE: 0.490234\n",
            "[2000]\ttrain's wSMAPE: 0.33398\tvalid's wSMAPE: 0.488912\n",
            "[2200]\ttrain's wSMAPE: 0.322897\tvalid's wSMAPE: 0.487868\n",
            "[2400]\ttrain's wSMAPE: 0.312627\tvalid's wSMAPE: 0.487138\n",
            "[2600]\ttrain's wSMAPE: 0.302839\tvalid's wSMAPE: 0.486453\n",
            "[2800]\ttrain's wSMAPE: 0.293356\tvalid's wSMAPE: 0.485803\n",
            "[3000]\ttrain's wSMAPE: 0.28464\tvalid's wSMAPE: 0.485652\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2876]\ttrain's wSMAPE: 0.290032\tvalid's wSMAPE: 0.485622\n",
            "[0]\ttrain-rmse:54.21702\tvalid-rmse:59.16349\n",
            "[200]\ttrain-rmse:10.00927\tvalid-rmse:23.03942\n",
            "[400]\ttrain-rmse:7.07345\tvalid-rmse:22.89456\n",
            "[556]\ttrain-rmse:5.82815\tvalid-rmse:22.93493\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\ttrain's wSMAPE: 0.513586\tvalid's wSMAPE: 0.534813\n",
            "[400]\ttrain's wSMAPE: 0.468441\tvalid's wSMAPE: 0.508756\n",
            "[600]\ttrain's wSMAPE: 0.443504\tvalid's wSMAPE: 0.499821\n",
            "[800]\ttrain's wSMAPE: 0.422694\tvalid's wSMAPE: 0.494545\n",
            "[1000]\ttrain's wSMAPE: 0.404989\tvalid's wSMAPE: 0.491084\n",
            "[1200]\ttrain's wSMAPE: 0.388949\tvalid's wSMAPE: 0.488088\n",
            "[1400]\ttrain's wSMAPE: 0.374179\tvalid's wSMAPE: 0.485874\n",
            "[1600]\ttrain's wSMAPE: 0.36039\tvalid's wSMAPE: 0.484118\n",
            "[1800]\ttrain's wSMAPE: 0.347594\tvalid's wSMAPE: 0.482184\n",
            "[2000]\ttrain's wSMAPE: 0.335729\tvalid's wSMAPE: 0.481093\n",
            "[2200]\ttrain's wSMAPE: 0.32456\tvalid's wSMAPE: 0.480145\n",
            "[2400]\ttrain's wSMAPE: 0.314052\tvalid's wSMAPE: 0.479707\n",
            "[2600]\ttrain's wSMAPE: 0.30434\tvalid's wSMAPE: 0.478944\n",
            "[2800]\ttrain's wSMAPE: 0.294915\tvalid's wSMAPE: 0.478333\n",
            "[3000]\ttrain's wSMAPE: 0.285913\tvalid's wSMAPE: 0.477656\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2998]\ttrain's wSMAPE: 0.286013\tvalid's wSMAPE: 0.477631\n",
            "[0]\ttrain-rmse:55.57770\tvalid-rmse:53.79797\n",
            "[200]\ttrain-rmse:9.96673\tvalid-rmse:21.58190\n",
            "[400]\ttrain-rmse:7.11685\tvalid-rmse:21.20226\n",
            "[600]\ttrain-rmse:5.60421\tvalid-rmse:21.12458\n",
            "[800]\ttrain-rmse:4.68317\tvalid-rmse:21.16288\n",
            "[875]\ttrain-rmse:4.44163\tvalid-rmse:21.17274\n",
            "  - Horizon +7d\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\ttrain's wSMAPE: 0.513487\tvalid's wSMAPE: 0.542391\n",
            "[400]\ttrain's wSMAPE: 0.468774\tvalid's wSMAPE: 0.516176\n",
            "[600]\ttrain's wSMAPE: 0.443955\tvalid's wSMAPE: 0.507843\n",
            "[800]\ttrain's wSMAPE: 0.42351\tvalid's wSMAPE: 0.502157\n",
            "[1000]\ttrain's wSMAPE: 0.405632\tvalid's wSMAPE: 0.49819\n",
            "[1200]\ttrain's wSMAPE: 0.389118\tvalid's wSMAPE: 0.495001\n",
            "[1400]\ttrain's wSMAPE: 0.374854\tvalid's wSMAPE: 0.493146\n",
            "[1600]\ttrain's wSMAPE: 0.361617\tvalid's wSMAPE: 0.491283\n",
            "[1800]\ttrain's wSMAPE: 0.348983\tvalid's wSMAPE: 0.489635\n",
            "[2000]\ttrain's wSMAPE: 0.336794\tvalid's wSMAPE: 0.488498\n",
            "[2200]\ttrain's wSMAPE: 0.325859\tvalid's wSMAPE: 0.487556\n",
            "[2400]\ttrain's wSMAPE: 0.315134\tvalid's wSMAPE: 0.486871\n",
            "[2600]\ttrain's wSMAPE: 0.305503\tvalid's wSMAPE: 0.486191\n",
            "[2800]\ttrain's wSMAPE: 0.29639\tvalid's wSMAPE: 0.48535\n",
            "[3000]\ttrain's wSMAPE: 0.287504\tvalid's wSMAPE: 0.484649\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2982]\ttrain's wSMAPE: 0.288345\tvalid's wSMAPE: 0.48461\n",
            "[0]\ttrain-rmse:56.47406\tvalid-rmse:48.61075\n",
            "[200]\ttrain-rmse:10.32935\tvalid-rmse:19.56943\n",
            "[400]\ttrain-rmse:7.43595\tvalid-rmse:19.13334\n",
            "[600]\ttrain-rmse:5.87571\tvalid-rmse:19.09631\n",
            "[800]\ttrain-rmse:4.98281\tvalid-rmse:19.08121\n",
            "[867]\ttrain-rmse:4.67752\tvalid-rmse:19.10744\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\ttrain's wSMAPE: 0.510471\tvalid's wSMAPE: 0.546912\n",
            "[400]\ttrain's wSMAPE: 0.465537\tvalid's wSMAPE: 0.522001\n",
            "[600]\ttrain's wSMAPE: 0.440793\tvalid's wSMAPE: 0.51368\n",
            "[800]\ttrain's wSMAPE: 0.420524\tvalid's wSMAPE: 0.508226\n",
            "[1000]\ttrain's wSMAPE: 0.402679\tvalid's wSMAPE: 0.504608\n",
            "[1200]\ttrain's wSMAPE: 0.386868\tvalid's wSMAPE: 0.501932\n",
            "[1400]\ttrain's wSMAPE: 0.372095\tvalid's wSMAPE: 0.499471\n",
            "[1600]\ttrain's wSMAPE: 0.358235\tvalid's wSMAPE: 0.497972\n",
            "[1800]\ttrain's wSMAPE: 0.345456\tvalid's wSMAPE: 0.496231\n",
            "[2000]\ttrain's wSMAPE: 0.333386\tvalid's wSMAPE: 0.495016\n",
            "[2200]\ttrain's wSMAPE: 0.322035\tvalid's wSMAPE: 0.494131\n",
            "[2400]\ttrain's wSMAPE: 0.311407\tvalid's wSMAPE: 0.493287\n",
            "[2600]\ttrain's wSMAPE: 0.301599\tvalid's wSMAPE: 0.492688\n",
            "[2800]\ttrain's wSMAPE: 0.292249\tvalid's wSMAPE: 0.49201\n",
            "[3000]\ttrain's wSMAPE: 0.283273\tvalid's wSMAPE: 0.49132\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[3000]\ttrain's wSMAPE: 0.283273\tvalid's wSMAPE: 0.49132\n",
            "[0]\ttrain-rmse:56.28837\tvalid-rmse:49.51725\n",
            "[200]\ttrain-rmse:10.27473\tvalid-rmse:20.37595\n",
            "[400]\ttrain-rmse:7.27394\tvalid-rmse:19.88130\n",
            "[600]\ttrain-rmse:5.74358\tvalid-rmse:19.81669\n",
            "[740]\ttrain-rmse:5.04503\tvalid-rmse:19.84845\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\ttrain's wSMAPE: 0.514723\tvalid's wSMAPE: 0.536306\n",
            "[400]\ttrain's wSMAPE: 0.469315\tvalid's wSMAPE: 0.509237\n",
            "[600]\ttrain's wSMAPE: 0.444225\tvalid's wSMAPE: 0.500417\n",
            "[800]\ttrain's wSMAPE: 0.423791\tvalid's wSMAPE: 0.495181\n",
            "[1000]\ttrain's wSMAPE: 0.405558\tvalid's wSMAPE: 0.491781\n",
            "[1200]\ttrain's wSMAPE: 0.389431\tvalid's wSMAPE: 0.488818\n",
            "[1400]\ttrain's wSMAPE: 0.374924\tvalid's wSMAPE: 0.486683\n",
            "[1600]\ttrain's wSMAPE: 0.361431\tvalid's wSMAPE: 0.484999\n",
            "[1800]\ttrain's wSMAPE: 0.348947\tvalid's wSMAPE: 0.483293\n",
            "[2000]\ttrain's wSMAPE: 0.3372\tvalid's wSMAPE: 0.482098\n",
            "[2200]\ttrain's wSMAPE: 0.326038\tvalid's wSMAPE: 0.480911\n",
            "[2400]\ttrain's wSMAPE: 0.315839\tvalid's wSMAPE: 0.479797\n",
            "[2600]\ttrain's wSMAPE: 0.306168\tvalid's wSMAPE: 0.479183\n",
            "[2800]\ttrain's wSMAPE: 0.297111\tvalid's wSMAPE: 0.478529\n",
            "[3000]\ttrain's wSMAPE: 0.28815\tvalid's wSMAPE: 0.477586\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2985]\ttrain's wSMAPE: 0.288786\tvalid's wSMAPE: 0.477579\n",
            "[0]\ttrain-rmse:54.12561\tvalid-rmse:58.39401\n",
            "[200]\ttrain-rmse:10.11711\tvalid-rmse:20.57347\n",
            "[400]\ttrain-rmse:7.29072\tvalid-rmse:20.17792\n",
            "[600]\ttrain-rmse:5.85182\tvalid-rmse:20.09625\n",
            "[800]\ttrain-rmse:4.91170\tvalid-rmse:20.06441\n",
            "[1000]\ttrain-rmse:4.22940\tvalid-rmse:20.05769\n",
            "[1095]\ttrain-rmse:3.99063\tvalid-rmse:20.05034\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\ttrain's wSMAPE: 0.512465\tvalid's wSMAPE: 0.538618\n",
            "[400]\ttrain's wSMAPE: 0.46628\tvalid's wSMAPE: 0.512715\n",
            "[600]\ttrain's wSMAPE: 0.441649\tvalid's wSMAPE: 0.505662\n",
            "[800]\ttrain's wSMAPE: 0.420613\tvalid's wSMAPE: 0.500325\n",
            "[1000]\ttrain's wSMAPE: 0.402845\tvalid's wSMAPE: 0.497158\n",
            "[1200]\ttrain's wSMAPE: 0.386831\tvalid's wSMAPE: 0.494379\n",
            "[1400]\ttrain's wSMAPE: 0.372421\tvalid's wSMAPE: 0.492004\n",
            "[1600]\ttrain's wSMAPE: 0.358991\tvalid's wSMAPE: 0.490538\n",
            "[1800]\ttrain's wSMAPE: 0.346456\tvalid's wSMAPE: 0.489199\n",
            "[2000]\ttrain's wSMAPE: 0.334643\tvalid's wSMAPE: 0.488231\n",
            "[2200]\ttrain's wSMAPE: 0.323668\tvalid's wSMAPE: 0.487541\n",
            "[2400]\ttrain's wSMAPE: 0.31354\tvalid's wSMAPE: 0.486764\n",
            "[2600]\ttrain's wSMAPE: 0.30376\tvalid's wSMAPE: 0.486164\n",
            "[2800]\ttrain's wSMAPE: 0.294461\tvalid's wSMAPE: 0.485834\n",
            "[3000]\ttrain's wSMAPE: 0.285711\tvalid's wSMAPE: 0.485155\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[3000]\ttrain's wSMAPE: 0.285711\tvalid's wSMAPE: 0.485155\n",
            "[0]\ttrain-rmse:53.20180\tvalid-rmse:61.78348\n",
            "[200]\ttrain-rmse:9.90795\tvalid-rmse:22.47082\n",
            "[400]\ttrain-rmse:7.11587\tvalid-rmse:22.25177\n",
            "[596]\ttrain-rmse:5.73587\tvalid-rmse:22.34553\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[200]\ttrain's wSMAPE: 0.513956\tvalid's wSMAPE: 0.542721\n",
            "[400]\ttrain's wSMAPE: 0.468864\tvalid's wSMAPE: 0.516046\n",
            "[600]\ttrain's wSMAPE: 0.443707\tvalid's wSMAPE: 0.507712\n",
            "[800]\ttrain's wSMAPE: 0.422895\tvalid's wSMAPE: 0.502149\n",
            "[1000]\ttrain's wSMAPE: 0.404373\tvalid's wSMAPE: 0.4984\n",
            "[1200]\ttrain's wSMAPE: 0.388051\tvalid's wSMAPE: 0.495723\n",
            "[1400]\ttrain's wSMAPE: 0.373166\tvalid's wSMAPE: 0.493639\n",
            "[1600]\ttrain's wSMAPE: 0.359328\tvalid's wSMAPE: 0.491934\n",
            "[1800]\ttrain's wSMAPE: 0.346937\tvalid's wSMAPE: 0.490488\n",
            "[2000]\ttrain's wSMAPE: 0.334808\tvalid's wSMAPE: 0.488542\n",
            "[2200]\ttrain's wSMAPE: 0.32363\tvalid's wSMAPE: 0.487655\n",
            "[2400]\ttrain's wSMAPE: 0.313289\tvalid's wSMAPE: 0.486757\n",
            "[2600]\ttrain's wSMAPE: 0.303418\tvalid's wSMAPE: 0.48593\n",
            "[2800]\ttrain's wSMAPE: 0.294249\tvalid's wSMAPE: 0.485367\n",
            "[3000]\ttrain's wSMAPE: 0.285539\tvalid's wSMAPE: 0.484728\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2983]\ttrain's wSMAPE: 0.286248\tvalid's wSMAPE: 0.484692\n",
            "[0]\ttrain-rmse:54.84947\tvalid-rmse:55.67950\n",
            "[200]\ttrain-rmse:10.09954\tvalid-rmse:22.97367\n",
            "[400]\ttrain-rmse:7.32414\tvalid-rmse:22.82882\n",
            "[600]\ttrain-rmse:5.87555\tvalid-rmse:22.83470\n",
            "[670]\ttrain-rmse:5.47546\tvalid-rmse:22.86526\n",
            "[OOF] weighted SMAPE (LGB+XGB+LSTM dyn /, OOB_BLEND=0.4, OOB_TAU=0.1)  0.4605\n",
            "       tuned  buckets: 49  |  buckets: 49 | q buckets: 49\n",
            "[Calibrate] gamma(h,dow) & beta(h,dow) from blended OOF ...\n",
            "  gamma_h (fallback): [0.927 0.918 0.918 0.922 0.919 0.917 0.917]\n",
            "  beta_h  (fallback): [0. 0. 0. 0. 0. 0. 0.]\n",
            "  gamma_hd keys: 49  beta_hd keys: 49\n",
            "[Calibrate] store / (store,dow) / (store,h) ratios from blended OOF ...\n",
            "[Calibrate] (store, winter_flag) & (store, weekend_or_holiday) ratios ...\n",
            "[Calibrate] (store, season_id) ratios from blended OOF ...\n",
            "  calibration maps ready.\n",
            "[Calibrate] winsor caps & positive floors ...\n",
            "[Retrain] full data with multi-seed bagging (LGB+XGB+(opt)LSTM) ...\n",
            "[Predict] build submission from TEST files ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 523 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7fc50f345da0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 524 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7fc50f347920> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DONE] saved: data/submission.csv\n",
            "            BBQ_1    BBQ_BBQ55()  \\\n",
            "0  TEST_00+1            5.450146             15.428449   \n",
            "1  TEST_00+2            4.288397             26.338662   \n",
            "2  TEST_00+3            3.345662             18.321037   \n",
            "\n",
            "    BBQ_ 30,000   BBQ_ 60,000   BBQ_ 90,000  \\\n",
            "0                3.517208                2.153524                1.103003   \n",
            "1                2.309348                1.541055                1.018989   \n",
            "2                2.005174                1.367700                1.021128   \n",
            "\n",
            "    BBQ_ ()   BBQ_   BBQ_   BBQ_   \\\n",
            "0               5.661681        1.956199       1.066400            1.548349   \n",
            "1               8.134324        1.706862       1.003731            1.868043   \n",
            "2               4.527285        1.652653       0.937654            1.499352   \n",
            "\n",
            "   ...  _  _   _  _  _  \\\n",
            "0  ...     5.179641      13.120575   11.626100  4.857456   51.325788   \n",
            "1  ...     2.971162       6.001856    4.839233  2.494127   18.084399   \n",
            "2  ...     2.524446       4.951125    3.884673  2.348494   15.567208   \n",
            "\n",
            "   _  _ HOT  _ ICE  _ ICE  \\\n",
            "0     33.825220         4.903203        34.918652        7.979195   \n",
            "1     12.906853         2.533520        13.725248        3.772785   \n",
            "2     12.906916         2.832539        15.236298        3.824792   \n",
            "\n",
            "   _  \n",
            "0     14.106519  \n",
            "1      4.927125  \n",
            "2      4.529322  \n",
            "\n",
            "[3 rows x 168 columns]\n",
            "[Check] same columns?  True\n",
            "[Check] same order?    True\n",
            "[Check] same dates?    True\n",
            "[Check] NaNs count:    0\n",
            "[Check] shape:         (70, 168)  / sample: (70, 168)\n"
          ]
        }
      ],
      "source": [
        "import os, glob, re, math, gc, warnings, random, unicodedata\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# ---- TensorFlow  LSTM ----\n",
        "TF_OK = False\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    from tensorflow import keras\n",
        "    from tensorflow.keras import layers\n",
        "    TF_OK = True\n",
        "    print(\"[Info] TensorFlow detected. LSTM head ENABLED.\")\n",
        "except Exception as e:\n",
        "    print(\"[Info] TensorFlow not available. LSTM head DISABLED.\", e)\n",
        "\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# ---------------- Repro & blend knobs ----------------\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED)\n",
        "if TF_OK:\n",
        "    tf.random.set_seed(SEED)\n",
        "\n",
        "# OOF   /(h,DOW)    (  )\n",
        "OOB_BLEND = 0.40   # OOF XGB  ( )\n",
        "OOB_TAU   = 0.10   # OOF LSTM  ( , )\n",
        "\n",
        "#   XGB/LSTM  ( )\n",
        "ALPHA_XGB = 0.40\n",
        "TAU_LSTM  = 0.10\n",
        "\n",
        "# + (   )\n",
        "MAX_AX_SUM = 0.70\n",
        "\n",
        "# ---------------- robust path resolver ----------------\n",
        "def _pick(*candidates):\n",
        "    for p in candidates:\n",
        "        if isinstance(p, str) and os.path.exists(p):\n",
        "            return p\n",
        "    return candidates[-1]\n",
        "\n",
        "def _first_existing(paths):\n",
        "    for p in paths:\n",
        "        if os.path.exists(p):\n",
        "            return p\n",
        "    return None\n",
        "\n",
        "BASE_DIRS = [\"data\", \"data\", \"./\"]\n",
        "TRAIN_DIRS = [\"data/train\", \"data\", \"train\", \"data\", \"./\"]\n",
        "TEST_DIRS  = [\"data/test\", \"data\", \"test\", \"data\", \"./\"]\n",
        "\n",
        "BASE_DIR  = _first_existing(BASE_DIRS) or \"./\"\n",
        "TRAIN_DIR = _first_existing(TRAIN_DIRS) or BASE_DIR\n",
        "TEST_DIR  = _first_existing(TEST_DIRS) or BASE_DIR\n",
        "\n",
        "# train.csv (/  )\n",
        "TRAIN_PATH = _pick(os.path.join(TRAIN_DIR, \"train.csv\"),\n",
        "                   os.path.join(BASE_DIR, \"train.csv\"),\n",
        "                   os.path.join(TRAIN_DIR, \"TRAIN.csv\"),\n",
        "                   os.path.join(BASE_DIR, \"TRAIN.csv\"))\n",
        "\n",
        "# sample_submission.csv\n",
        "SAMPLE_SUB_PATH = _pick(os.path.join(BASE_DIR, \"sample_submission.csv\"),\n",
        "                        os.path.join(TRAIN_DIR, \"sample_submission.csv\"),\n",
        "                        os.path.join(BASE_DIR, \"SAMPLE_SUBMISSION.csv\"),\n",
        "                        os.path.join(TRAIN_DIR, \"SAMPLE_SUBMISSION.csv\"))\n",
        "\n",
        "# TEST_*.csv (train/test     )\n",
        "TEST_FILES = sorted(\n",
        "    glob.glob(os.path.join(TEST_DIR, 'TEST_*.csv')) +\n",
        "    glob.glob(os.path.join(BASE_DIR, 'TEST_*.csv'))\n",
        ")\n",
        "\n",
        "#  \n",
        "OUT_PATH = os.path.join(BASE_DIR, 'submission.csv')\n",
        "\n",
        "print(f\"[Path] BASE_DIR={BASE_DIR}\")\n",
        "print(f\"       TRAIN_DIR={TRAIN_DIR}\")\n",
        "print(f\"       TEST_DIR={TEST_DIR}\")\n",
        "print(f\"       TRAIN_PATH={TRAIN_PATH}\")\n",
        "print(f\"       SAMPLE_SUB_PATH={SAMPLE_SUB_PATH}\")\n",
        "print(f\"       #TEST_FILES={len(TEST_FILES)}\")\n",
        "\n",
        "# ---------------- Utils ----------------\n",
        "def _canon_text(s: str) -> str:\n",
        "    if s is None: return ''\n",
        "    s = unicodedata.normalize('NFKC', str(s))\n",
        "    return s.replace('\\ufeff','').replace('\\u200b','').replace('\\xa0','').strip()\n",
        "\n",
        "def _clean_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    return df.rename(columns={c:_canon_text(c) for c in df.columns})\n",
        "\n",
        "def _force_kor_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    m = { 'date':'', 'key':'_', 'y':'' }\n",
        "    return df.rename(columns={k:v for k,v in m.items() if k in df.columns})\n",
        "\n",
        "def _normalize_key_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    if '_' in df.columns:\n",
        "        df['_'] = df['_'].map(_canon_text)\n",
        "    return df\n",
        "\n",
        "# --- Calibration shrink helper ---\n",
        "def _shrink(x, tau=0.5):\n",
        "    return 1.0 + float(tau) * (float(x) - 1.0)\n",
        "\n",
        "TAU = dict(\n",
        "    gamma=1.05,  #   (  )\n",
        "    store=0.40,\n",
        "    sdow=0.40,\n",
        "    sh=0.40,\n",
        "    winter=0.50,\n",
        "    wk=0.25,\n",
        "    season=0.30,\n",
        "    aff=0.20\n",
        ")\n",
        "\n",
        "# ---------------- Domain: stores/holidays ----------------\n",
        "HEAVY_STORES = {'',''}\n",
        "HEAVY_W = 1.35  # (/  )\n",
        "def store_weight(name: str) -> float:\n",
        "    for s in HEAVY_STORES:\n",
        "        if str(name).startswith(s):\n",
        "            return HEAVY_W\n",
        "    return 1.0\n",
        "\n",
        "# / feature helpers\n",
        "def is_hwadam_store(s: str) -> bool:\n",
        "    s = str(s)\n",
        "    return s.startswith('') or s.startswith('')\n",
        "\n",
        "def _season_id_from_dt(dt: pd.Timestamp) -> int:\n",
        "    \"\"\"0: spring(3~5), 1: summer(6~8), 2: autumn(9~11), 3: winter(12~2)\"\"\"\n",
        "    m = int(dt.month)\n",
        "    return 1 if 6<=m<=8 else (2 if 9<=m<=11 else (0 if 3<=m<=5 else 3))\n",
        "\n",
        "def _spring_autumn_active(dt: pd.Timestamp) -> int:\n",
        "    \"\"\"  (/ , 46, 911)\"\"\"\n",
        "    m = int(dt.month)\n",
        "    return int(m in [4,5,6,9,10,11])\n",
        "\n",
        "def _summer_family(dt: pd.Timestamp) -> int:\n",
        "    \"\"\" ()\"\"\"\n",
        "    return int(int(dt.month) in [7,8])\n",
        "\n",
        "# -  prior\n",
        "STORE_SEASON_AFFINITY = {\n",
        "    ('','spring'): 1.15, ('','autumn'): 1.20,\n",
        "    ('','spring'): 1.15, ('','autumn'): 1.20,\n",
        "    ('','summer'): 1.08, (' BBQ','summer'): 1.10,\n",
        "}\n",
        "\n",
        "K_HOLS = set(pd.to_datetime([\n",
        "    # 2023\n",
        "    \"2023-01-01\",\"2023-01-21\",\"2023-01-22\",\"2023-01-23\",\"2023-01-24\",\n",
        "    \"2023-03-01\",\"2023-05-05\",\"2023-05-27\",\"2023-06-06\",\"2023-08-15\",\n",
        "    \"2023-09-28\",\"2023-09-29\",\"2023-09-30\",\"2023-10-03\",\"2023-10-09\",\n",
        "    \"2023-12-25\",\n",
        "    # 2024\n",
        "    \"2024-01-01\",\"2024-02-09\",\"2024-02-10\",\"2024-02-11\",\"2024-02-12\",\n",
        "    \"2024-03-01\",\"2024-05-05\",\"2024-05-06\",\"2024-05-15\",\"2024-06-06\",\n",
        "    \"2024-08-15\",\"2024-09-16\",\"2024-09-17\",\"2024-09-18\",\"2024-10-03\",\n",
        "    \"2024-10-09\",\"2024-12-25\",\n",
        "    # 2025\n",
        "    \"2025-01-01\",\"2025-01-27\",\"2025-01-28\",\"2025-01-29\",\n",
        "    \"2025-03-01\",\"2025-05-05\",\"2025-05-06\",\"2025-06-06\",\"2025-08-15\",\n",
        "    \"2025-10-03\",\"2025-10-06\",\"2025-10-07\",\"2025-10-08\",\"2025-10-09\",\n",
        "    \"2025-12-25\",\n",
        "]).date)\n",
        "\n",
        "def is_holiday(ts) -> pd.Series:\n",
        "    td = pd.to_datetime(ts)\n",
        "    if not isinstance(td, pd.Series):\n",
        "        td = pd.Series(td)\n",
        "    return td.dt.date.map(lambda d: int(d in K_HOLS)).astype(int)\n",
        "\n",
        "def season_code(m: int) -> int:\n",
        "    if m in [12,1,2]: return 0   # =0, =1, =2, =3\n",
        "    if m in [3,4,5]:  return 1\n",
        "    if m in [6,7,8]:  return 2\n",
        "    return 3\n",
        "\n",
        "def _is_winter_date(dt: pd.Timestamp) -> int:\n",
        "    m = dt.month; d = dt.day\n",
        "    return int((m==12 and d>=10) or (m in [1,2]) or (m==3 and d<=10))\n",
        "\n",
        "# ---------------- Store/menu ecosystem priors ----------------\n",
        "_STORE_PRIOR_RAW = {\n",
        "    '': 47.84,\n",
        "    '': 34.38,\n",
        "    '': 18.86,\n",
        "    '': 5.50,\n",
        "    '': 5.50,\n",
        "    '': 1.31\n",
        "}\n",
        "_sp_mean = float(np.mean(list(_STORE_PRIOR_RAW.values()))) if _STORE_PRIOR_RAW else 1.0\n",
        "STORE_PRIOR = {k: (v/_sp_mean if _sp_mean>0 else 1.0) for k,v in _STORE_PRIOR_RAW.items()}\n",
        "def store_prior_index(store: str) -> float:\n",
        "    return float(STORE_PRIOR.get(str(store), 1.0))\n",
        "\n",
        "# ---------------- Time-pattern priors (/) ----------------\n",
        "_MONTH_PRIOR_RAW = {1:21.64, 2:17.37, 3:2.64, 10:15.47, 12:13.71}\n",
        "_m_mean = float(np.mean(list(_MONTH_PRIOR_RAW.values()))) if _MONTH_PRIOR_RAW else 1.0\n",
        "MONTH_PRIOR = {m: (v/_m_mean if _m_mean>0 else 1.0) for m,v in _MONTH_PRIOR_RAW.items()}\n",
        "\n",
        "_DOW_PRIOR_RAW = {0:7.76, 4:12.26, 5:15.28, 6:12.72}\n",
        "_d_mean = float(np.mean(list(_DOW_PRIOR_RAW.values()))) if _DOW_PRIOR_RAW else 1.0\n",
        "DOW_PRIOR = {d: (v/_d_mean if _d_mean>0 else 1.0) for d,v in _DOW_PRIOR_RAW.items()}\n",
        "\n",
        "def month_prior(m: int) -> float:\n",
        "    return float(MONTH_PRIOR.get(int(m), 1.0))\n",
        "\n",
        "def dow_prior(d: int) -> float:\n",
        "    return float(DOW_PRIOR.get(int(d), 1.0))\n",
        "\n",
        "# ---------------- General helpers ----------------\n",
        "def split_store_menu(x: str):\n",
        "    x = str(x)\n",
        "    if \"_\" in x:\n",
        "        p = x.find(\"_\"); return x[:p], x[p+1:]\n",
        "    return \"UNKNOWN\", x\n",
        "\n",
        "def ensure_full_daily_index_multi(df, value_cols):\n",
        "    df = df.copy()\n",
        "    df[''] = pd.to_datetime(df[''])\n",
        "    out=[]\n",
        "    for name,g in df.groupby('_'):\n",
        "        g = g.sort_values('')\n",
        "        idx = pd.date_range(g[''].min(), g[''].max(), freq=\"D\")\n",
        "        gg = g.set_index('').reindex(idx).rename_axis('').reset_index()\n",
        "        gg['_'] = name\n",
        "        for c in value_cols:\n",
        "            gg[c] = gg[c].fillna(0.0)\n",
        "        out.append(gg)\n",
        "    return pd.concat(out, ignore_index=True)\n",
        "\n",
        "def build_feature_from_window(y28: np.ndarray, y28_raw: np.ndarray):\n",
        "    f={}\n",
        "    y = np.clip(y28.astype(float), 0.0, None)\n",
        "    yr = y28_raw.astype(float)\n",
        "\n",
        "    for i in range(28):\n",
        "        f[f'lag_{i+1}'] = float(y[-(i+1)])\n",
        "\n",
        "    def stats(prefix, arr):\n",
        "        f[f'{prefix}_mean'] = float(arr.mean())\n",
        "        f[f'{prefix}_std']  = float(arr.std(ddof=0))\n",
        "        f[f'{prefix}_max']  = float(arr.max())\n",
        "        f[f'{prefix}_sum']  = float(arr.sum())\n",
        "        f[f'{prefix}_zero_share'] = float((arr==0).mean())\n",
        "        f[f'{prefix}_nz_share']   = float((arr>0).mean())\n",
        "    stats('r7',  y[-7:])\n",
        "    stats('r14', y[-14:])\n",
        "    stats('r28', y[-28:])\n",
        "\n",
        "    a7 = y[-7:].mean(); a14 = y[-14:-7].mean() if y[-14:-7].size>0 else 0.0\n",
        "    f['mom_7_7']  = float(a7/(a14+1e-6))\n",
        "    f['mom_7_28'] = float(a7/(y.mean()+1e-6))\n",
        "    x = np.arange(1,29); ylog = np.log1p(y)\n",
        "    denom = np.sum((x-x.mean())**2); slope = 0.0 if denom==0 else np.sum((x-x.mean())*(ylog-ylog.mean()))/denom\n",
        "    f['trend_slope'] = float(slope)\n",
        "\n",
        "    nz_idx = np.where(y>0)[0]\n",
        "    f['days_since_last_nz'] = float(28 - (nz_idx[-1]+1) if len(nz_idx)>0 else 28)\n",
        "    f['last_nz_val'] = float(y[nz_idx[-1]] if len(nz_idx)>0 else 0.0)\n",
        "\n",
        "    neg_mask = (yr < 0)\n",
        "    f['neg_cnt_7']  = float(neg_mask[-7:].sum())\n",
        "    f['neg_cnt_14'] = float(neg_mask[-14:].sum())\n",
        "    f['neg_cnt_28'] = float(neg_mask[-28:].sum())\n",
        "    f['neg_sum_abs_28'] = float(np.abs(yr[neg_mask][-28:]).sum()) if neg_mask.any() else 0.0\n",
        "    f['neg_min_28'] = float(yr[-28:].min())\n",
        "    f['neg_share_28'] = float((yr[-28:]<0).mean())\n",
        "    neg_idx = np.where(yr<0)[0]\n",
        "    f['days_since_last_neg'] = float(28 - (neg_idx[-1]+1) if len(neg_idx)>0 else 28)\n",
        "    return f\n",
        "\n",
        "# ---------------- Load ----------------\n",
        "print(f\"[Load] {TRAIN_PATH}\")\n",
        "train_raw = pd.read_csv(TRAIN_PATH)\n",
        "train_raw = _clean_columns(train_raw); train_raw = _force_kor_cols(train_raw)\n",
        "train_raw = train_raw.rename(columns={\"\":\"date\",\"_\":\"key\",\"\":\"y_raw\"})\n",
        "train_raw[\"date\"] = pd.to_datetime(train_raw[\"date\"])\n",
        "train_raw[\"y_raw\"] = train_raw[\"y_raw\"].astype(float)\n",
        "\n",
        "train = train_raw.copy()\n",
        "train[\"y\"] = train[\"y_raw\"].clip(lower=0.0)\n",
        "train[\"store\"], train[\"menu\"] = zip(*train[\"key\"].map(split_store_menu))\n",
        "\n",
        "print(\"Train:\", train.shape, \"| Period:\", train[\"date\"].min().date(), \"~\", train[\"date\"].max().date())\n",
        "print(\"Unique series:\", train[\"key\"].nunique(), \"| Stores:\", train[\"store\"].nunique())\n",
        "print(f\"Zero share: {(train['y']==0).mean():.3f}\")\n",
        "\n",
        "train_full = ensure_full_daily_index_multi(\n",
        "    train[[\"date\",\"key\",\"y\",\"y_raw\"]].rename(columns={\"date\":\"\",\"key\":\"_\"}),\n",
        "    value_cols=[\"y\",\"y_raw\"]\n",
        ")\n",
        "train_full = train_full.rename(columns={\"\":\"date\",\"_\":\"key\"})\n",
        "train_full[\"store\"], train_full[\"menu\"] = zip(*train_full[\"key\"].map(split_store_menu))\n",
        "\n",
        "# ---------------- Store totals (for peer/ecosystem) ----------------\n",
        "_store_tot_ser = train_full.groupby(['date','store'])['y'].sum()\n",
        "STORE_TOTAL_SER = _store_tot_ser.rename_axis(['date','store']).swaplevel(0,1).sort_index()\n",
        "\n",
        "def _lookup_store_total_series(store_name, dates, ser):\n",
        "    idx = pd.MultiIndex.from_arrays(\n",
        "        [np.array([store_name]*len(dates), dtype=object), pd.to_datetime(dates)],\n",
        "        names=['store','date']\n",
        "    )\n",
        "    return ser.reindex(idx, fill_value=0.0).values\n",
        "\n",
        "# ---------------- Build windows (Tabular + Seq for LSTM) ----------------\n",
        "print(\"[Windows] 28->7 supervised windows ...\")\n",
        "\n",
        "SEQ_FEAT_NAMES = ['y_clip', 'store_tot', 'peer_tot', 'dow_norm', 'is_weekend', 'is_holiday', 'is_winter']\n",
        "\n",
        "def make_supervised_windows(df_item, store_name, menu_name):\n",
        "    vals     = df_item['y'].values.astype(float)\n",
        "    vals_raw = df_item['y_raw'].values.astype(float)\n",
        "    dates    = pd.to_datetime(df_item['']).values\n",
        "    st28_all = _lookup_store_total_series(store_name, dates, STORE_TOTAL_SER)\n",
        "\n",
        "    X_rows=[]; Y_rows=[]; meta=[]; SEQ_rows=[]\n",
        "    if len(vals)<35: return X_rows, Y_rows, meta, SEQ_rows\n",
        "    for end in range(27, len(vals)-7):\n",
        "        y28  = vals[end-27:end+1]\n",
        "        y28r = vals_raw[end-27:end+1]\n",
        "        st28 = st28_all[end-27:end+1]\n",
        "        peer28 = np.clip(st28 - y28, 0.0, None)\n",
        "        dts = pd.to_datetime(dates[end-27:end+1])\n",
        "        dows = dts.dayofweek.values.astype(float)\n",
        "        wknd = (dows>=5).astype(float)\n",
        "        hol  = is_holiday(dts).values.astype(float)\n",
        "        winter  = np.array([_is_winter_date(pd.Timestamp(x)) for x in dts], dtype=float)\n",
        "        seq = np.stack([\n",
        "            np.clip(y28, 0.0, None),\n",
        "            st28,\n",
        "            peer28,\n",
        "            dows/6.0,\n",
        "            wknd,\n",
        "            hol,\n",
        "            winter\n",
        "        ], axis=1).astype(float)  # shape (28, n_seq_feat)\n",
        "\n",
        "        yf  = vals[end+1:end+8]\n",
        "        feats = build_feature_from_window(y28, y28r)\n",
        "\n",
        "        feats['store_prior']    = store_prior_index(store_name)\n",
        "        feats['store_r7_mean']  = float(st28[-7:].mean())\n",
        "        feats['store_r28_mean'] = float(st28[-28:].mean())\n",
        "        feats['peer_r7_mean']   = float(peer28[-7:].mean())\n",
        "        feats['peer_r28_mean']  = float(peer28[-28:].mean())\n",
        "        feats['share_in_store_r7']  = float(y28[-7:].sum()/(st28[-7:].sum()+1e-6))\n",
        "        feats['share_in_store_r28'] = float(y28[-28:].sum()/(st28[-28:].sum()+1e-6))\n",
        "\n",
        "        feats['store_name'] = store_name\n",
        "        feats['menu_name']  = menu_name\n",
        "        X_rows.append(feats); Y_rows.append(yf); SEQ_rows.append(seq)\n",
        "        meta.append({'anchor_date': pd.to_datetime(dates[end]),\n",
        "                     'store_name': store_name,\n",
        "                     'menu_name' : menu_name})\n",
        "    return X_rows, Y_rows, meta, SEQ_rows\n",
        "\n",
        "def df_to_windows(df_full):\n",
        "    X_all=[]; Y_all=[]; M_all=[]; S_all=[]\n",
        "    for name,g in df_full.groupby('_'):\n",
        "        store, menu = split_store_menu(name)\n",
        "        g = g.sort_values('')\n",
        "        X,Y,M,S = make_supervised_windows(g, store, menu)\n",
        "        if X:\n",
        "            X_all.extend(X); Y_all.extend(Y); M_all.extend(M); S_all.extend(S)\n",
        "    return pd.DataFrame(X_all), np.array(Y_all), pd.DataFrame(M_all), np.array(S_all)\n",
        "\n",
        "X_base, Y, META, X_SEQ = df_to_windows(\n",
        "    train_full[[\"date\",\"key\",\"y\",\"y_raw\"]].rename(columns={\"date\":\"\",\"key\":\"_\"})\n",
        ")\n",
        "META = META.reset_index(drop=True); X_base = X_base.reset_index(drop=True)\n",
        "\n",
        "# Encoders\n",
        "def fit_le_with_unk(values):\n",
        "    le = LabelEncoder()\n",
        "    uniq = pd.Series(values).astype(str).unique().tolist()\n",
        "    uniq = sorted(list(set(uniq + [\"<UNK>\"])))\n",
        "    le.fit(uniq)\n",
        "    return le\n",
        "\n",
        "le_store = fit_le_with_unk(META['store_name'])\n",
        "le_menu  = fit_le_with_unk(META['menu_name'])\n",
        "\n",
        "X_base[\"store_le\"] = le_store.transform(X_base[\"store_name\"].astype(str).fillna(\"<UNK>\"))\n",
        "X_base[\"menu_le\"]  = le_menu.transform(X_base[\"menu_name\"].astype(str).fillna(\"<UNK>\"))\n",
        "\n",
        "def add_kw(df):\n",
        "    MENU_KW = ['','','','','','','','','','','','','','',\n",
        "               '','','','','','','','','','','','','','',\n",
        "               '','BBQ','','','','']\n",
        "    STORE_KW = ['','','BBQ','','','','','','','']\n",
        "    df = df.copy()\n",
        "    df['store_name'] = df['store_name'].astype(str).fillna('')\n",
        "    df['menu_name']  = df['menu_name'].astype(str).fillna('')\n",
        "    for w in MENU_KW:\n",
        "        df[f'kw_m_{w}'] = df['menu_name'].str.contains(w).astype(int)\n",
        "    for w in STORE_KW:\n",
        "        df[f'kw_s_{w}'] = df['store_name'].str.contains(w).astype(int)\n",
        "    df['menu_len'] = df['menu_name'].str.len().astype(int)\n",
        "    return df\n",
        "\n",
        "X_base = add_kw(X_base)\n",
        "\n",
        "X_feat = X_base.drop(columns=['store_name','menu_name']).copy()\n",
        "anchor = META['anchor_date']\n",
        "stores_for_rows = META['store_name'].values\n",
        "\n",
        "print(\"Supervised X:\", X_feat.shape, \"Y:\", Y.shape, \"| Seq:\", X_SEQ.shape)\n",
        "\n",
        "# ---------------- Site/Store calendar profiles ----------------\n",
        "print(\"[Profiles] build Site/Store calendar indices (DOW & WOY) ...\")\n",
        "tf_ = train_full[['date','store','y']].copy()\n",
        "tf_['date'] = pd.to_datetime(tf_['date'])\n",
        "\n",
        "site_daily = tf_.groupby('date')['y'].sum().sort_index()\n",
        "site_mu = site_daily.mean()\n",
        "SITE_DOW_IDX = {d: (site_daily[site_daily.index.dayofweek==d].mean()/site_mu if site_mu>0 else 1.0) for d in range(7)}\n",
        "SITE_WOY_IDX = {}\n",
        "tmp = site_daily.to_frame('y')\n",
        "tmp['woy'] = tmp.index.isocalendar().week.astype(int)\n",
        "for w, g in tmp.groupby('woy'):\n",
        "    SITE_WOY_IDX[int(w)] = (g['y'].mean()/site_mu if site_mu>0 else 1.0)\n",
        "\n",
        "store_daily = tf_.groupby(['date','store'])['y'].sum().rename('y').reset_index()\n",
        "STORE_DOW_IDX = {}\n",
        "STORE_WOY_IDX = {}\n",
        "for s, g in store_daily.groupby('store'):\n",
        "    g = g.sort_values('date').copy()\n",
        "    mu = g['y'].mean()\n",
        "    if mu<=0:\n",
        "        for d in range(7): STORE_DOW_IDX[(s,d)] = 1.0\n",
        "        continue\n",
        "    g['dow'] = g['date'].dt.dayofweek\n",
        "    for d, gg in g.groupby('dow'):\n",
        "        STORE_DOW_IDX[(s,int(d))] = gg['y'].mean()/mu\n",
        "    g['woy'] = g['date'].dt.isocalendar().week.astype(int)\n",
        "    for w, gg in g.groupby('woy'):\n",
        "        STORE_WOY_IDX[(s,int(w))] = gg['y'].mean()/mu\n",
        "\n",
        "def horizon_calendar(anchor_dates: pd.Series, h: int) -> pd.DataFrame:\n",
        "    td = pd.to_datetime(anchor_dates) + pd.to_timedelta(h, unit='D') + pd.to_timedelta(1, unit='D')\n",
        "    dow = td.dt.dayofweek\n",
        "    mon = td.dt.month\n",
        "    woy = td.dt.isocalendar().week.astype(int)\n",
        "    hol = is_holiday(td)\n",
        "    hol_adj = ((td - pd.Timedelta(days=1)).dt.date.map(lambda d: int(d in K_HOLS)) |\n",
        "               (td + pd.Timedelta(days=1)).dt.date.map(lambda d: int(d in K_HOLS))).astype(int)\n",
        "    is_winter = td.map(_is_winter_date).astype(int)\n",
        "    season_id = td.map(_season_id_from_dt).astype(int)      # 0 1 2 3\n",
        "    spring_autumn_act = td.map(_spring_autumn_active).astype(int)\n",
        "    summer_fam = td.map(_summer_family).astype(int)\n",
        "\n",
        "    dow_prior_vals = dow.map(dow_prior).astype(float)\n",
        "    mon_prior_vals = mon.map(month_prior).astype(float)\n",
        "    return pd.DataFrame({\n",
        "        f'dow_h{h}': dow.astype(int),\n",
        "        f'month_h{h}': mon.astype(int),\n",
        "        f'woy_h{h}': woy.astype(int),\n",
        "        f'is_weekend_h{h}': (dow>=5).astype(int),\n",
        "        f'is_holiday_h{h}': hol,\n",
        "        f'is_hol_adj_h{h}': hol_adj,\n",
        "        f'season_h{h}': mon.map(season_code).astype(int),\n",
        "        f'season_id_h{h}': season_id,                       # 0 1 2 3\n",
        "        f'is_winter_h{h}': is_winter,\n",
        "        f'spring_autumn_active_h{h}': spring_autumn_act,\n",
        "        f'summer_family_h{h}': summer_fam,\n",
        "        f'is_winter_weekend_h{h}': (is_winter & (dow>=5)).astype(int),\n",
        "        f'dow_prior_h{h}': dow_prior_vals.values,\n",
        "        f'month_prior_h{h}': mon_prior_vals.values,\n",
        "    })\n",
        "\n",
        "def _profile_block(anchor_series: pd.Series, h: int, store_series: pd.Series) -> pd.DataFrame:\n",
        "    cal = horizon_calendar(anchor_series, h+1)\n",
        "    dow = cal[f'dow_h{h+1}'].values\n",
        "    woy = cal[f'woy_h{h+1}'].values\n",
        "    stores = store_series.values\n",
        "    site_dow = np.array([SITE_DOW_IDX.get(int(d),1.0) for d in dow], dtype=float)\n",
        "    site_woy = np.array([SITE_WOY_IDX.get(int(w),1.0) for w in woy], dtype=float)\n",
        "    store_dow= np.array([STORE_DOW_IDX.get((str(s), int(d)),1.0) for s,d in zip(stores,dow)], dtype=float)\n",
        "    store_woy= np.array([STORE_WOY_IDX.get((str(s), int(w)),1.0) for s,w in zip(stores,woy)], dtype=float)\n",
        "    return pd.DataFrame({\n",
        "        f'site_dow_idx_h{h+1}': site_dow,\n",
        "        f'site_woy_idx_h{h+1}': site_woy,\n",
        "        f'store_dow_idx_h{h+1}': store_dow,\n",
        "        f'store_woy_idx_h{h+1}': store_woy,\n",
        "    })\n",
        "\n",
        "def add_h_feats(Xb: pd.DataFrame, anchor_series: pd.Series, h: int, store_series: pd.Series):\n",
        "    cal = horizon_calendar(anchor_series, h+1)  # +1..+7\n",
        "    prof = _profile_block(anchor_series, h, store_series)\n",
        "    return pd.concat([Xb.reset_index(drop=True), cal.reset_index(drop=True), prof.reset_index(drop=True)], axis=1)\n",
        "\n",
        "# weighted SMAPE\n",
        "def smape_store_weighted(y_true, y_pred, stores_arr):\n",
        "    idx = y_true>0\n",
        "    if not np.any(idx): return 0.0\n",
        "    A = y_true[idx]; P = y_pred[idx]; S = stores_arr[idx]\n",
        "    sm = 2.0*np.abs(A-P)/(np.abs(A)+np.abs(P)+1e-8)\n",
        "    score=0.0; tot=0.0\n",
        "    for s in np.unique(S):\n",
        "        m = (S==s)\n",
        "        w = store_weight(s)\n",
        "        score += w * sm[m].mean()\n",
        "        tot   += w\n",
        "    return float(score/tot)\n",
        "\n",
        "def lgb_wsmape_feval(preds, dset):\n",
        "    y = dset.get_label()\n",
        "    w = dset.get_weight()\n",
        "    if w is None: w = np.ones_like(y)\n",
        "    mask = y>0\n",
        "    if not np.any(mask):\n",
        "        return ('wSMAPE', 0.0, False)\n",
        "    s = 2.0*np.abs(y - preds) / (np.abs(y) + np.abs(preds) + 1e-8)\n",
        "    return ('wSMAPE', float(np.average(s[mask], weights=w[mask])), False)\n",
        "\n",
        "def xgb_wsmape_feval(preds, dtrain):\n",
        "    y = dtrain.get_label()\n",
        "    w = dtrain.get_weight()\n",
        "    if w is None or len(w)==0:\n",
        "        w = np.ones_like(y)\n",
        "    mask = y>0\n",
        "    if not np.any(mask):\n",
        "        return 'wSMAPE', 0.0\n",
        "    s = 2.0*np.abs(y - preds) / (np.abs(y) + np.abs(preds) + 1e-8)\n",
        "    return 'wSMAPE', float(np.average(s[mask], weights=w[mask]))\n",
        "\n",
        "# ---- XGBoost compatibility helpers (feval / prediction API) ----\n",
        "def xgb_train_compat(params, dtrain, num_boost_round, evals, early_stopping_rounds=200, verbose_eval=200):\n",
        "    \"\"\"Try custom feval; if not supported, fallback to built-in eval_metric.\"\"\"\n",
        "    try:\n",
        "        return xgb.train(params, dtrain, num_boost_round=num_boost_round, evals=evals,\n",
        "                         feval=xgb_wsmape_feval, early_stopping_rounds=early_stopping_rounds,\n",
        "                         verbose_eval=verbose_eval)\n",
        "    except TypeError:\n",
        "        params2 = dict(params)\n",
        "        params2.setdefault('eval_metric', 'rmse')\n",
        "        return xgb.train(params2, dtrain, num_boost_round=num_boost_round, evals=evals,\n",
        "                         early_stopping_rounds=early_stopping_rounds, verbose_eval=verbose_eval)\n",
        "\n",
        "def xgb_best_iteration(model):\n",
        "    bi = getattr(model, 'best_iteration', None)\n",
        "    if bi is not None:\n",
        "        return int(bi)\n",
        "    bn = getattr(model, 'best_ntree_limit', None)\n",
        "    if bn is not None and bn > 0:\n",
        "        return int(bn)\n",
        "    return None\n",
        "\n",
        "def xgb_predict_compat(model, dmatrix):\n",
        "    bi = xgb_best_iteration(model)\n",
        "    try:\n",
        "        if bi is not None:\n",
        "            return model.predict(dmatrix, iteration_range=(0, bi))\n",
        "        else:\n",
        "            return model.predict(dmatrix)\n",
        "    except TypeError:\n",
        "        bn = getattr(model, 'best_ntree_limit', None)\n",
        "        if bn is not None and bn > 0:\n",
        "            return model.predict(dmatrix, ntree_limit=bn)\n",
        "        return model.predict(dmatrix)\n",
        "\n",
        "# ---------------- Model params ----------------\n",
        "lgb_params = dict(\n",
        "    objective=\"poisson\",\n",
        "    learning_rate=0.05,\n",
        "    num_leaves=45,\n",
        "    feature_fraction=0.80,\n",
        "    bagging_fraction=0.70,\n",
        "    bagging_freq=1,\n",
        "    min_data_in_leaf=80,\n",
        "    max_depth=-1,\n",
        "    lambda_l1=1e-3,\n",
        "    lambda_l2=5e-2,\n",
        "    metric=\"None\",\n",
        "    n_estimators=3000,\n",
        "    verbosity=-1\n",
        ")\n",
        "\n",
        "# XGBoost Tweedie\n",
        "XGB_TREE_METHOD = 'hist'\n",
        "xgb_params_base = dict(\n",
        "    objective='reg:tweedie',\n",
        "    tweedie_variance_power=1.2,\n",
        "    eta=0.05,\n",
        "    max_depth=7,\n",
        "    min_child_weight=8,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    reg_lambda=1.2,\n",
        "    reg_alpha=0.0,\n",
        "    tree_method=XGB_TREE_METHOD,\n",
        "    disable_default_eval_metric=1\n",
        ")\n",
        "\n",
        "def get_cat_cols(cols):\n",
        "    cats = []\n",
        "    base_cats = ['store_le','menu_le']\n",
        "    hol_cats  = [c for c in cols if c.startswith((\n",
        "        'dow_h','month_h','woy_h','is_weekend_h','is_holiday_h','is_hol_adj_h',\n",
        "        'season_h','season_id_h','is_winter_h','is_winter_weekend_h',\n",
        "        'spring_autumn_active_h','summer_family_h'\n",
        "    ))]\n",
        "    kw_cats   = [c for c in cols if c.startswith(('kw_m_','kw_s_'))]\n",
        "    cats.extend(base_cats + hol_cats + kw_cats)\n",
        "    return sorted(list(set([c for c in cats if c in cols])))\n",
        "\n",
        "# ---------------- Train 7 heads with KFold (LGB + XGB + (opt) LSTM OOF) ----------------\n",
        "print(\"[Train] LGB, XGB & (opt) LSTM heads with weighted early stopping ...\")\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "oof_lgb = np.zeros_like(Y, dtype=float)\n",
        "oof_xgb = np.zeros_like(Y, dtype=float)\n",
        "oof_lstm = np.zeros_like(Y, dtype=float)\n",
        "models_cv_lgb = []\n",
        "models_cv_xgb = []\n",
        "models_cv_lstm = []  # list of tuples per h: [(model, mu, std), ...]\n",
        "avg_iters_lgb=[]; avg_iters_xgb=[]\n",
        "best_epochs_lstm=[]\n",
        "\n",
        "META_store_series = META['store_name']\n",
        "\n",
        "# ---- LSTM helpers ----\n",
        "def standardize_seq(x, mu, std):\n",
        "    return (x - mu) / (std + 1e-6)\n",
        "\n",
        "def compute_seq_mu_std(x_seq):\n",
        "    # x_seq: (N, T, F)\n",
        "    mu  = x_seq.reshape(-1, x_seq.shape[-1]).mean(axis=0)\n",
        "    std = x_seq.reshape(-1, x_seq.shape[-1]).std(axis=0)\n",
        "    return mu.astype('float32'), (std+1e-6).astype('float32')\n",
        "\n",
        "def build_lstm_model(seq_len, n_feat, seed=42):\n",
        "    keras.utils.set_random_seed(seed)\n",
        "    inp = keras.Input(shape=(seq_len, n_feat))\n",
        "    x = layers.Masking(mask_value=0.0)(inp)\n",
        "    x = layers.LSTM(32, return_sequences=False)(x)\n",
        "    x = layers.Dropout(0.10)(x)\n",
        "    x = layers.Dense(16, activation='relu')(x)\n",
        "    out = layers.Dense(1, activation='relu')(x)  # non-neg\n",
        "    m = keras.Model(inp, out)\n",
        "    m.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
        "              loss='mae')\n",
        "    return m\n",
        "\n",
        "for h in range(7):\n",
        "    print(f\"  - Horizon +{h+1}d\")\n",
        "    yh = Y[:,h].astype(float).clip(min=0.0)\n",
        "    Xh = add_h_feats(X_feat, anchor, h, META_store_series)\n",
        "\n",
        "    cal_h = horizon_calendar(anchor, h+1)\n",
        "    is_weekend = cal_h[f'is_weekend_h{h+1}'].values\n",
        "    mon = cal_h[f'month_h{h+1}'].values\n",
        "    is_winter = cal_h[f'is_winter_h{h+1}'].values\n",
        "\n",
        "    # recency & weights\n",
        "    max_anchor = pd.to_datetime(anchor).max()\n",
        "    days_from_edge = (pd.to_datetime(anchor) - (max_anchor - pd.Timedelta(days=240))).dt.days.clip(lower=0, upper=240).values\n",
        "    rec = 0.5 + 0.8*(days_from_edge/240.0)**2\n",
        "\n",
        "    pos = yh[yh>0]\n",
        "    thr_low = np.quantile(pos, 0.25) if pos.size>50 else (pos.mean() if pos.size>0 else 0.0)\n",
        "    lowpos_bonus = np.where((yh>0) & (yh<=thr_low), 1.10, 1.0)\n",
        "\n",
        "    sw = np.array([store_weight(s) for s in stores_for_rows]) * rec * lowpos_bonus\n",
        "    sw = sw * (1.15**(is_weekend)) * (1.10**(pd.Series(mon).isin([12,1,2]).astype(int).values))\n",
        "    sw = sw * (1.10**(is_winter)) * (1.05**(is_winter & is_weekend))\n",
        "    sw = sw * (yh>0)\n",
        "\n",
        "    #  / \n",
        "    spring_autumn_act = cal_h[f'spring_autumn_active_h{h+1}'].values.astype(int)\n",
        "    summer_fam = cal_h[f'summer_family_h{h+1}'].values.astype(int)\n",
        "    is_hwadam_arr = np.array([is_hwadam_store(s) for s in stores_for_rows]).astype(int)\n",
        "    summer_aff = np.array([STORE_SEASON_AFFINITY.get((s,'summer'),1.0) for s in stores_for_rows])\n",
        "    sw = sw * (1.05 ** (is_hwadam_arr & spring_autumn_act))\n",
        "    sw = sw * np.where((summer_fam==1) & (summer_aff>1.0), 1.03, 1.0)\n",
        "\n",
        "    cat_cols = get_cat_cols(Xh.columns.tolist())\n",
        "    fold_models_lgb=[]; fold_models_xgb=[]; fold_models_lstm=[];\n",
        "    oof_col_lgb=np.zeros_like(yh); oof_col_xgb=np.zeros_like(yh); oof_col_lstm=np.zeros_like(yh)\n",
        "\n",
        "    for fold,(tr,va) in enumerate(kf.split(Xh)):\n",
        "        # LGB\n",
        "        dtr = lgb.Dataset(Xh.iloc[tr], label=yh[tr], weight=sw[tr], categorical_feature=cat_cols, free_raw_data=False)\n",
        "        dva = lgb.Dataset(Xh.iloc[va], label=yh[va], weight=sw[va], categorical_feature=cat_cols, free_raw_data=False)\n",
        "        callbacks=[lgb.early_stopping(200), lgb.log_evaluation(200)]\n",
        "        model_lgb = lgb.train(lgb_params, dtr, valid_sets=[dtr,dva], valid_names=['train','valid'],\n",
        "                              feval=lgb_wsmape_feval, num_boost_round=5000, callbacks=callbacks)\n",
        "        best_it_lgb = model_lgb.best_iteration or model_lgb.current_iteration()\n",
        "        avg_iters_lgb.append(best_it_lgb)\n",
        "        oof_col_lgb[va] = model_lgb.predict(Xh.iloc[va], num_iteration=best_it_lgb)\n",
        "        fold_models_lgb.append(model_lgb)\n",
        "\n",
        "        # XGB\n",
        "        xtr = xgb.DMatrix(Xh.iloc[tr], label=yh[tr], weight=sw[tr], feature_names=Xh.columns.tolist())\n",
        "        xva = xgb.DMatrix(Xh.iloc[va], label=yh[va], weight=sw[va], feature_names=Xh.columns.tolist())\n",
        "        params = dict(xgb_params_base); params['seed'] = SEED + fold + 100*h\n",
        "        evals = [(xtr,'train'), (xva,'valid')]\n",
        "        model_xgb = xgb_train_compat(params, xtr, num_boost_round=5000, evals=evals,\n",
        "                                     early_stopping_rounds=200, verbose_eval=200)\n",
        "        best_it_xgb = xgb_best_iteration(model_xgb)\n",
        "        avg_iters_xgb.append(best_it_xgb if best_it_xgb is not None else model_xgb.num_boosted_rounds())\n",
        "        oof_col_xgb[va] = xgb_predict_compat(model_xgb, xva)\n",
        "        fold_models_xgb.append(model_xgb)\n",
        "\n",
        "        # LSTM\n",
        "        if TF_OK:\n",
        "            Xtr_seq = X_SEQ[tr]  # (Ntr, 28, F)\n",
        "            Xva_seq = X_SEQ[va]\n",
        "            mu,std = compute_seq_mu_std(Xtr_seq)\n",
        "            Xtr_s = standardize_seq(Xtr_seq, mu, std)\n",
        "            Xva_s = standardize_seq(Xva_seq, mu, std)\n",
        "\n",
        "            model_lstm = build_lstm_model(seq_len=Xtr_s.shape[1], n_feat=Xtr_s.shape[2], seed=SEED+fold+1000*h)\n",
        "            es = keras.callbacks.EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True, verbose=0)\n",
        "            hist = model_lstm.fit(\n",
        "                Xtr_s, yh[tr],\n",
        "                validation_data=(Xva_s, yh[va], sw[va]),\n",
        "                sample_weight=sw[tr],\n",
        "                epochs=120,\n",
        "                batch_size=256,\n",
        "                verbose=0,\n",
        "                callbacks=[es]\n",
        "            )\n",
        "            be = np.argmin(hist.history['val_loss']) if 'val_loss' in hist.history else len(hist.history.get('loss',[]))\n",
        "            best_epochs_lstm.append(be)\n",
        "            pred_l = model_lstm.predict(Xva_s, verbose=0).reshape(-1)\n",
        "            oof_col_lstm[va] = np.maximum(0.0, pred_l)\n",
        "            fold_models_lstm.append((model_lstm, mu, std))\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "    oof_lgb[:,h] = oof_col_lgb\n",
        "    oof_xgb[:,h] = oof_col_xgb\n",
        "    if TF_OK:\n",
        "        oof_lstm[:,h] = oof_col_lstm\n",
        "    models_cv_lgb.append((fold_models_lgb, Xh.columns.tolist(), cat_cols))\n",
        "    models_cv_xgb.append((fold_models_xgb, Xh.columns.tolist()))\n",
        "    if TF_OK:\n",
        "        models_cv_lstm.append((fold_models_lstm, list(SEQ_FEAT_NAMES)))\n",
        "    else:\n",
        "        models_cv_lstm.append(([], list(SEQ_FEAT_NAMES)))\n",
        "    gc.collect()\n",
        "\n",
        "# ---------------- OOF dynamic blend (h,DOW) + (h,DOW) + q(h,DOW) ----------------\n",
        "stores_rep = np.repeat(stores_for_rows, 7)\n",
        "\n",
        "# === (h, DOW) XGB   & LSTM   (+) ===\n",
        "alpha_hd = {}  # (h,d) -> alpha for XGB\n",
        "tau_hd   = {}  # (h,d) -> tau for LSTM\n",
        "for h in range(7):\n",
        "    y   = Y[:, h].astype(float)\n",
        "    pL  = oof_lgb[:, h].astype(float)\n",
        "    pX  = oof_xgb[:, h].astype(float)\n",
        "    pS  = oof_lstm[:, h].astype(float) if TF_OK else np.zeros_like(pL)\n",
        "    dws = horizon_calendar(anchor, h+1)[f'dow_h{h+1}'].values\n",
        "    for d in range(7):\n",
        "        m = (y>0) & (dws==d)\n",
        "        if m.sum() < 300:\n",
        "            continue\n",
        "        best_s, best_a, best_t = 1e9, OOB_BLEND, (OOB_TAU if TF_OK else 0.0)\n",
        "        a_grid = [0.25,0.30,0.35,0.40,0.45,0.50,0.55]\n",
        "        t_grid = ([0.00,0.05,0.10,0.15,0.20,0.25] if TF_OK else [0.00])\n",
        "        for a in a_grid:\n",
        "            for t in t_grid:\n",
        "                if a + t > MAX_AX_SUM:\n",
        "                    continue\n",
        "                pr = pL[m]*(1-a-t) + pX[m]*a + pS[m]*t\n",
        "                s  = smape_store_weighted(y[m], pr, stores_for_rows[m])\n",
        "                if s < best_s:\n",
        "                    best_s, best_a, best_t = s, a, t\n",
        "        #  50% \n",
        "        alpha_hd[(h,d)] = OOB_BLEND + 0.5*(best_a - OOB_BLEND)\n",
        "        base_tau = (OOB_TAU if TF_OK else 0.0)\n",
        "        tau_hd[(h,d)]   = base_tau + 0.5*(best_t - base_tau)\n",
        "\n",
        "#  , OOF  \n",
        "oof_blend = np.zeros_like(oof_lgb)\n",
        "for h in range(7):\n",
        "    pL  = oof_lgb[:, h].astype(float)\n",
        "    pX  = oof_xgb[:, h].astype(float)\n",
        "    pS  = oof_lstm[:, h].astype(float) if TF_OK else np.zeros_like(pL)\n",
        "    dws = horizon_calendar(anchor, h+1)[f'dow_h{h+1}'].values\n",
        "    a_arr = np.array([alpha_hd.get((h, int(dd)), OOB_BLEND) for dd in dws], dtype=float)\n",
        "    t_arr = np.array([tau_hd.get((h, int(dd)), (OOB_TAU if TF_OK else 0.0)) for dd in dws], dtype=float)\n",
        "    a_arr = np.clip(a_arr, 0.0, 1.0)\n",
        "    t_arr = np.clip(t_arr, 0.0, 1.0)\n",
        "    sum_at = np.clip(a_arr + t_arr, None, MAX_AX_SUM)\n",
        "    excess = (a_arr + t_arr) - sum_at\n",
        "    t_arr = np.clip(t_arr - np.maximum(0.0, excess), 0.0, 1.0)\n",
        "    oof_blend[:, h] = pL*(1.0 - a_arr - t_arr) + pX*a_arr + pS*t_arr\n",
        "\n",
        "# === (h, DOW)    q  ( 0.60) ===\n",
        "Q_HD = {}  # (h,d) -> 0.50 or 0.60\n",
        "for h in range(7):\n",
        "    y  = Y[:, h].astype(float)\n",
        "    p  = oof_blend[:, h].astype(float)\n",
        "    dws= horizon_calendar(anchor, h+1)[f'dow_h{h+1}'].values\n",
        "    for d in range(7):\n",
        "        m = (y>0) & (dws==d)\n",
        "        if m.sum() < 300:\n",
        "            continue\n",
        "        r = np.median(y[m] / (p[m] + 1e-6))\n",
        "        Q_HD[(h,d)] = 0.60 if r > 1.03 else 0.50\n",
        "\n",
        "val_score = smape_store_weighted(Y.flatten(), oof_blend.flatten(), stores_rep)\n",
        "print(f\"[OOF] weighted SMAPE (LGB+XGB+LSTM dyn /, OOB_BLEND={OOB_BLEND}, OOB_TAU={OOB_TAU})  {val_score:.4f}\")\n",
        "print(f\"       tuned  buckets: {len(alpha_hd)}  |  buckets: {len(tau_hd)} | q buckets: {len(Q_HD)}\")\n",
        "\n",
        "# ---------------- Horizon gamma/beta from blended OOF ----------------\n",
        "print(\"[Calibrate] gamma(h,dow) & beta(h,dow) from blended OOF ...\")\n",
        "eps=1e-6\n",
        "h_gamma = np.ones(7, dtype=float)\n",
        "h_beta  = np.zeros(7, dtype=float)\n",
        "gamma_hd = {}\n",
        "beta_hd  = {}\n",
        "\n",
        "for h in range(7):\n",
        "    y  = Y[:,h].astype(float)\n",
        "    p  = oof_blend[:,h].astype(float)\n",
        "    nz = y>0\n",
        "    if np.any(nz):\n",
        "        ratio = np.median(y[nz]/(p[nz]+eps))\n",
        "        h_gamma[h] = float(np.clip(ratio, 0.90, 1.08))\n",
        "    naive = X_feat[f'lag_{7-h}'].values.astype(float)\n",
        "    best_s, best_b = 1e9, 0.0\n",
        "    for b in [0.00,0.02,0.04,0.06,0.08,0.10,0.12,0.15,0.18,0.20]:\n",
        "        pr = p*h_gamma[h]*(1-b) + naive*b\n",
        "        s = smape_store_weighted(y, pr, stores_for_rows)\n",
        "        if s < best_s:\n",
        "            best_s, best_b = s, b\n",
        "    h_beta[h] = best_b\n",
        "\n",
        "    dows = horizon_calendar(anchor, h+1)[f'dow_h{h+1}'].values\n",
        "    for d in range(7):\n",
        "        m = (y>0) & (dows==d)\n",
        "        if m.sum() >= 200:\n",
        "            r = np.median(y[m]/(p[m]+eps))\n",
        "            gamma_hd[(h,d)] = float(np.clip(r, 0.88, 1.10))\n",
        "            n = naive[m]\n",
        "            best_s, best_b = 1e9, h_beta[h]\n",
        "            for b in [0.00,0.02,0.04,0.06,0.08,0.10,0.12,0.15,0.18]:\n",
        "                pr = p[m]*gamma_hd[(h,d)]*(1-b) + n*b\n",
        "                s = smape_store_weighted(y[m], pr, stores_for_rows[m])\n",
        "                if s < best_s:\n",
        "                    best_s, best_b = s, b\n",
        "            beta_hd[(h,d)] = best_b\n",
        "\n",
        "print(\"  gamma_h (fallback):\", np.round(h_gamma,3))\n",
        "print(\"  beta_h  (fallback):\", np.round(h_beta,3))\n",
        "print(f\"  gamma_hd keys: {len(gamma_hd)}  beta_hd keys: {len(beta_hd)}\")\n",
        "\n",
        "# ---------------- OOF-based calibration maps (store/*) ----------------\n",
        "print(\"[Calibrate] store / (store,dow) / (store,h) ratios from blended OOF ...\")\n",
        "all_dows=[]; h_idx=[]\n",
        "for h in range(7):\n",
        "    d = horizon_calendar(anchor, h+1)[f'dow_h{h+1}'].values\n",
        "    all_dows.append(d)\n",
        "    h_idx.append(np.full(Y.shape[0], h, dtype=int))\n",
        "all_dows = np.concatenate(all_dows)\n",
        "h_idx    = np.concatenate(h_idx)\n",
        "\n",
        "y_flat = Y.flatten(); p_flat = oof_blend.flatten()\n",
        "mask = y_flat>0\n",
        "y_pos, p_pos = y_flat[mask], p_flat[mask]\n",
        "s_pos = stores_rep[mask]\n",
        "dow_pos = all_dows[mask]\n",
        "h_pos = h_idx[mask]\n",
        "\n",
        "store_corr = {}; store_dow_corr = {}; store_h_corr = {}\n",
        "for s in np.unique(s_pos):\n",
        "    m = (s_pos==s)\n",
        "    if m.sum()>=50:\n",
        "        r = np.median(y_pos[m]/(p_pos[m]+eps))\n",
        "        store_corr[s] = float(np.clip(r, 0.75, 1.35))\n",
        "    else:\n",
        "        store_corr[s] = 1.0\n",
        "\n",
        "for s in np.unique(s_pos):\n",
        "    for d in range(7):\n",
        "        m = (s_pos==s) & (dow_pos==d)\n",
        "        if m.sum()>=30:\n",
        "            r = np.median(y_pos[m]/(p_pos[m]+eps))\n",
        "            store_dow_corr[(s,d)] = float(np.clip(r, 0.80, 1.30))\n",
        "\n",
        "for s in np.unique(s_pos):\n",
        "    for hh in range(7):\n",
        "        m = (s_pos==s) & (h_pos==hh)\n",
        "        if m.sum()>=30:\n",
        "            r = np.median(y_pos[m]/(p_pos[m]+eps))\n",
        "            store_h_corr[(s,hh)] = float(np.clip(r, 0.80, 1.25))\n",
        "\n",
        "print(\"[Calibrate] (store, winter_flag) & (store, weekend_or_holiday) ratios ...\")\n",
        "winter_flags=[]; wk_or_h_flags=[]\n",
        "for h in range(7):\n",
        "    cal = horizon_calendar(anchor, h+1)\n",
        "    winter_flags.append(cal[f'is_winter_h{h+1}'].values)\n",
        "    wk_or_h_flags.append((cal[f'is_weekend_h{h+1}'].values | cal[f'is_holiday_h{h+1}'].values).astype(int))\n",
        "winter_flags = np.concatenate(winter_flags)\n",
        "wk_or_h_flags = np.concatenate(wk_or_h_flags)\n",
        "\n",
        "winter_pos = winter_flags[mask]\n",
        "wk_pos  = wk_or_h_flags[mask]\n",
        "\n",
        "store_winter_corr = {}\n",
        "for s in np.unique(s_pos):\n",
        "    for flag in [0,1]:\n",
        "        m = (s_pos==s) & (winter_pos==flag)\n",
        "        if m.sum()>=30:\n",
        "            r = np.median(y_pos[m]/(p_pos[m]+eps))\n",
        "            store_winter_corr[(s,int(flag))] = float(np.clip(r, 0.80, 1.25))\n",
        "\n",
        "store_weekend_corr = {}\n",
        "for s in np.unique(s_pos):\n",
        "    for flag in [0,1]:\n",
        "        m = (s_pos==s) & (wk_pos==flag)\n",
        "        if m.sum()>=30:\n",
        "            r = np.median(y_pos[m]/(p_pos[m]+eps))\n",
        "            store_weekend_corr[(s,int(flag))] = float(np.clip(r, 0.80, 1.25))\n",
        "\n",
        "# ---------------- [NEW] (storeseason) OOF  ----------------\n",
        "print(\"[Calibrate] (store, season_id) ratios from blended OOF ...\")\n",
        "all_season_ids=[]\n",
        "for h in range(7):\n",
        "    all_season_ids.append(horizon_calendar(anchor, h+1)[f'season_id_h{h+1}'].values)\n",
        "season_ids = np.concatenate(all_season_ids)\n",
        "g_pos = season_ids[mask]\n",
        "\n",
        "store_season_corr = {}\n",
        "for s in np.unique(s_pos):\n",
        "    for g in [0,1,2,3]:  # 0spring 1summer 2autumn 3winter\n",
        "        m = (s_pos==s) & (g_pos==g)\n",
        "        if m.sum()>=30:\n",
        "            r = np.median(y_pos[m]/(p_pos[m]+eps))\n",
        "            store_season_corr[(s,int(g))] = float(np.clip(r, 0.85, 1.20))\n",
        "print(\"  calibration maps ready.\")\n",
        "\n",
        "# ---------------- Winsorized caps & floors ----------------\n",
        "print(\"[Calibrate] winsor caps & positive floors ...\")\n",
        "caps = {}\n",
        "floor_sm = {}\n",
        "floor_s  = {}\n",
        "GLOBAL_POS_Q05 = 0.0\n",
        "\n",
        "train_full2 = train_full.copy()\n",
        "train_full2['dow'] = pd.to_datetime(train_full2['date']).dt.dayofweek\n",
        "train_full2['is_weekend'] = (train_full2['dow']>=5).astype(int)\n",
        "train_full2['is_hol'] = is_holiday(train_full2['date'])\n",
        "\n",
        "pos_all = train_full2.loc[train_full2['y']>0, 'y'].values\n",
        "if pos_all.size>0:\n",
        "    GLOBAL_POS_Q05 = float(np.quantile(pos_all, 0.05))\n",
        "\n",
        "for (s,m), g in train_full2.groupby(['store','menu']):\n",
        "    pos = g.loc[g['y']>0, 'y'].values\n",
        "    if pos.size>=20:\n",
        "        floor_sm[(s,m)] = float(np.quantile(pos, 0.05))\n",
        "\n",
        "for s, g in train_full2.groupby('store'):\n",
        "    pos = g.loc[g['y']>0, 'y'].values\n",
        "    if pos.size>=20:\n",
        "        floor_s[s] = float(np.quantile(pos, 0.05))\n",
        "\n",
        "for s, g1 in train_full2.groupby('store'):\n",
        "    for d, g2 in g1.groupby('dow'):\n",
        "        for flag, g3 in g2.groupby((g2['is_weekend'] | g2['is_hol']).astype(int)):\n",
        "            arr = g3['y'].values.astype(float)\n",
        "            if len(arr)>=50:\n",
        "                q95 = float(np.quantile(arr, 0.95))\n",
        "                q99 = float(np.quantile(arr, 0.99))\n",
        "                caps[(s,d,int(flag))] = (q95, q99)\n",
        "\n",
        "def get_floor(store, menu):\n",
        "    return 0.0\n",
        "\n",
        "def apply_cap(store, dow, is_weekend, is_holiday, yhat):\n",
        "    flag = int((is_weekend==1) or (is_holiday==1))\n",
        "    q = caps.get((store,dow,flag))\n",
        "    if q is None: return yhat\n",
        "    q95,q99 = q\n",
        "    lim = (1.05*q99) if flag else (1.00*q99)\n",
        "    return float(min(yhat, max(q95, lim)))\n",
        "\n",
        "# ---------------- Retrain on FULL with multi-seed bagging (LGB+XGB+(opt)LSTM) ----------------\n",
        "print(\"[Retrain] full data with multi-seed bagging (LGB+XGB+(opt)LSTM) ...\")\n",
        "num_rounds_lgb = int(1.05*(np.mean(avg_iters_lgb) if len(avg_iters_lgb)>0 else 1500))\n",
        "num_rounds_xgb = int(1.05*(np.mean(avg_iters_xgb) if len(avg_iters_xgb)>0 else 1500))\n",
        "if TF_OK and len(best_epochs_lstm)>0:\n",
        "    num_epochs_lstm = int(np.clip(1.15*np.mean(best_epochs_lstm), 40, 150))\n",
        "else:\n",
        "    num_epochs_lstm = 0\n",
        "\n",
        "SEEDS = [42, 777, 1201, 2025, 314159]\n",
        "SEEDS_LSTM = [42, 777, 1201] if TF_OK else []\n",
        "\n",
        "final_models_lgb = []\n",
        "final_models_xgb = []\n",
        "final_models_lstm = []  # per h: list of (keras_model, mu, std)\n",
        "\n",
        "for h in range(7):\n",
        "    yh = Y[:,h].astype(float).clip(min=0.0)\n",
        "    Xh = add_h_feats(X_feat, anchor, h, META_store_series)\n",
        "    cal_h = horizon_calendar(anchor, h+1)\n",
        "    is_weekend = cal_h[f'is_weekend_h{h+1}'].values\n",
        "    mon = cal_h[f'month_h{h+1}'].values\n",
        "    is_winter = cal_h[f'is_winter_h{h+1}'].values\n",
        "\n",
        "    max_anchor = pd.to_datetime(anchor).max()\n",
        "    days_from_edge = (pd.to_datetime(anchor) - (max_anchor - pd.Timedelta(days=240))).dt.days.clip(lower=0, upper=240).values\n",
        "    rec = 0.5 + 0.8*(days_from_edge/240.0)**2\n",
        "\n",
        "    pos = yh[yh>0]\n",
        "    thr_low = np.quantile(pos, 0.25) if pos.size>50 else (pos.mean() if pos.size>0 else 0.0)\n",
        "    lowpos_bonus = np.where((yh>0) & (yh<=thr_low), 1.10, 1.0)\n",
        "\n",
        "    sw = np.array([store_weight(s) for s in stores_for_rows]) * rec * lowpos_bonus\n",
        "    sw = sw * (1.15**(is_weekend)) * (1.10**(pd.Series(mon).isin([12,1,2]).astype(int).values))\n",
        "    sw = sw * (1.10**(is_winter)) * (1.05**(is_winter & is_weekend))\n",
        "    sw = sw * (yh>0)\n",
        "\n",
        "    #  / \n",
        "    spring_autumn_act = cal_h[f'spring_autumn_active_h{h+1}'].values.astype(int)\n",
        "    summer_fam = cal_h[f'summer_family_h{h+1}'].values.astype(int)\n",
        "    is_hwadam_arr = np.array([is_hwadam_store(s) for s in stores_for_rows]).astype(int)\n",
        "    summer_aff = np.array([STORE_SEASON_AFFINITY.get((s,'summer'),1.0) for s in stores_for_rows])\n",
        "    sw = sw * (1.05 ** (is_hwadam_arr & spring_autumn_act))\n",
        "    sw = sw * np.where((summer_fam==1) & (summer_aff>1.0), 1.03, 1.0)\n",
        "\n",
        "    cat_cols = get_cat_cols(Xh.columns.tolist())\n",
        "\n",
        "    # LGB multi-seed\n",
        "    models_h_lgb=[]\n",
        "    for sd in SEEDS:\n",
        "        params = dict(lgb_params); params['random_state']=sd\n",
        "        dtr = lgb.Dataset(Xh, label=yh, weight=sw, categorical_feature=cat_cols)\n",
        "        model = lgb.train(params, dtr, num_boost_round=num_rounds_lgb, feval=lgb_wsmape_feval,\n",
        "                          callbacks=[lgb.log_evaluation(250)])\n",
        "        models_h_lgb.append((model, Xh.columns.tolist(), cat_cols))\n",
        "    final_models_lgb.append(models_h_lgb)\n",
        "\n",
        "    # XGB multi-seed (version-safe)\n",
        "    models_h_xgb=[]\n",
        "    for sd in SEEDS:\n",
        "        params = dict(xgb_params_base); params['seed']=sd\n",
        "        dtr = xgb.DMatrix(Xh, label=yh, weight=sw, feature_names=Xh.columns.tolist())\n",
        "        model = xgb.train(params, dtr, num_boost_round=num_rounds_xgb, verbose_eval=250)\n",
        "        models_h_xgb.append((model, Xh.columns.tolist()))\n",
        "    final_models_xgb.append(models_h_xgb)\n",
        "\n",
        "    # LSTM multi-seed\n",
        "    models_h_lstm=[]\n",
        "    if TF_OK and num_epochs_lstm>0:\n",
        "        mu,std = compute_seq_mu_std(X_SEQ)\n",
        "        Xs = standardize_seq(X_SEQ, mu, std)\n",
        "        for sd in SEEDS_LSTM:\n",
        "            m = build_lstm_model(seq_len=Xs.shape[1], n_feat=Xs.shape[2], seed=sd+1000*h)\n",
        "            m.fit(Xs, yh, sample_weight=sw, epochs=num_epochs_lstm, batch_size=256, verbose=0)\n",
        "            models_h_lstm.append((m, mu, std))\n",
        "    final_models_lstm.append(models_h_lstm)\n",
        "    gc.collect()\n",
        "\n",
        "# ---------------- Inference helpers ----------------\n",
        "def weekly_naive_from_history(history28, h):\n",
        "    return float(history28[-(7 - h)])\n",
        "\n",
        "def build_seq_from_history(history28_raw, store_name, history_dates, store_total_lookup):\n",
        "    history28 = np.clip(np.asarray(history28_raw, dtype=float), 0.0, None)\n",
        "    st28 = _lookup_store_total_series(store_name, pd.to_datetime(history_dates), store_total_lookup)\n",
        "    peer28 = np.clip(st28 - history28, 0.0, None)\n",
        "    dts = pd.to_datetime(history_dates)\n",
        "    dows = dts.dayofweek.values.astype(float)\n",
        "    wknd = (dows>=5).astype(float)\n",
        "    hol  = is_holiday(dts).values.astype(float)\n",
        "    winter  = np.array([_is_winter_date(pd.Timestamp(x)) for x in dts], dtype=float)\n",
        "    seq = np.stack([\n",
        "        history28,\n",
        "        st28,\n",
        "        peer28,\n",
        "        dows/6.0,\n",
        "        wknd,\n",
        "        hol,\n",
        "        winter\n",
        "    ], axis=1).astype(float)\n",
        "    return seq  # (28, F)\n",
        "\n",
        "def infer_single_item(history28_raw, store_name, menu_name, last_date,\n",
        "                      history_dates=None, store_total_lookup=None):\n",
        "    history28 = np.clip(np.asarray(history28_raw, dtype=float), 0.0, None)\n",
        "    feats = build_feature_from_window(history28, np.asarray(history28_raw, dtype=float))\n",
        "\n",
        "    feats['store_prior'] = store_prior_index(store_name)\n",
        "    if (history_dates is not None) and (store_total_lookup is not None):\n",
        "        st28 = _lookup_store_total_series(store_name, pd.to_datetime(history_dates), store_total_lookup)\n",
        "        peer28 = np.clip(st28 - history28, 0.0, None)\n",
        "        feats['store_r7_mean']  = float(np.mean(st28[-7:]))\n",
        "        feats['store_r28_mean'] = float(np.mean(st28[-28:]))\n",
        "        feats['peer_r7_mean']   = float(np.mean(peer28[-7:]))\n",
        "        feats['peer_r28_mean']  = float(np.mean(peer28[-28:]))\n",
        "        feats['share_in_store_r7']  = float(history28[-7:].sum()/(st28[-7:].sum()+1e-6))\n",
        "        feats['share_in_store_r28'] = float(history28[-28:].sum()/(st28[-28:].sum()+1e-6))\n",
        "    else:\n",
        "        feats['store_r7_mean']=feats['store_r28_mean']=0.0\n",
        "        feats['peer_r7_mean']=feats['peer_r28_mean']=0.0\n",
        "        feats['share_in_store_r7']=feats['share_in_store_r28']=0.0\n",
        "\n",
        "    feats['store_name']=store_name; feats['menu_name']=menu_name\n",
        "    row = pd.DataFrame([feats])\n",
        "    row = add_kw(row)\n",
        "    s_in = store_name if store_name in le_store.classes_ else \"<UNK>\"\n",
        "    m_in = menu_name  if menu_name  in le_menu.classes_  else \"<UNK>\"\n",
        "    row['store_le'] = le_store.transform([s_in])[0]\n",
        "    row['menu_le']  = le_menu.transform([m_in])[0]\n",
        "    row = row.drop(columns=['store_name','menu_name'])\n",
        "\n",
        "    # LSTM seq build (once)\n",
        "    seq28 = None\n",
        "    if TF_OK and (history_dates is not None) and (store_total_lookup is not None):\n",
        "        seq28 = build_seq_from_history(np.asarray(history28_raw, dtype=float), store_name, history_dates, store_total_lookup)\n",
        "\n",
        "    anchor_date = pd.to_datetime(last_date)\n",
        "    preds=[]\n",
        "    for h in range(7):\n",
        "        cal = horizon_calendar(pd.Series([anchor_date]), h+1)\n",
        "        prof = _profile_block(pd.Series([anchor_date]), h, pd.Series([store_name]))\n",
        "        Xh = pd.concat([row.reset_index(drop=True), cal.reset_index(drop=True), prof.reset_index(drop=True)], axis=1)\n",
        "\n",
        "        dow = int(cal[f'dow_h{h+1}'].iloc[0])\n",
        "        is_wk = int(cal[f'is_weekend_h{h+1}'].iloc[0])\n",
        "        is_h  = int(cal[f'is_holiday_h{h+1}'].iloc[0])\n",
        "        is_winter= int(cal[f'is_winter_h{h+1}'].iloc[0])\n",
        "\n",
        "        # ---- (h,DOW) \n",
        "        q = Q_HD.get((h, dow), 0.50)\n",
        "\n",
        "        seed_preds_lgb=[]; seed_preds_xgb=[]; seed_preds_lstm=[]\n",
        "        # LGB\n",
        "        for (model, cols, cat_cols) in final_models_lgb[h]:\n",
        "            Xuse = Xh.reindex(columns=cols, fill_value=0)\n",
        "            yhat = float(model.predict(Xuse)[0])\n",
        "            seed_preds_lgb.append(max(0.0, yhat))\n",
        "        # XGB\n",
        "        for (model, cols) in final_models_xgb[h]:\n",
        "            Xuse = Xh.reindex(columns=cols, fill_value=0)\n",
        "            dm = xgb.DMatrix(Xuse, feature_names=cols)\n",
        "            yhat = float(xgb_predict_compat(model, dm)[0])\n",
        "            seed_preds_xgb.append(max(0.0, yhat))\n",
        "        # LSTM\n",
        "        if TF_OK and final_models_lstm[h]:\n",
        "            for (m, mu, std) in final_models_lstm[h]:\n",
        "                xs = standardize_seq(seq28[np.newaxis, ...], mu, std)\n",
        "                yhat = float(m.predict(xs, verbose=0).reshape(-1)[0])\n",
        "                seed_preds_lstm.append(max(0.0, yhat))\n",
        "\n",
        "        y_lgb = float(np.quantile(seed_preds_lgb, q)) if seed_preds_lgb else 0.0\n",
        "        y_xgb = float(np.quantile(seed_preds_xgb, q)) if seed_preds_xgb else 0.0\n",
        "        y_lstm= float(np.quantile(seed_preds_lstm, q)) if seed_preds_lstm else 0.0\n",
        "\n",
        "        # ---- (h,DOW) / LGB/XGB/LSTM \n",
        "        a = alpha_hd.get((h, dow), ALPHA_XGB)\n",
        "        t = tau_hd.get((h, dow), (TAU_LSTM if TF_OK else 0.0))\n",
        "        a = max(0.0, min(1.0, a)); t = max(0.0, min(1.0, t))\n",
        "        if a + t > MAX_AX_SUM:\n",
        "            excess = a + t - MAX_AX_SUM\n",
        "            t = max(0.0, t - excess)\n",
        "        yhat  = (1.0-a-t)*y_lgb + a*y_xgb + t*y_lstm\n",
        "\n",
        "        # (h,d)-specific gamma/beta + shrink\n",
        "        g = _shrink(gamma_hd.get((h,dow), h_gamma[h]), TAU['gamma'])\n",
        "        b = beta_hd.get((h,dow), h_beta[h])\n",
        "\n",
        "        naive = weekly_naive_from_history(history28, h)\n",
        "        if naive <= 1e-6:\n",
        "            b = 0.0\n",
        "\n",
        "        nz_idx = np.where(np.asarray(history28) > 0)[0]\n",
        "        days_since_last_nz = 28 - (nz_idx[-1]+1) if len(nz_idx)>0 else 28\n",
        "        if days_since_last_nz >= 21:\n",
        "            b *= 0.75\n",
        "        elif days_since_last_nz >= 14:\n",
        "            b *= 0.90\n",
        "\n",
        "        yhat = yhat * g * (1.0 - b) + naive * b\n",
        "\n",
        "        # (storeseason) OOF  +  prior\n",
        "        g_sid = int(cal[f'season_id_h{h+1}'].iloc[0])      # 0-, 1-, 2-,3-\n",
        "        m_season = _shrink(store_season_corr.get((store_name, g_sid), 1.0), TAU['season'])\n",
        "        g_name = {0:'spring',1:'summer',2:'autumn',3:'winter'}[g_sid]\n",
        "        m_aff = _shrink(STORE_SEASON_AFFINITY.get((store_name, g_name), 1.0), TAU['aff'])\n",
        "        if is_hwadam_store(store_name) and int(cal[f'spring_autumn_active_h{h+1}'].iloc[0])==1:\n",
        "            m_season *= 1.02\n",
        "        m_season = float(np.clip(m_season * m_aff, 0.90, 1.15))\n",
        "        yhat *= m_season\n",
        "\n",
        "        #  store/* \n",
        "        m_store = _shrink(store_corr.get(store_name, 1.0), TAU['store'])\n",
        "        m_sdow  = _shrink(store_dow_corr.get((store_name, dow), 1.0), TAU['sdow'])\n",
        "        m_sh    = _shrink(store_h_corr.get((store_name, h), 1.0), TAU['sh'])\n",
        "        m_winter   = _shrink(store_winter_corr.get((store_name, is_winter), 1.0), TAU['winter'])\n",
        "        m_wk    = _shrink(store_weekend_corr.get((store_name, int(is_wk or is_h)), 1.0), TAU['wk'])\n",
        "        mult = m_store * m_sdow * m_sh * m_winter * m_wk\n",
        "        yhat = yhat * mult\n",
        "\n",
        "        floor_val = get_floor(store_name, menu_name)\n",
        "        if floor_val > 0:\n",
        "            yhat = max(yhat, floor_val)\n",
        "\n",
        "        yhat = apply_cap(store_name, dow, is_wk, is_h, yhat)\n",
        "        preds.append(max(0.0, yhat))\n",
        "    return np.array(preds, dtype=float)\n",
        "\n",
        "# ---------------- Build submission ----------------\n",
        "print(\"[Predict] build submission from TEST files ...\")\n",
        "pred_rows=[]\n",
        "for tfp in TEST_FILES:\n",
        "    tid_m = re.findall(r'TEST_(\\d+)\\.csv', os.path.basename(tfp))\n",
        "    tid = int(tid_m[0]) if tid_m else 0\n",
        "    tdf = pd.read_csv(tfp)\n",
        "    tdf = _clean_columns(tdf); tdf = _force_kor_cols(tdf); tdf = _normalize_key_cols(tdf)\n",
        "    tdf = tdf.rename(columns={\"\":\"date\",\"_\":\"key\",\"\":\"y_raw\"})\n",
        "    tdf[\"date\"] = pd.to_datetime(tdf[\"date\"])\n",
        "    tdf[\"store\"], tdf[\"menu\"] = zip(*tdf[\"key\"].map(split_store_menu))\n",
        "\n",
        "    # TEST   28 (store,date)    \n",
        "    tdf['_yc'] = tdf['y_raw'].clip(lower=0.0)\n",
        "    test_store_total_ser = tdf.groupby(['store','date'])['_yc'].sum().sort_index()\n",
        "\n",
        "    for name,g in tdf.groupby(\"key\"):\n",
        "        g = g.sort_values(\"date\")\n",
        "        vals_raw = g[\"y_raw\"].values.astype(float)\n",
        "        dates_28 = g[\"date\"].values\n",
        "        assert len(vals_raw)==28, f\"{name} in {tfp} is not 28 days\"\n",
        "        store,menu = split_store_menu(name)\n",
        "        preds = infer_single_item(vals_raw, store, menu, g[\"date\"].max(),\n",
        "                                  history_dates=dates_28,\n",
        "                                  store_total_lookup=test_store_total_ser)\n",
        "        for h in range(7):\n",
        "            pred_rows.append({\n",
        "                \"\": f\"TEST_{tid:02d}+{h+1}\",\n",
        "                \"_\": name,\n",
        "                \"\": float(preds[h])\n",
        "            })\n",
        "\n",
        "pred_df = pd.DataFrame(pred_rows)\n",
        "pred_df = _clean_columns(pred_df)\n",
        "pred_df = _force_kor_cols(pred_df)\n",
        "pred_df = _normalize_key_cols(pred_df)\n",
        "\n",
        "pred_wide_clean = pred_df.pivot_table(index='',\n",
        "                                      columns='_',\n",
        "                                      values='',\n",
        "                                      aggfunc='first').reset_index()\n",
        "\n",
        "raw_sample = pd.read_csv(SAMPLE_SUB_PATH)\n",
        "raw_cols = raw_sample.columns.tolist()\n",
        "raw_dates = raw_sample[''].astype(str)\n",
        "\n",
        "def _canon(s: str) -> str:\n",
        "    return (unicodedata.normalize('NFKC', str(s))\n",
        "            .replace('\\ufeff','').replace('\\u200b','').replace('\\xa0','').strip())\n",
        "clean_cols_target = [''] + [_canon(c) for c in raw_cols if c != '']\n",
        "clean_to_raw = { _canon(c): c for c in raw_cols }\n",
        "\n",
        "submission_clean = pd.DataFrame({'': raw_dates})\n",
        "submission_clean = submission_clean.merge(pred_wide_clean, on='', how='left')\n",
        "\n",
        "for c in clean_cols_target:\n",
        "    if c not in submission_clean.columns and c != '':\n",
        "        submission_clean[c] = 0.0\n",
        "extra_cols = [c for c in submission_clean.columns if c not in clean_cols_target]\n",
        "if extra_cols:\n",
        "    submission_clean = submission_clean.drop(columns=extra_cols, errors='ignore')\n",
        "submission_clean = submission_clean[clean_cols_target]\n",
        "\n",
        "submission_final = submission_clean.rename(columns=clean_to_raw)\n",
        "submission_final = submission_final[raw_cols]\n",
        "if submission_final.isna().any().any():\n",
        "    print(\"[Warn] NaN detected in submission; filling 0.0\")\n",
        "    submission_final = submission_final.fillna(0.0)\n",
        "\n",
        "submission_final.to_csv(OUT_PATH, index=False, encoding='utf-8-sig')\n",
        "print(f\"[DONE] saved: {OUT_PATH}\")\n",
        "print(submission_final.head(3))\n",
        "\n",
        "s2 = pd.read_csv(SAMPLE_SUB_PATH)\n",
        "print('[Check] same columns? ', set(submission_final.columns) == set(s2.columns))\n",
        "print('[Check] same order?   ', list(submission_final.columns) == list(s2.columns))\n",
        "print('[Check] same dates?   ', submission_final[''].tolist() == s2[''].tolist())\n",
        "print('[Check] NaNs count:   ', int(submission_final.isna().sum().sum()))\n",
        "print('[Check] shape:        ', submission_final.shape, ' / sample:', s2.shape)"
      ]
    }
  ]
}